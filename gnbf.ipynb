{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Optional, Type\n",
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "import re\n",
    "\n",
    "os.environ[\"PYTHONPATH\"] = \"/home/aksha/Workbench/Research/Labs/e-lab/parser/constrain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constrain.grammars.gnbf import GNBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'ThoughtState', 'type': 'object', 'properties': {'thought': {'title': 'Thought', 'type': 'string'}, 'goal': {'title': 'Goal', 'type': 'string'}, 'tool': {'title': 'Tool', 'description': \"Choose one of ['Web_QA', 'Web_Search', 'Web_Scraping', 'Web_Automation', 'Web_Research']\", 'type': 'string'}, 'action': {'title': 'Action', 'description': \"Choose one of ['Create', 'Update', 'Delete', 'Read']\", 'type': 'string'}, 'action_input': {'title': 'Action Input', 'description': 'The input data for the action', 'type': 'string'}, 'thought_id': {'title': 'Thought Id', 'description': 'The unique identifier for the thought', 'type': 'array', 'items': {}}}, 'required': ['thought', 'goal', 'tool', 'action', 'action_input', 'thought_id']}\n",
      "-------------------\n",
      "['string', 'number', 'bool', 'none']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "root ::= thoughtstate ws \n",
      "thoughtstate ::= [{] ws [\"] [t] [h] [o] [u] [g] [h] [t] [\"] [:] ws string [,] ws [\"] [g] [o] [a] [l] [\"] [:] ws string [,] ws [\"] [t] [o] [o] [l] [\"] [:] ws string [,] ws [\"] [a] [c] [t] [i] [o] [n] [\"] [:] ws string [,] ws [\"] [a] [c] [t] [i] [o] [n] [-] [i] [n] [p] [u] [t] [\"] [:] ws string [,] ws [\"] [t] [h] [o] [u] [g] [h] [t] [-] [i] [d] [\"] [:] ws array [}] ws \n",
      "ws ::= ws_5 \n",
      "string ::= [\"] string_6 [\"] ws \n",
      "array ::= [[] ws array_27 []] ws \n",
      "ws_5 ::= [ <U+0009><U+000A>] ws_5 | \n",
      "string_6 ::= string_7 \n",
      "string_7 ::= [^\"] string_7 | \n",
      "number ::= number_9 number_15 number_19 ws \n",
      "number_9 ::= number_10 number_11 \n",
      "number_10 ::= [-] | \n",
      "number_11 ::= [0-9] | [1-9] number_12 \n",
      "number_12 ::= [0-9] number_12 | \n",
      "number_13 ::= [.] number_14 \n",
      "number_14 ::= [0-9] number_14 | [0-9] \n",
      "number_15 ::= number_13 | \n",
      "number_16 ::= [ee] number_17 number_18 \n",
      "number_17 ::= [-+] | \n",
      "number_18 ::= [0-9] number_18 | [0-9] \n",
      "number_19 ::= number_16 | \n",
      "bool ::= bool_21 ws \n",
      "bool_21 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] \n",
      "none ::= [n] [o] [n] [e] ws \n",
      "array_23 ::= thought-id-value array_26 \n",
      "thought-id-value ::= string | number | bool | none \n",
      "array_25 ::= [,] ws thought-id-value \n",
      "array_26 ::= array_25 array_26 | \n",
      "array_27 ::= array_23 | \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<llama_cpp.llama_grammar.LlamaGrammar at 0x7fab1946a3a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ThoughtState(BaseModel):\n",
    "    thought: str\n",
    "    goal: str\n",
    "    tool: str = Field(\n",
    "        ...,\n",
    "        description=\"Choose one of ['Web_QA', 'Web_Search', 'Web_Scraping', 'Web_Automation', 'Web_Research']\",\n",
    "    )\n",
    "    action: str = Field(\n",
    "        ..., description=\"Choose one of ['Create', 'Update', 'Delete', 'Read']\"\n",
    "    )\n",
    "    action_input: str = Field(..., description=\"The input data for the action\")\n",
    "    thought_id: list = Field(..., description=\"The unique identifier for the thought\")\n",
    "\n",
    "\n",
    "print(ThoughtState.schema())\n",
    "print(\"-------------------\")\n",
    "converter = GNBF(ThoughtState.schema())\n",
    "grammar = converter.generate_grammar()\n",
    "converter.verify_grammar(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root ::= thoughtstate ws\n",
      "thoughtstate ::= \"{\" ws \"\\\"thought\\\":\" ws string \",\" ws \"\\\"goal\\\":\" ws string \",\" ws \"\\\"tool\\\":\" ws string \",\" ws \"\\\"action\\\":\" ws string \",\" ws \"\\\"action-input\\\":\" ws string \",\" ws \"\\\"thought-id\\\":\" ws array \"}\" ws\n",
      "ws ::= [ \\t\\n]*\n",
      "string ::= \"\\\"\"   ([^\"]*)   \"\\\"\" ws\n",
      "number ::= (\"-\"? ([0-9] | [1-9] [0-9]*)) (\".\" [0-9]+)? ([ee] [-+]? [0-9]+)? ws\n",
      "bool ::= (\"true\" | \"false\") ws\n",
      "none ::= \"none\" ws\n",
      "array ::= \"[\" ws (\n",
      "                thought-id-value\n",
      "                (\",\" ws thought-id-value)*\n",
      "            )? \"]\" ws\n",
      "thought-id-value ::= string | number | bool | none\n"
     ]
    }
   ],
   "source": [
    "print(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thoughtstate ws\n",
      "\"{\" ws \"\\\"thought\\\":\" ws string \",\" ws \"\\\"goal\\\":\" ws string \",\" ws \"\\\"tool\\\":\" ws string \",\" ws \"\\\"action\\\":\" ws string \",\" ws \"\\\"action-input\\\":\" ws string \",\" ws \"\\\"thought-id\\\":\" ws string \"}\" ws\n",
      "[ \\t\\n]*\n",
      "\"\\\"\"   ([^\"]*)   \"\\\"\" ws\n",
      "{'root': ['thoughtstate', 'ws'], 'thoughtstate': ['{', 'ws', '\"thought\":', 'ws', 'string', ',', 'ws', '\"goal\":', 'ws', 'string', ',', 'ws', '\"tool\":', 'ws', 'string', ',', 'ws', '\"action\":', 'ws', 'string', ',', 'ws', '\"action-input\":', 'ws', 'string', ',', 'ws', '\"thought-id\":', 'ws', 'string', '}', 'ws'], 'ws': ['[', '\\\\t\\\\n]', '*'], 'string': ['\"', '([^\"]', '*)', '\"', 'ws']}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class GrammarParser:\n",
    "    def __init__(self, grammar):\n",
    "        self.grammar = grammar\n",
    "        self.rules = {}\n",
    "        self.parse_grammar()\n",
    "\n",
    "    def parse_grammar(self):\n",
    "        # Split the grammar into lines and parse each rule\n",
    "        lines = self.grammar.strip().split(\"\\n\")\n",
    "        for line in lines:\n",
    "            non_terminal, expression = line.split(\"::=\")\n",
    "            non_terminal = non_terminal.strip()\n",
    "            expression = expression.strip()\n",
    "            self.rules[non_terminal] = self.parse_grammar_rule(expression)\n",
    "\n",
    "    def parse_grammar_rule(self, expression):\n",
    "        elements = []\n",
    "\n",
    "        # Handle grouping and repetition symbols\n",
    "        expression = self.process_repetitions_and_grouping(expression)\n",
    "\n",
    "        # Split the expression into tokens while handling escape sequences correctly\n",
    "        tokens = re.split(r\"(?<!\\\\)\\s+\", expression)\n",
    "\n",
    "        for token in tokens:\n",
    "            if token.startswith('\"') and token.endswith('\"'):\n",
    "                # Remove escape characters for quotes\n",
    "                token = token[1:-1].replace('\\\\\"', '\"')\n",
    "            elements.append(token)\n",
    "\n",
    "        return elements\n",
    "\n",
    "    def process_repetitions_and_grouping(self, expression):\n",
    "        # This is a placeholder for processing repetition symbols and grouping\n",
    "        # You would expand this method to handle *, +, ?, and () appropriately\n",
    "        # For simplicity, this example does not implement the full logic\n",
    "        print(expression)\n",
    "        return expression.replace(\"*\", \" *\").replace(\"+\", \" +\").replace(\"?\", \" ?\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "grammar = r\"\"\"\n",
    "root ::= thoughtstate ws\n",
    "thoughtstate ::= \"{\" ws \"\\\"thought\\\":\" ws string \",\" ws \"\\\"goal\\\":\" ws string \",\" ws \"\\\"tool\\\":\" ws string \",\" ws \"\\\"action\\\":\" ws string \",\" ws \"\\\"action-input\\\":\" ws string \",\" ws \"\\\"thought-id\\\":\" ws string \"}\" ws\n",
    "ws ::= [ \\t\\n]*\n",
    "string ::= \"\\\"\"   ([^\"]*)   \"\\\"\" ws\n",
    "\"\"\"\n",
    "\n",
    "parser = GrammarParser(grammar)\n",
    "print(parser.rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ye )\n",
      "root: [Token(thoughtstate, non-terminal, None, None, None), Token(ws, non-terminal, None, None, None)]\n",
      "thoughtstate: [Token({, terminal, None, None, None), Token(ws, non-terminal, None, None, None), Token(thought, literal, None, \", :), Token(ws, non-terminal, None, None, None), Token(string, non-terminal, None, None, None), Token(,, terminal, None, None, None), Token(ws, non-terminal, None, None, None), Token(goal, literal, None, \", :), Token(ws, non-terminal, None, None, None), Token(string, non-terminal, None, None, None), Token(,, terminal, None, None, None), Token(ws, non-terminal, None, None, None), Token(tool, literal, None, \", :), Token(ws, non-terminal, None, None, None), Token(string, non-terminal, None, None, None), Token(,, terminal, None, None, None), Token(ws, non-terminal, None, None, None), Token(action, literal, None, \", :), Token(ws, non-terminal, None, None, None), Token(string, non-terminal, None, None, None), Token(,, terminal, None, None, None), Token(ws, non-terminal, None, None, None), Token(action-input, literal, None, \", :), Token(ws, non-terminal, None, None, None), Token(string, non-terminal, None, None, None), Token(,, terminal, None, None, None), Token(ws, non-terminal, None, None, None), Token(thought-id, literal, None, \", :), Token(ws, non-terminal, None, None, None), Token(string, non-terminal, None, None, None), Token(}, terminal, None, None, None), Token(ws, non-terminal, None, None, None)]\n",
      "ws: [Token([ \\t\\n], literal, +, None, None)]\n",
      "string: [Token(\", literal, None, \", None), Token(( [^\"\\\\], terminal, None, None, None), Token(| \"\\\\\", terminal, None, None, None), Token(([\"\\\\/bfnrt], terminal, None, None, None), Token(| \"u\", terminal, None, None, None), Token([0-9a-fA-F], literal, None, None, None), Token([0-9a-fA-F], literal, None, None, None), Token([0-9a-fA-F], literal, None, None, None), Token([0-9a-fA-F]), terminal, None, None, None), Token(), terminal, *, None, None), Token(\", literal, None, \", None)]\n"
     ]
    }
   ],
   "source": [
    "class Token:\n",
    "    def __init__(\n",
    "        self,\n",
    "        value,\n",
    "        token_type,\n",
    "        repetition=None,\n",
    "        enclosing_char=None,\n",
    "        assignment_char=None,\n",
    "    ):\n",
    "        self.value = value\n",
    "        self.token_type = token_type  # 'terminal', 'non-terminal', 'literal'\n",
    "        self.repetition = repetition  # '*', '+', '?'\n",
    "        self.enclosing_char = enclosing_char  # '\"', etc.\n",
    "        self.assignment_char = assignment_char  # ':', '='\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Token({self.value}, {self.token_type}, {self.repetition}, {self.enclosing_char}, {self.assignment_char})\"\n",
    "\n",
    "\n",
    "class GrammarParser:\n",
    "    def __init__(self, grammar):\n",
    "        self.grammar = grammar\n",
    "        self.rules = {}\n",
    "        self.parse_grammar()\n",
    "\n",
    "    def parse_grammar_rule(self, expression):\n",
    "        elements = []\n",
    "\n",
    "        # Updated to handle tokens\n",
    "        tokens = re.split(r\"(?<!\\\\)\\s+\", expression)\n",
    "        carry = None\n",
    "        for token in tokens:\n",
    "            if carry:\n",
    "                token = carry + \" \" + token\n",
    "                carry = None\n",
    "            parsed_token = self.parse_token(token)\n",
    "            if type(parsed_token) == str:\n",
    "                carry = parsed_token\n",
    "            if type(parsed_token) == list:\n",
    "                elements.extend(parsed_token)\n",
    "            if parsed_token and not carry:  # Ignore None returned for grouping symbols\n",
    "                elements.append(parsed_token)\n",
    "\n",
    "        return elements\n",
    "\n",
    "    def parse_grammar(self):\n",
    "        lines = self.grammar.strip().split(\"\\n\")\n",
    "        for line in lines:\n",
    "            non_terminal, expression = line.split(\"::=\")\n",
    "            non_terminal = non_terminal.strip()\n",
    "            self.rules[non_terminal] = {}\n",
    "\n",
    "        for line in lines:\n",
    "            non_terminal, expression = line.split(\"::=\")\n",
    "            non_terminal = non_terminal.strip()\n",
    "            expression = expression.strip()\n",
    "\n",
    "            self.rules[non_terminal] = self.parse_grammar_rule(expression)\n",
    "\n",
    "    def parse_token(self, element):\n",
    "        if len(element) == 1:\n",
    "            return element\n",
    "\n",
    "        repetition = None\n",
    "        if element.endswith((\"*\", \"+\", \"?\")):\n",
    "            repetition = element[-1]\n",
    "            element = element[:-1]\n",
    "\n",
    "        # Check for literal enclosed in quotes\n",
    "        if element.startswith('\"') and element.endswith('\"'):\n",
    "            assignment_char = None\n",
    "            if element[:-1].endswith(\":\"):\n",
    "                assignment_char = \":\"\n",
    "            elif element[:-1].endswith(\"=\"):\n",
    "                assignment_char = \"=\"\n",
    "\n",
    "            element = element[1:-1].replace(r\"\\\"\", '\"')\n",
    "            if assignment_char:\n",
    "                element = element.replace(f\"{assignment_char}\", \"\")\n",
    "\n",
    "            if (\"(\" in element and \")\" in element) or (\n",
    "                element.endswith((\"*\", \"+\", \"?\"))\n",
    "            ):\n",
    "                return self.parse_token(element.replace(\")\", \"\").replace(\"(\", \"\"))\n",
    "\n",
    "            if element in \"{}./.,><!@#$%&\":\n",
    "                return Token(element, \"terminal\", repetition=repetition)\n",
    "\n",
    "            if len(element) > 1:\n",
    "                element = element.replace('\"', \"\")\n",
    "            else:\n",
    "                element\n",
    "            return Token(\n",
    "                element,\n",
    "                \"literal\",\n",
    "                repetition=repetition,\n",
    "                enclosing_char='\"',\n",
    "                assignment_char=assignment_char,\n",
    "            )\n",
    "\n",
    "        if element in self.rules.keys():\n",
    "            return Token(element, \"non-terminal\", repetition=repetition)\n",
    "\n",
    "        if element.startswith(\"[\") and element.endswith(\"]\"):\n",
    "            return Token(element, \"literal\", repetition=repetition)\n",
    "        elif element in (\"(\", \")\"):\n",
    "            print(\"ye\", element)\n",
    "\n",
    "        return Token(element, \"terminal\", repetition=repetition)\n",
    "\n",
    "\n",
    "grammar = r\"\"\"\n",
    "root ::= thoughtstate ws\n",
    "thoughtstate ::= \"{\" ws \"\\\"thought\\\":\" ws string \",\" ws \"\\\"goal\\\":\" ws string \",\" ws \"\\\"tool\\\":\" ws string \",\" ws \"\\\"action\\\":\" ws string \",\" ws \"\\\"action-input\\\":\" ws string \",\" ws \"\\\"thought-id\\\":\" ws string \"}\" ws\n",
    "ws ::= [ \\t\\n]+\n",
    "string ::= \"\\\"\" ( [^\"\\\\] | \"\\\\\" ([\"\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F]) )* \"\\\"\"\n",
    "\"\"\"\n",
    "\n",
    "parser = GrammarParser(grammar)\n",
    "for key, value in parser.rules.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarStack:\n",
    "    def __init__(self, grammar_rules):\n",
    "        self.grammar_rules = grammar_rules\n",
    "        self.stack = []\n",
    "        self.initialize_stack(\"root\")\n",
    "\n",
    "    def initialize_stack(self, rule_name):\n",
    "        \"\"\"Recursively traverse grammar rules to initialize the stack.\"\"\"\n",
    "        rule_tokens = self.grammar_rules[rule_name]\n",
    "        token = rule_tokens[0]\n",
    "        idx = 0\n",
    "        if token.token_type == \"non-terminal\":\n",
    "            self.stack.append((rule_name, idx))\n",
    "            self.initialize_stack(token.value)\n",
    "        else:\n",
    "            self.stack.append((rule_name, idx))\n",
    "\n",
    "    def push_next_token_or_rule(self, rule_name, idx):\n",
    "        \"\"\"Pushes the next token or rule based on the current index.\"\"\"\n",
    "        print(\"pushing\", rule_name, idx)\n",
    "        if idx < len(self.grammar_rules[rule_name]) - 1:\n",
    "            self.stack.pop()  # Pop the completed rule\n",
    "            self.stack.append((rule_name, idx + 1))\n",
    "            print(\"pushed\", rule_name, idx + 1)\n",
    "\n",
    "            if self.grammar_rules[rule_name][idx + 1].token_type == \"non-terminal\":\n",
    "                self.push_next_token_or_rule(\n",
    "                    self.grammar_rules[rule_name][idx + 1].value, 0\n",
    "                )\n",
    "        elif rule_name not in [x[0] for x in self.stack]:\n",
    "            print(\"pushed\", rule_name, idx)\n",
    "            self.stack.append((rule_name, idx))\n",
    "        else:\n",
    "            # If it was the last token, pop the current rule and update the parent rule\n",
    "            self.stack.pop()  # Pop the completed rule\n",
    "            if self.stack:\n",
    "                self.push_next_token_or_rule(*self.stack[-1])  # Update parent rule\n",
    "\n",
    "    def update_stack_with_valid_substring(self, valid_substring):\n",
    "        if not self.stack:\n",
    "            return\n",
    "\n",
    "        current_rule, idx = self.stack[-1]\n",
    "        current_tokens = self.grammar_rules[current_rule]\n",
    "        current_token = current_tokens[idx]\n",
    "\n",
    "        # Check if the valid_substring matches the current token's value\n",
    "        if valid_substring == current_token.value:\n",
    "            # If match found, move to the next token in the current rule\n",
    "            self.push_next_token_or_rule(current_rule, idx)\n",
    "        else:\n",
    "            # Handle the case where a closing token might complete a non-terminal\n",
    "            # And needs to return to the parent rule\n",
    "            for i in range(len(self.stack) - 1, -1, -1):\n",
    "                rule_name, token_idx = self.stack[i]\n",
    "                tokens = self.grammar_rules[rule_name]\n",
    "\n",
    "                if (\n",
    "                    token_idx < len(tokens)\n",
    "                    and tokens[token_idx].value == valid_substring\n",
    "                ):\n",
    "                    self.push_next_token_or_rule(rule_name, token_idx)\n",
    "                    break\n",
    "\n",
    "        print(\"stack shit\", self.stack)\n",
    "\n",
    "    def next_expected_token(self):\n",
    "        if self.stack:\n",
    "            current_rule, idx = self.stack[-1]\n",
    "            return self.grammar_rules[current_rule][idx]\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = GrammarStack(parser.rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('root', 0), ('thoughtstate', 0)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack.stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarParser:\n",
    "    def __init__(self, grammar_stack, input_string):\n",
    "        \"\"\"\n",
    "        Initializes the parser with a grammar stack and the input string.\n",
    "\n",
    "        Args:\n",
    "            grammar_stack (GrammarStack): An instance of GrammarStack managing the parsing context.\n",
    "            input_string (str): The string to be parsed.\n",
    "        \"\"\"\n",
    "        self.grammar_stack = grammar_stack\n",
    "        self.input_string = input_string\n",
    "\n",
    "    def convert_grammar_to_regex(self, token):\n",
    "        \"\"\"\n",
    "        Converts a grammar token's value to a regex pattern, adding support for \n",
    "        enclosing characters and assignment characters.\n",
    "\n",
    "        Args:\n",
    "            token (Token): The grammar token to be converted to a regex pattern.\n",
    "\n",
    "        Returns:\n",
    "            str: A regex pattern derived from the token's characteristics.\n",
    "        \"\"\"\n",
    "\n",
    "        # Start with the token's value, assuming it might be a character class or literal\n",
    "        regex_pattern = token.value\n",
    "\n",
    "        # If the token represents a pattern (e.g., whitespace, character class)\n",
    "\n",
    "        # If an enclosing character is specified, ensure it is present around the token\n",
    "        if token.enclosing_char:\n",
    "            # Escape the enclosing character if it's a special regex character\n",
    "            escaped_enclosing_char = token.enclosing_char\n",
    "            regex_pattern = f\"{escaped_enclosing_char}{regex_pattern.replace(\n",
    "                escaped_enclosing_char, '')}{escaped_enclosing_char}\"\n",
    "        if token.token_type == 'literal':\n",
    "            if token.repetition:\n",
    "                regex_pattern += token.repetition\n",
    "        # If an assignment character is specified, ensure it is present (with an optional space after)\n",
    "        if token.assignment_char:\n",
    "            # Escape the assignment character if it's a special regex character\n",
    "            escaped_assignment_char = re.escape(token.assignment_char)\n",
    "            regex_pattern += f\"{escaped_assignment_char}\\\\s?\"\n",
    "\n",
    "        return regex_pattern\n",
    "\n",
    "    def parse_input(self):\n",
    "        \"\"\"\n",
    "        Iterates through the input string, matches tokens against the grammar stack,\n",
    "        and updates the stack based on the parsing progression. Handles mismatches\n",
    "        and enforces grammar rules as needed.\n",
    "        \"\"\"\n",
    "        current_idx = 0\n",
    "        expected_token = self.grammar_stack.next_expected_token()\n",
    "        while current_idx < len(self.input_string):\n",
    "            print('expected_token ', expected_token)\n",
    "\n",
    "            self.grammar_stack.push_next_token_or_rule(\n",
    "                *self.grammar_stack.stack[-1])\n",
    "            next_expected_token = self.grammar_stack.next_expected_token()\n",
    "            print('next_expected_token ', next_expected_token)\n",
    "\n",
    "            if not next_expected_token:\n",
    "                end_idx = len(self.input_string)\n",
    "            else:\n",
    "                # From the current index, keep incrementing the end index to find a matching token with next_expected_token using regex\n",
    "                end_idx = current_idx + 1\n",
    "                print(current_idx, end_idx)\n",
    "                while end_idx <= len(self.input_string):\n",
    "                    print('endidx', end_idx)\n",
    "                    actual_substring = self.input_string[current_idx:end_idx]\n",
    "                    print(f'whle checking |{actual_substring}|')\n",
    "                    match, match_end_obj = self.match_token(\n",
    "                        next_expected_token, actual_substring)\n",
    "                    if match:\n",
    "                        if type(match_end_obj) == re.Match:\n",
    "                            end_idx = current_idx + match_end_obj.start()\n",
    "                        else:\n",
    "                            end_idx = current_idx + match_end_obj\n",
    "                        break\n",
    "                    else:\n",
    "                        end_idx += 1\n",
    "                print(current_idx, end_idx)\n",
    "\n",
    "            # Determine the end index for the current token match attempt\n",
    "            actual_substring = self.input_string[current_idx:end_idx]\n",
    "            print(f'actual_substring |{actual_substring}|')\n",
    "            match, end_obj = self.match_token(expected_token, actual_substring)\n",
    "\n",
    "            if match:\n",
    "                print('matched ', self.input_string[current_idx:end_idx])\n",
    "                current_idx += end_obj.end() if type(end_obj) == re.Match else end_obj\n",
    "                expected_token = self.grammar_stack.next_expected_token()\n",
    "                # self.grammar_stack.push_next_token_or_rule(*self.grammar_stack.stack[-1])\n",
    "            else:\n",
    "                # Handle mismatch, including potential enforcement of missing tokens\n",
    "                self.handle_mismatch(expected_token, actual_substring)\n",
    "                break\n",
    "\n",
    "            current_idx = end_idx\n",
    "\n",
    "            expected_token = next_expected_token\n",
    "            print('DONE WITH ', actual_substring)\n",
    "            print('-----------------------------')\n",
    "            print('stack ', self.grammar_stack.stack)\n",
    "\n",
    "    def match_token(self, expected_token, actual_substring) -> (bool, int):\n",
    "        \"\"\"\n",
    "        Checks if the actual substring matches the expected token and returns\n",
    "        the index up to which it matches.\n",
    "\n",
    "        Args:\n",
    "            expected_token (Token): The token expected based on the current grammar context.\n",
    "            actual_substring (str): The substring from the input string being examined.\n",
    "\n",
    "        Returns:\n",
    "            (bool, int): A tuple where the first element is True if the substring matches the expected token, \n",
    "                        False otherwise, and the second element is the index of the string up to which \n",
    "                        it can match or the index of mismatch.\n",
    "        \"\"\"\n",
    "\n",
    "        if expected_token.token_type == 'terminal':\n",
    "            match = expected_token.value == actual_substring[-1]\n",
    "            idx = len(actual_substring) - 2 if match else 0\n",
    "            if idx < 0:\n",
    "                idx = 0\n",
    "            print('matching terminal')\n",
    "            if match:\n",
    "                print('matched')\n",
    "            return (match, idx)\n",
    "\n",
    "        elif expected_token.token_type == 'literal':\n",
    "            # Use regex for literals, particularly for pattern tokens\n",
    "            pattern = self.convert_grammar_to_regex(expected_token)\n",
    "            if '\\' ' in pattern:\n",
    "                pattern = re.escape(pattern)\n",
    "            print('pattern', repr(pattern))\n",
    "            print(f'matching |{actual_substring}|')\n",
    "            match_obj = re.search(pattern, actual_substring)\n",
    "\n",
    "            if match_obj:\n",
    "                print('worked')\n",
    "                # If there's a match, return True and the end index of the match\n",
    "                return (True, match_obj)\n",
    "            else:\n",
    "                # Find the first character that doesn't match the pattern\n",
    "                for idx, char in enumerate(actual_substring):\n",
    "                    if not re.match(pattern, char):\n",
    "                        return (False, idx)\n",
    "                return (False, len(actual_substring))\n",
    "\n",
    "    def handle_mismatch(self, expected_token, actual_substring):\n",
    "        print('handling mismatch', expected_token, '|', actual_substring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected_token  Token({, terminal, None, None, None)\n",
      "pushing thoughtstate 0\n",
      "pushed thoughtstate 1\n",
      "pushing ws 0\n",
      "pushed ws 0\n",
      "next_expected_token  Token([ \t\n",
      "], literal, +, None, None)\n",
      "0 1\n",
      "endidx 1\n",
      "whle checking |{|\n",
      "pattern '[ \\t\\n]+'\n",
      "matching |{|\n",
      "endidx 2\n",
      "whle checking |{ |\n",
      "pattern '[ \\t\\n]+'\n",
      "matching |{ |\n",
      "worked\n",
      "0 1\n",
      "actual_substring |{|\n",
      "matching terminal\n",
      "matched\n",
      "matched  {\n",
      "DONE WITH  {\n",
      "-----------------------------\n",
      "stack  [('root', 0), ('thoughtstate', 1), ('ws', 0)]\n",
      "expected_token  Token([ \t\n",
      "], literal, +, None, None)\n",
      "pushing ws 0\n",
      "pushing thoughtstate 1\n",
      "pushed thoughtstate 2\n",
      "next_expected_token  Token(\"thought\", literal, None, \", :)\n",
      "1 2\n",
      "endidx 2\n",
      "whle checking | |\n",
      "pattern '\"thought\":\\\\s?'\n",
      "matching | |\n",
      "endidx 3\n",
      "whle checking | \"|\n",
      "pattern '\"thought\":\\\\s?'\n",
      "matching | \"|\n",
      "endidx 4\n",
      "whle checking | \"t|\n",
      "pattern '\"thought\":\\\\s?'\n",
      "matching | \"t|\n",
      "endidx 5\n",
      "whle checking | \"th|\n",
      "pattern '\"thought\":\\\\s?'\n",
      "matching | \"th|\n",
      "endidx 6\n",
      "whle checking | \"tho|\n",
      "pattern '\"thought\":\\\\s?'\n",
      "matching | \"tho|\n",
      "endidx 7\n",
      "whle checking | \"thou|\n",
      "pattern '\"thought\":\\\\s?'\n",
      "matching | \"thou|\n",
      "endidx 8\n",
      "whle checking | \"thoug|\n",
      "pattern '\"thought\":\\\\s?'\n",
      "matching | \"thoug|\n",
      "endidx 9\n",
      "whle checking | \"though|\n",
      "pattern '\"thought\":\\\\s?'\n",
      "matching | \"though|\n",
      "endidx 10\n",
      "whle checking | \"thought|\n",
      "pattern '\"thought\":\\\\s?'\n",
      "matching | \"thought|\n",
      "endidx 11\n",
      "whle checking | \"thought\"|\n",
      "pattern '\"thought\":\\\\s?'\n",
      "matching | \"thought\"|\n",
      "endidx 12\n",
      "whle checking | \"thought\":|\n",
      "pattern '\"thought\":\\\\s?'\n",
      "matching | \"thought\":|\n",
      "worked\n",
      "1 2\n",
      "actual_substring | |\n",
      "pattern '[ \\t\\n]+'\n",
      "matching | |\n",
      "worked\n",
      "matched   \n",
      "DONE WITH   \n",
      "-----------------------------\n",
      "stack  [('root', 0), ('thoughtstate', 2)]\n",
      "expected_token  Token(\"thought\", literal, None, \", :)\n",
      "pushing thoughtstate 2\n",
      "pushed thoughtstate 3\n",
      "pushing ws 0\n",
      "pushed ws 0\n",
      "next_expected_token  Token([ \t\n",
      "], literal, +, None, None)\n",
      "2 3\n",
      "endidx 3\n",
      "whle checking |\"|\n",
      "pattern '[ \\t\\n]+'\n",
      "matching |\"|\n",
      "endidx 4\n",
      "whle checking |\"t|\n",
      "pattern '[ \\t\\n]+'\n",
      "matching |\"t|\n",
      "endidx 5\n",
      "whle checking |\"th|\n",
      "pattern '[ \\t\\n]+'\n",
      "matching |\"th|\n",
      "endidx 6\n",
      "whle checking |\"tho|\n",
      "pattern '[ \\t\\n]+'\n",
      "matching |\"tho|\n",
      "endidx 7\n",
      "whle checking |\"thou|\n",
      "pattern '[ \\t\\n]+'\n",
      "matching |\"thou|\n",
      "endidx 8\n",
      "whle checking |\"thoug|\n",
      "pattern '[ \\t\\n]+'\n",
      "matching |\"thoug|\n",
      "endidx 9\n",
      "whle checking |\"though|\n",
      "pattern '[ \\t\\n]+'\n",
      "matching |\"though|\n",
      "endidx 10\n",
      "whle checking |\"thought|\n",
      "pattern '[ \\t\\n]+'\n",
      "matching |\"thought|\n",
      "endidx 11\n",
      "whle checking |\"thought\"|\n",
      "pattern '[ \\t\\n]+'\n",
      "matching |\"thought\"|\n",
      "endidx 12\n",
      "whle checking |\"thought\":|\n",
      "pattern '[ \\t\\n]+'\n",
      "matching |\"thought\":|\n",
      "endidx 13\n",
      "whle checking |\"thought\": |\n",
      "pattern '[ \\t\\n]+'\n",
      "matching |\"thought\": |\n",
      "worked\n",
      "2 12\n",
      "actual_substring |\"thought\":|\n",
      "pattern '\"thought\":\\\\s?'\n",
      "matching |\"thought\":|\n",
      "worked\n",
      "matched  \"thought\":\n",
      "DONE WITH  \"thought\":\n",
      "-----------------------------\n",
      "stack  [('root', 0), ('thoughtstate', 3), ('ws', 0)]\n",
      "expected_token  Token([ \t\n",
      "], literal, +, None, None)\n",
      "pushing ws 0\n",
      "pushing thoughtstate 3\n",
      "pushed thoughtstate 4\n",
      "pushing string 0\n",
      "pushed string 0\n",
      "next_expected_token  Token(\\\"[0-9a-fA-F]+\\\", literal, +, None, None)\n",
      "12 13\n",
      "endidx 13\n",
      "whle checking | |\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | |\n",
      "endidx 14\n",
      "whle checking | \"|\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"|\n",
      "endidx 15\n",
      "whle checking | \"A|\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"A|\n",
      "endidx 16\n",
      "whle checking | \"AI|\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"AI|\n",
      "endidx 17\n",
      "whle checking | \"AI |\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"AI |\n",
      "endidx 18\n",
      "whle checking | \"AI p|\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"AI p|\n",
      "endidx 19\n",
      "whle checking | \"AI pa|\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"AI pa|\n",
      "endidx 20\n",
      "whle checking | \"AI par|\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"AI par|\n",
      "endidx 21\n",
      "whle checking | \"AI pars|\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"AI pars|\n",
      "endidx 22\n",
      "whle checking | \"AI parsi|\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"AI parsi|\n",
      "endidx 23\n",
      "whle checking | \"AI parsin|\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"AI parsin|\n",
      "endidx 24\n",
      "whle checking | \"AI parsing|\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"AI parsing|\n",
      "endidx 25\n",
      "whle checking | \"AI parsing |\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"AI parsing |\n",
      "endidx 26\n",
      "whle checking | \"AI parsing e|\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"AI parsing e|\n",
      "endidx 27\n",
      "whle checking | \"AI parsing ex|\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"AI parsing ex|\n",
      "endidx 28\n",
      "whle checking | \"AI parsing exa|\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"AI parsing exa|\n",
      "endidx 29\n",
      "whle checking | \"AI parsing exam|\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"AI parsing exam|\n",
      "endidx 30\n",
      "whle checking | \"AI parsing examp|\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"AI parsing examp|\n",
      "endidx 31\n",
      "whle checking | \"AI parsing exampl|\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"AI parsing exampl|\n",
      "endidx 32\n",
      "whle checking | \"AI parsing example|\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"AI parsing example|\n",
      "endidx 33\n",
      "whle checking | \"AI parsing example\"|\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"AI parsing example\"|\n",
      "endidx 34\n",
      "whle checking | \"AI parsing example\",|\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"AI parsing example\",|\n",
      "endidx 35\n",
      "whle checking | \"AI parsing example\", |\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"AI parsing example\", |\n",
      "endidx 36\n",
      "whle checking | \"AI parsing example\", }|\n",
      "pattern '\\\\\"[0-9a-fA-F]+\\\\\"+'\n",
      "matching | \"AI parsing example\", }|\n",
      "12 37\n",
      "actual_substring | \"AI parsing example\", }|\n",
      "pattern '[ \\t\\n]+'\n",
      "matching | \"AI parsing example\", }|\n",
      "worked\n",
      "matched   \"AI parsing example\", }\n",
      "DONE WITH   \"AI parsing example\", }\n",
      "-----------------------------\n",
      "stack  [('root', 0), ('thoughtstate', 4), ('string', 0)]\n"
     ]
    }
   ],
   "source": [
    "input_string = '{ \"thought\": \"AI parsing example\", }'\n",
    "\n",
    "# Define grammar rules based on the provided structure\n",
    "grammar_rules = {\n",
    "    \"root\": [Token(\"thoughtstate\", \"non-terminal\")],\n",
    "    \"thoughtstate\": [\n",
    "        Token(\"{\", \"terminal\"),\n",
    "        Token(\"ws\", \"non-terminal\"),\n",
    "        Token('\"thought\"', \"literal\", None, '\"', \":\"),\n",
    "        Token(\"ws\", \"non-terminal\"),\n",
    "        Token(\"string\", \"non-terminal\"),\n",
    "        Token(\",\", \"terminal\"),\n",
    "        Token(\"}\", \"terminal\"),\n",
    "    ],\n",
    "    \"ws\": [Token(\"[ \\t\\n]\", \"literal\", \"+\", None, None)],\n",
    "    \"string\": [Token(r\"\\\"[0-9a-fA-F]+\\\"\", \"literal\", \"+\", None, None)],\n",
    "}\n",
    "\n",
    "# Initialize the GrammarStack with the defined grammar rules\n",
    "grammar_stack = GrammarStack(grammar_rules)\n",
    "\n",
    "# Initialize the GrammarParser with the grammar stack and the input string\n",
    "parse = GrammarParser(grammar_stack, input_string)\n",
    "\n",
    "# Run the parser\n",
    "parse.parse_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(' \"thought\"', '\"thought\":')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateMismatchProbability(expected_token, input_string, current_idx, grammar_rules):\n",
    "    \"\"\"\n",
    "    Calculate the mismatch probability between the expected grammar structure and the input string.\n",
    "\n",
    "    Args:\n",
    "    expected_token (Token): The current token being considered.\n",
    "    input_string (str): The complete input string being parsed.\n",
    "    current_idx (int): The current index in the input string under consideration.\n",
    "    grammar_rules (dict): The grammar rules defining valid structures.\n",
    "\n",
    "    Returns:\n",
    "    float: A probability score indicating the likelihood of a mismatch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize mismatch probability as a float between 0 and 1.\n",
    "    mismatch_probability = 0.0\n",
    "\n",
    "    # Check for the next expected structural token based on the current token type\n",
    "    if expected_token.token_type == 'terminal':\n",
    "        # For terminals, directly check for their presence at the current index\n",
    "        if input_string[current_idx:current_idx+len(expected_token.value)] != expected_token.value:\n",
    "            mismatch_probability += 0.5  # Increase mismatch probability for missing terminal\n",
    "\n",
    "    elif expected_token.token_type == 'non-terminal':\n",
    "        # For non-terminals, look ahead to see if the next expected structure matches\n",
    "        # This requires parsing the next part of the input based on the non-terminal's definition\n",
    "\n",
    "        # next_structure = grammar_rules[expected_token.value][0]  # Simplification: consider the first token of the rule\n",
    "        lookahead_idx, found_mismatch = lookahead_and_evaluate_mismatch(\n",
    "            input_string, current_idx, grammar_rules)\n",
    "        if found_mismatch:\n",
    "            # Increase mismatch probability for missing non-terminal structure\n",
    "            mismatch_probability += 0.5\n",
    "\n",
    "    # Adjust probability based on specifics, like missing enclosing or assignment chars\n",
    "    if expected_token.enclosing_char and (input_string[current_idx] != expected_token.enclosing_char):\n",
    "        mismatch_probability += 0.25  # Adjust for missing enclosing char\n",
    "\n",
    "    if expected_token.assignment_char:\n",
    "        # Look for the assignment character near the current position\n",
    "        if input_string[current_idx:current_idx+2].find(expected_token.assignment_char) == -1:\n",
    "            mismatch_probability += 0.25  # Adjust for missing assignment char\n",
    "\n",
    "    # Cap the probability at 1\n",
    "    mismatch_probability = min(mismatch_probability, 1.0)\n",
    "\n",
    "    return mismatch_probability\n",
    "\n",
    "\n",
    "def lookahead_and_evaluate_mismatch(input_string, current_idx, grammar_stack):\n",
    "    expected_token = grammar_stack.next_expected_token()\n",
    "    lookahead_idx = current_idx\n",
    "    found_mismatch = False\n",
    "\n",
    "    while lookahead_idx < len(input_string) and expected_token:\n",
    "        lookahead_segment = input_string[current_idx:lookahead_idx+1]\n",
    "        if expected_token.value in lookahead_segment:\n",
    "            # Found the expected token value in the lookahead segment\n",
    "            return lookahead_idx, False  # Return the index and mismatch status\n",
    "\n",
    "        lookahead_idx += 1\n",
    "\n",
    "    # If the loop completes without finding the expected token,\n",
    "    # it indicates a potential mismatch.\n",
    "    found_mismatch = True\n",
    "\n",
    "    # Evaluate mismatch probability (simplified for demonstration)\n",
    "    # In practice, this would involve more complex logic based on grammar rules\n",
    "    # and potentially correcting the mismatch by inserting or skipping tokens.\n",
    "    if found_mismatch:\n",
    "        # Placeholder for mismatch handling logic\n",
    "        print(f\"Mismatch found at index {\n",
    "              lookahead_idx}. Expected token: {expected_token.value}\")\n",
    "\n",
    "    return lookahead_idx, found_mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(ws, non-terminal, None, None, None)\n"
     ]
    }
   ],
   "source": [
    "grammar_rules = {\n",
    "    \"root\": [Token(\"thoughtstate\", \"non-terminal\")],\n",
    "    \"thoughtstate\": [\n",
    "        Token(\"{\", \"terminal\"),\n",
    "        Token(\"ws\", \"non-terminal\"),\n",
    "        Token(\"string\", \"non-terminal\"),\n",
    "        Token(\":\", \"terminal\"),\n",
    "        Token(\"string\", \"non-terminal\"),\n",
    "        Token(\"}\", \"terminal\"),\n",
    "    ],\n",
    "    \"ws\": [Token(\"[ \\t\\n]\", \"literal\", \"*\")],\n",
    "    # Simplified regex-like representation for demonstration\n",
    "    \"string\": [Token('\"[^\"]*\"', \"literal\")],\n",
    "}\n",
    "\n",
    "# Sample input string\n",
    "input_string = '{\"key\": \"value\"}'\n",
    "\n",
    "# Assume we're starting at the beginning of the string, looking for the thoughtstate structure\n",
    "current_idx = 1\n",
    "\n",
    "# The expected token at the beginning of a thoughtstate (assuming we're parsing a thoughtstate structure)\n",
    "# This should be the \"{\" token\n",
    "expected_token = grammar_rules[\"thoughtstate\"][1]\n",
    "print(expected_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch found at index 16. Expected token: {\n",
      "Mismatch Probability: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Call the function with the sample fields\n",
    "mismatch_probability = calculateMismatchProbability(\n",
    "    expected_token, input_string, current_idx, stack\n",
    ")\n",
    "\n",
    "# Print the mismatch probability\n",
    "print(f\"Mismatch Probability: {mismatch_probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" \n",
    "```\n",
    "{\n",
    "\"ThoughtState\": {\n",
    "\"thought\": \"Vladimir Putin is the current President of Russia.\",\n",
    "\"goal\": \"To provide information about Vladimir Putin.\",\n",
    "\"tool\": \"Web_Search\",\n",
    "\"action\": \"Read\",\n",
    "\"action_input\": \"Vladimir Putin biography\",\n",
    "\"thought_id\": \"12345\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m bot_input_ids \u001b[38;5;241m=\u001b[39m new_user_input_ids\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# generated a response while limiting the total chat history to 1000 tokens, \u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m chat_history_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbot_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# pretty print last output tokens from bot\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDialoGPT: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(tokenizer\u001b[38;5;241m.\u001b[39mdecode(chat_history_ids[:, bot_input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:][\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/transformers/generation/utils.py:1544\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1527\u001b[0m         input_ids,\n\u001b[1;32m   1528\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1541\u001b[0m     )\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1543\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/transformers/generation/utils.py:2404\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2401\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2403\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2404\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2405\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2407\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2408\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2409\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2412\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    877\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         output_attentions,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:427\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    425\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    426\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 427\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[1;32m    429\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:354\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[0;32m--> 354\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_fc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[1;32m    356\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/transformers/pytorch_utils.py:103\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    102\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[0;32m--> 103\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/DialoGPT-medium\", padding_side=\"left\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "# source: https://huggingface.co/microsoft/DialoGPT-medium\n",
    "\n",
    "# encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "new_user_input_ids = tokenizer.encode(\n",
    "    \"Can you generate a simple JSON object?\" + tokenizer.eos_token, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# append the new user input tokens to the chat history\n",
    "bot_input_ids = new_user_input_ids\n",
    "\n",
    "# generated a response while limiting the total chat history to 1000 tokens,\n",
    "chat_history_ids = model.generate(\n",
    "    bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# pretty print last output tokens from bot\n",
    "print(\n",
    "    \"DialoGPT: {}\".format(\n",
    "        tokenizer.decode(\n",
    "            chat_history_ids[:, bot_input_ids.shape[-1] :][0], skip_special_tokens=True\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_tokens_ids = [\n",
    "    tokenizer.encode(token)[0] for token in [\"{\", \"}\", '\"', \":\", \",\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_boost_factor = 1.1  # Slightly boost content token probabilities\n",
    "structure_boost_factor = 5  # Significantly boost terminal token probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (0.27.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: huggingface-hub in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from accelerate) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: pyyaml in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from accelerate) (2.1.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from accelerate) (0.4.1)\n",
      "Requirement already satisfied: psutil in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: sympy in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: typing-extensions in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: networkx in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: fsspec in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.18.1)\n",
      "Requirement already satisfied: filelock in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n",
      "Requirement already satisfied: requests in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f395301997714d99b6bf9087ae2bf351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42217c946c644fc585267cfea6ff0c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00019.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mixtral-8x7B-v0.1\", device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
    "\n",
    "prompt = \"My favourite condiment is\"\n",
    "\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': 'value',\n",
       " 'array': [1, 2, 3],\n",
       " 'bro': {'ayo': 'huh'},\n",
       " 'nested': {'another_key': 'another_value'}}"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_string = r'{\"key\": \"value\", \"array\": [1, 2, 3], \"bro\": {\"ayo\": \"huh\"}, \"nested\": {\"another_key\": \"another_value\"}} ohhhh'\n",
    "JSON.parse(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'products': [{'name': 'Hammer', 'sku': 738594937}, {}, {'name': 'Nail', 'sku': 284758393, 'color': 'gray'}], 'products.sub': [{'hey': 'yo'}]}\n"
     ]
    }
   ],
   "source": [
    "toml_string = \"\"\"\n",
    "[products]\n",
    "name = \"Hammer\"\n",
    "sku = 738594937\n",
    "\n",
    "[products]\n",
    "\n",
    "[products]\n",
    "name = \"Nail\"\n",
    "sku = 284758393\n",
    "color = \"gray\"\n",
    "\n",
    "[products.sub]\n",
    "hey = \"yo\"\n",
    "\"\"\"\n",
    "print(TOML.parse(toml_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"studentsList\": {\n",
      "        \"student\": {\n",
      "            \"attributes\": {\n",
      "                \"ind\": \"2\"\n",
      "            },\n",
      "            \"firstName\": {\n",
      "                \"value\": \"Wirt\"\n",
      "            },\n",
      "            \"lastName\": {\n",
      "                \"value\": \"Wood\"\n",
      "            },\n",
      "            \"certificate\": {\n",
      "                \"value\": \"True\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\" \n",
    "<studentsList>\n",
    "    <student id=\"1\">\n",
    "        <firstName>Greg</firstName>\n",
    "        <lastName>Dean</lastName>\n",
    "        <certificate>True</certificate>\n",
    "        <scores>\n",
    "            <module1>70</module1>\n",
    "            <module12>80</module12>\n",
    "            <module3>90</module3>\n",
    "        </scores>\n",
    "    </student>\n",
    "    <student ind=\"2\">\n",
    "        <firstName>Wirt</firstName>\n",
    "        <lastName>Wood</lastName>\n",
    "        <certificate>True</certificate>\n",
    "    </student>\n",
    "</studentsList>\n",
    "\"\"\"\n",
    "\n",
    "text = \"\"\" \n",
    "<studentsList>\n",
    "    <student ind=\"2\">\n",
    "        <firstName>Wirt</firstName>\n",
    "        <lastName>Wood</lastName>\n",
    "        <certificate>True</certificate>\n",
    "    </student>\n",
    "</studentsList>\n",
    "\"\"\"\n",
    "\n",
    "print(json.dumps(parse_xml(text), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex:\n",
      "```\n",
      "<Complex>\n",
      "<real> #float# </real>\n",
      "<imaginary> #float# </imaginary>\n",
      "</Complex>\n",
      "```\n",
      "\n",
      "\n",
      "Complex:\n",
      "```\n",
      "[Complex]\n",
      "real = # Type: float\n",
      "imaginary = # Type: float\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "{\n",
      "\"Complex\": {\n",
      "real = # Type: float\n",
      "imaginary = # Type: float\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Complex(BaseModel):\n",
    "    real: float\n",
    "    imaginary: float\n",
    "\n",
    "\n",
    "print(XML.make_format([{\"model\": Complex}], \"single\")[0])\n",
    "print(TOML.make_format([{\"model\": Complex}], \"single\")[0])\n",
    "print(JSON.make_format([{\"model\": Complex}], \"single\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TOML:\n",
    "    @staticmethod\n",
    "    def make_format(grammars: List[dict], return_sequence: str) -> str:\n",
    "        grammar, instruct = \"\", []\n",
    "        for task in grammars:\n",
    "            model = task.get(\"model\")\n",
    "            command = task.get(\"task_name\", \"\")\n",
    "            if isinstance(model, list):\n",
    "                name = \"_\".join([m.__name__ for m in model])\n",
    "            else:\n",
    "                name = model.__name__\n",
    "                model = [model]\n",
    "            instruct.append(name)\n",
    "\n",
    "            fields = ModelParser.extract_fields_with_descriptions(model)\n",
    "            forma = TOML.generate_prompt_from_fields(fields, nested=True)\n",
    "            grammar += f\"{name}:\\n```\\n{forma}\\n```\\n\\n\"\n",
    "\n",
    "        return grammar, instruct\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_prompt_from_fields(fields_info: dict, nested: bool = False) -> str:\n",
    "        prompt_lines = []\n",
    "        for model_name, fields in fields_info.items():\n",
    "            if nested:\n",
    "                prompt_lines.append(f\"[{model_name}]\")\n",
    "            for var_name, details in fields.items():\n",
    "                line = f\"{var_name} = \"\n",
    "                if details.get(\"description\"):\n",
    "                    line += f'\"{details[\"description\"]}\"'\n",
    "                line += f'# Type: {details[\"type\"]}'\n",
    "                if str(details.get(\"default\")) not in [\"PydanticUndefined\", \"None\"]:\n",
    "                    line += f', Default: \"{details[\"default\"]}\"'\n",
    "                prompt_lines.append(line)\n",
    "        return \"\\n\".join(prompt_lines)\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_single_model_prompt(\n",
    "        fields: dict, model_name: str, nested: bool = False\n",
    "    ) -> dict:\n",
    "        model_data = {}\n",
    "        for var_name, details in fields.items():\n",
    "            model_data[var_name] = {\n",
    "                \"description\": details[\"description\"],\n",
    "                \"type\": details[\"type\"],\n",
    "                \"default\": (\n",
    "                    details[\"default\"]\n",
    "                    if str(details.get(\"default\")) != \"PydanticUndefined\"\n",
    "                    else None\n",
    "                ),\n",
    "            }\n",
    "        return model_data\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_toml(toml_string):\n",
    "        def parse_section(toml_string, i):\n",
    "            start = i\n",
    "            while toml_string[i] != \"]\":\n",
    "                i += 1\n",
    "            key = toml_string[start:i]\n",
    "            i = skip_whitespace(toml_string, i + 1)\n",
    "            section = {key: {}}\n",
    "            while i < len(toml_string) and toml_string[i] not in \"[\":\n",
    "                subkey, i = parse_key(toml_string, i)\n",
    "                i = skip_whitespace(toml_string, i)\n",
    "                if toml_string[i] == \"=\":\n",
    "                    i = skip_whitespace(toml_string, i + 1)\n",
    "                    value, i = parse_value(toml_string, i)\n",
    "                    section[key][subkey.replace(\"\\n\", \"\")] = value\n",
    "                i = skip_whitespace(toml_string, i)\n",
    "            return section, i\n",
    "\n",
    "        def parse_key(toml_string, i):\n",
    "            start = i\n",
    "            while toml_string[i] not in \"=\":\n",
    "                i += 1\n",
    "            return toml_string[start:i], i\n",
    "\n",
    "        def parse_value(toml_string, i):\n",
    "            if toml_string[i] == '\"':\n",
    "                print(\"starting string\")\n",
    "                return parse_string(toml_string, i + 1)\n",
    "            elif toml_string[i] == \"[\":\n",
    "                print(\"starting array\")\n",
    "                return parse_array(toml_string, i)\n",
    "            else:\n",
    "                return parse_number(toml_string, i)\n",
    "\n",
    "        def parse_string(toml_string, i):\n",
    "            start = i\n",
    "            while toml_string[i] != '\"':\n",
    "                print(\"char\", toml_string[i])\n",
    "                i += 1\n",
    "            print(\"string\", toml_string[start:i])\n",
    "            return toml_string[start:i], i\n",
    "\n",
    "        def parse_number(toml_string, i):\n",
    "            start = i\n",
    "            while toml_string[i] in \"0123456789.-\":\n",
    "                i += 1\n",
    "\n",
    "            val = toml_string[start:i]\n",
    "            try:\n",
    "                return int(val), i\n",
    "            except ValueError:\n",
    "                return \"\", i\n",
    "\n",
    "        def parse_array(toml_string, i):\n",
    "            array = []\n",
    "            i = skip_whitespace(toml_string, i + 1)\n",
    "            while toml_string[i] != \"]\":\n",
    "                print(\"array\", toml_string[:i])\n",
    "                value, i = parse_value(toml_string, i)\n",
    "                array.append(value)\n",
    "                i = skip_whitespace(toml_string, i + 1)\n",
    "            array = [x for x in array if x]\n",
    "            return array, i + 1\n",
    "\n",
    "        def skip_whitespace(toml_string, i):\n",
    "            while i < len(toml_string) and toml_string[i] in \" \\t\\n\\r\":\n",
    "                print(\"skipped IN CONSIDERED\", toml_string[i])\n",
    "                i += 1\n",
    "            print(\"skipped\", toml_string[:i])\n",
    "            return i\n",
    "\n",
    "        i = 0\n",
    "        storage = {}\n",
    "        while i < len(toml_string):\n",
    "            i = skip_whitespace(toml_string, i)\n",
    "            if i < len(toml_string) and toml_string[i] == \"[\":\n",
    "                section, i = parse_section(toml_string, i + 1)\n",
    "                key = list(section.keys())[0]\n",
    "                if key in storage:\n",
    "                    storage[key].append(section[key])\n",
    "                else:\n",
    "                    storage[key] = [section[key]]\n",
    "            else:\n",
    "                break\n",
    "        return storage\n",
    "\n",
    "    @staticmethod\n",
    "    def parse(text):\n",
    "        return TOML.parse_toml(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped IN CONSIDERED  \n",
      "skipped IN CONSIDERED \n",
      "\n",
      "skipped  \n",
      "\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\" \n",
    "Man\n",
    "[BudgetPlan]\n",
    "total_cost = 500\n",
    "all_items = [\"cake\", \"balloons\", \"roses\", \"ice cream\"]\n",
    "[EventSchedule]\n",
    "start_time = \"12:00 PM\"\n",
    "end_time = \"4:00 PM\"\n",
    "activities = [\"cake cutting\", \"balloon decoration\", \"rose gifting\", \"ice cream party\"]\n",
    "\"\"\"\n",
    "\n",
    "print(TOML.parse(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def tokenize_and_count_hf(text, model_name=\"bert-base-uncased\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# tokens, count = tokenize_and_count_hf(, 'Hello, world!')\n",
    "# print(f'Tokens: {tokens}, Count: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = \"\"\" \n",
    "```\n",
    "{\n",
    "\"ThoughtState\": {\n",
    "\"thought\": \"Vladimir Putin is the current President of Russia.\",\n",
    "\"goal\": \"To provide information about Vladimir Putin.\",\n",
    "\"tool\": \"Web_Search\",\n",
    "\"action\": \"Read\",\n",
    "\"action_input\": \"Vladimir Putin biography\",\n",
    "\"thought_id\": \"12345\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "expected_tokens = \"\"\" \n",
    "{\n",
    "\"ThoughtState\": {\n",
    "\"thought\": \"\",\n",
    "\"goal\": \"\",\n",
    "\"tool\": \"\",\n",
    "\"action\": \"\",\n",
    "\"action_input\": \"\",\n",
    "\"thought_id\": \"\"\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder: \n",
    "    def __init__(self, expected): \n",
    "        self.encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "        self.expected_tokens  = self.tokenize(expected)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return self.encoding.encode(text)\n",
    "        \n",
    "    def get_next_states(current_state, tokens, expected_tokens):\n",
    "    # Initialize probabilities for each token in the input string\n",
    "    token_probabilities = [0] * len(tokens)  # Placeholder for actual probability calculation\n",
    "\n",
    "    # Determine the type of the next expected token (terminal or non-terminal)\n",
    "    next_expected_token_type = get_token_type(expected_tokens, current_state)\n",
    "\n",
    "    # Calculate probabilities for each token in the input string\n",
    "    for i, token in enumerate(tokens):\n",
    "        token_type = 'terminal' if token in expected_tokens else 'non-terminal'\n",
    "        if token_type == next_expected_token_type:\n",
    "            # Assign higher probability if the token matches the expected type\n",
    "            token_probabilities[i] += calculate_probability(token, i, tokens, expected_tokens)\n",
    "\n",
    "    # Choose the most probable next token\n",
    "    next_token_index = token_probabilities.index(max(token_probabilities))\n",
    "    next_token = tokens[next_token_index]\n",
    "\n",
    "    return next_token\n",
    "    \n",
    "    def get_next_states(self, current_state, tokens):\n",
    "        token_probabilities = [self.calculate_probability(token, i, tokens) for i, token in enumerate(tokens)]\n",
    "        # Choose the token with the highest probability score as the most probable next state\n",
    "        next_token_index = token_probabilities.index(max(token_probabilities))\n",
    "        next_token = tokens[next_token_index]\n",
    "        return next_token, next_token_index\n",
    "\n",
    "    def decode(self, input_string):\n",
    "        tokens = self.tokenize(input_string)\n",
    "        path = []\n",
    "        current_state = 'root'  # Starting state\n",
    "        token_index = 0\n",
    "\n",
    "        while token_index < len(tokens):\n",
    "            current_token = tokens[token_index]\n",
    "            next_token, next_token_index = self.get_next_states(current_state, tokens[token_index:])\n",
    "            path.append(next_token)\n",
    "            token_index += next_token_index + 1  # Move to the index of the next token\n",
    "\n",
    "            # Update current_state based on your state transition logic\n",
    "            # Placeholder: current_state = next_token\n",
    "            current_state = next_token\n",
    "\n",
    "            if current_state == 'complete':\n",
    "                break  # Terminate if the decoding process is complete\n",
    "\n",
    "        return ' '.join(path)\n",
    "\n",
    "    def calculate_probability(token, index, tokens, expected_tokens):\n",
    "        \"\"\"\n",
    "        Calculates a probability score for a token based on its match with expected type,\n",
    "        its closeness to the expected position, and its fit within the expected sequence.\n",
    "        \n",
    "        :param token: The current token being evaluated.\n",
    "        :param index: The index of the current token in the input tokens list.\n",
    "        :param tokens: The list of all tokens in the input string.\n",
    "        :param expected_tokens: The list of tokens in the expected format, including placeholders for non-terminal tokens.\n",
    "        :return: A probability score for the token.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Constants for weighting different factors\n",
    "        TYPE_WEIGHT = 0.5\n",
    "        POSITION_WEIGHT = 0.3\n",
    "        SEQUENCE_WEIGHT = 0.2\n",
    "\n",
    "        # Determine if the token is terminal or non-terminal\n",
    "        is_terminal = token in expected_tokens\n",
    "        type_score = TYPE_WEIGHT if is_terminal else 0\n",
    "\n",
    "        # Calculate positional closeness score\n",
    "        # Assuming expected position is based on the order in expected_tokens\n",
    "        if token in expected_tokens:\n",
    "            expected_index = expected_tokens.index(token)\n",
    "            position_diff = abs(index - expected_index)\n",
    "        else:\n",
    "            # If it's a non-terminal, we find the closest terminal token's expected position\n",
    "            closest_terminal_index = min([i for i, t in enumerate(tokens) if t in expected_tokens], key=lambda x: abs(x - index))\n",
    "            expected_index = expected_tokens.index(tokens[closest_terminal_index])\n",
    "            position_diff = abs(index - expected_index)\n",
    "        \n",
    "        # Normalize position_diff to a score between 0 and 1, assuming max diff is the length of tokens\n",
    "        max_diff = len(tokens)\n",
    "        positional_score = POSITION_WEIGHT * (1 - (position_diff / max_diff))\n",
    "\n",
    "        # Calculate sequence fit score\n",
    "        # This simplistic approach checks if the next expected token matches the next actual token\n",
    "        # More complex logic could involve checking subsequences\n",
    "        if index + 1 < len(tokens) and tokens[index + 1] in expected_tokens:\n",
    "            next_expected_token = expected_tokens[expected_tokens.index(token) + 1] if token in expected_tokens else None\n",
    "            sequence_fit_score = SEQUENCE_WEIGHT if tokens[index + 1] == next_expected_token else 0\n",
    "        else:\n",
    "            sequence_fit_score = 0\n",
    "\n",
    "        # Combine scores for final probability\n",
    "        probability_score = type_score + positional_score + sequence_fit_score\n",
    "        return probability_score\n",
    "\n",
    "    def get_token_type(expected_tokens, current_state):\n",
    "        if current_state in expected_tokens:\n",
    "            return 'terminal'\n",
    "        else:\n",
    "            return 'non-terminal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m decoder \u001b[38;5;241m=\u001b[39m Decoder(expected_tokens)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 25\u001b[0m, in \u001b[0;36mDecoder.decode\u001b[0;34m(self, input_string)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m token_index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens):\n\u001b[1;32m     24\u001b[0m     current_token \u001b[38;5;241m=\u001b[39m tokens[token_index]\n\u001b[0;32m---> 25\u001b[0m     next_token, next_token_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken_index\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     path\u001b[38;5;241m.\u001b[39mappend(next_token)\n\u001b[1;32m     27\u001b[0m     token_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m next_token_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Move to the index of the next token\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[37], line 11\u001b[0m, in \u001b[0;36mDecoder.get_next_states\u001b[0;34m(self, current_state, tokens)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_next_states\u001b[39m(\u001b[38;5;28mself\u001b[39m, current_state, tokens):\n\u001b[0;32m---> 11\u001b[0m     token_probabilities \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_probability(token, i, tokens) \u001b[38;5;28;01mfor\u001b[39;00m i, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens)]\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Choose the token with the highest probability score as the most probable next state\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     next_token_index \u001b[38;5;241m=\u001b[39m token_probabilities\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mmax\u001b[39m(token_probabilities))\n",
      "Cell \u001b[0;32mIn[37], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_next_states\u001b[39m(\u001b[38;5;28mself\u001b[39m, current_state, tokens):\n\u001b[0;32m---> 11\u001b[0m     token_probabilities \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_probability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens)]\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Choose the token with the highest probability score as the most probable next state\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     next_token_index \u001b[38;5;241m=\u001b[39m token_probabilities\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mmax\u001b[39m(token_probabilities))\n",
      "Cell \u001b[0;32mIn[37], line 66\u001b[0m, in \u001b[0;36mDecoder.calculate_probability\u001b[0;34m(token, index, tokens, expected_tokens)\u001b[0m\n\u001b[1;32m     63\u001b[0m     position_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(index \u001b[38;5;241m-\u001b[39m expected_index)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# If it's a non-terminal, we find the closest terminal token's expected position\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     closest_terminal_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m([i \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m expected_tokens], key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mabs\u001b[39m(x \u001b[38;5;241m-\u001b[39m index))\n\u001b[1;32m     67\u001b[0m     expected_index \u001b[38;5;241m=\u001b[39m expected_tokens\u001b[38;5;241m.\u001b[39mindex(tokens[closest_terminal_index])\n\u001b[1;32m     68\u001b[0m     position_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(index \u001b[38;5;241m-\u001b[39m expected_index)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(expected_tokens)\n",
    "decoder.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = tokenize_and_count_tiktoken(expected_tokens)\n",
    "real = tokenize_and_count_tiktoken(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_states(0, real, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real[2].decode([72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[720,\n",
       " 14196,\n",
       " 4077,\n",
       " 517,\n",
       " 1,\n",
       " 85269,\n",
       " 1423,\n",
       " 794,\n",
       " 341,\n",
       " 1,\n",
       " 61665,\n",
       " 794,\n",
       " 330,\n",
       " 53,\n",
       " 18599,\n",
       " 31204,\n",
       " 21810,\n",
       " 374,\n",
       " 279,\n",
       " 1510,\n",
       " 4900,\n",
       " 315,\n",
       " 8524,\n",
       " 10560,\n",
       " 1,\n",
       " 35039,\n",
       " 794,\n",
       " 330,\n",
       " 1271,\n",
       " 3493,\n",
       " 2038,\n",
       " 922,\n",
       " 36011,\n",
       " 21810,\n",
       " 10560,\n",
       " 1,\n",
       " 14506,\n",
       " 794,\n",
       " 330,\n",
       " 6109,\n",
       " 67013,\n",
       " 761,\n",
       " 1,\n",
       " 1335,\n",
       " 794,\n",
       " 330,\n",
       " 4518,\n",
       " 761,\n",
       " 1,\n",
       " 1335,\n",
       " 6022,\n",
       " 794,\n",
       " 330,\n",
       " 53,\n",
       " 18599,\n",
       " 31204,\n",
       " 21810,\n",
       " 48345,\n",
       " 761,\n",
       " 1,\n",
       " 61665,\n",
       " 851,\n",
       " 794,\n",
       " 330,\n",
       " 4513,\n",
       " 1774,\n",
       " 702,\n",
       " 262,\n",
       " 457,\n",
       " 534,\n",
       " 14196,\n",
       " 4077]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
