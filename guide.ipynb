{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Any, List, Tuple, Type, Optional, Union\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LLM:\n",
        "    def __init__(self):\n",
        "        self.client = OpenAI(\n",
        "            api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "        )\n",
        "\n",
        "    def request(self, prompt, temperature=0.2, context=None):\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "\n",
        "llm = LLM()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def log(msg):\n",
        "    print(msg)\n",
        "    print(\"-------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How to make Prompts? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from grammarflow.prompt.builder import PromptBuilder  # Prompt builder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prompts can be supplied to the `Constrain` block (below) as a string, `Prompt` or `PromptBuilder` object. \n",
        "\n",
        "`PromptBuilder` is a straightforward interface to build prompts, which are built using 'sections' which are user-defined. You can specificy which sections of the prompt can be conditionally switched on/off, which can be edited using placeholders, which are fixed sections, where the grammars can be defined, etc.\n",
        "\n",
        "Each prompt can have one `define_grammar`, `placeholders` and `add_few_shot_examples` boolean fields. These switch on certain functionalities within the sections. You can pass in a function to the `enable_on` field for the conditional enabling.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample Llama Prompt\n",
        "\n",
        "llama_prompt = PromptBuilder()\n",
        "llama_prompt.add_section(\n",
        "    text=\"<s>[INST] <<SYS>>\\n{system_context}\\n<</SYS>>\",\n",
        "    placeholders=[\"system_context\"],\n",
        "    define_grammar=True,\n",
        ")\n",
        "llama_prompt.add_section(\n",
        "    text=\"{user_message}[/INST]\",\n",
        "    placeholders=[\"user_message\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How to use `enable_on`: \n",
        "\n",
        "Say we are working on a conversation chatbot, a chain that needs history for context. We might only want certain sections to be enabled when we have a history. We can use `enable_on` to enable or disable sections based on the presence of a history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dummy Example \n",
        "\n",
        "def check_previous_interaction(id_): return id_ > 1\n",
        "\n",
        "sample_prompt = PromptBuilder() \n",
        "sample_prompt.add_section(\n",
        "    text=\"You are a intelligent search machine. Your goal is to think about what topics to search about to provide the user with relevant information. Here is his question: {question}\",\n",
        "    placeholders=['question']\n",
        ") \n",
        "sample_prompt.add_section(\n",
        "  define_grammar=True\n",
        ")\n",
        "sample_prompt.add_section(\n",
        "  text=\"Choose keywords from the context given below: \\n{history}\",\n",
        "  placeholders=[\"history\"],\n",
        "  enable_on=check_previous_interaction\n",
        ") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How to make Grammars? \n",
        "\n",
        "Using Pydantic, we can define how data should be in pure python. GrammarFlow takes care of the rest. In this guide, you will find multiple examples of pydantic models for different use-cases. (Some of them are quite random, but I'm using them to prove effectiveness!)\n",
        "\n",
        "Here are some important rules! \n",
        "\n",
        "1. You can use Optional from `typing`, but the LLM won't understand when and when not to output an optional field. From experience, there are better ways to deal with Optional fields, such as enabling it within the grammar when needed. \n",
        "2. When you want to use regex, you can do it using `Field(..., pattern=\"\", description=\"\")`. The `pattern` field will be used during decoding, but is ignored during prompt embedding. This is because LLMs cannot 100% perform regex handling unless explained in human-terms. Moreover, when you see LLMs conforming to the regex expectations, its mostly during the token sampling that it is achieved. So, to overcome this, you can embed a semantic explanation of the regex, like `Field(..., pattern=\"^(Akshath|Raghav|Ravikiran)$\", description=\"Akshath OR Raghav OR Ravikiran\")`.\n",
        "3. Avoid using `Dict` in the pydantic model directly. Instead, make another model with the Dict fields and add that as a parameter to the original model. This becomes a nested structure, and can be handled well. If you choose `Dict[str, str]` method, then all I can do is put this into the prompt, which is ineffective and random. \n",
        "\n",
        "### Important note on acceptable regex for GNBF! \n",
        "\n",
        "Ensure your regex is solely `string` or `number` constraining. \n",
        "> Example: `'(\"Akshath\"|\"Raghav\"|\"Ravikiran\")'` for multiple OR scenarios; `'\"https://\"[0-9a-fA-F]*'` for links, etc. \n",
        "\n",
        "If you want a `string` to be present, enclose it in double-qoutes. To allow for any other sequences, bound with `()` or `[]` and specify directly (`[0-9a-fA-F]*`). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Constraining with GrammarFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from grammarflow.constrain import Constrain  # Main class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A `Constrain` block acts as a context manager. It keeps track of the serialization type and the #grammars you want to handle. \n",
        "\n",
        "It offers three main functions. \n",
        "1. `.format()` to format a prompt of your choice. Pass in `placeholders` as present in the format {'placeholder': text}. `grammars` needs to be a list of {'description': , 'model': }. `examples` needs to be a list of {'query': , 'model'}. \n",
        "2. `.get_grammar()` to get the corresponding GNBF grammar of the model you pass in. \n",
        "3. `.parse()` to parse the response into a `Response` dataclass.  \n",
        "4. `.inflation_rate()` can be used to get the token size at which the latest prompt has been increased. For smaller prompts, the number is >4x. For larger prompts, the number is <2x. You will find that **with increase of prompt size, the cost you will pay per LLM call will remain the same.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_message = \"\"\n",
        "system_context = \"\"\n",
        "\n",
        "\n",
        "class Model(BaseModel):\n",
        "    model_name: str\n",
        "\n",
        "\n",
        "with Constrain('json') as manager:\n",
        "    prompt = manager.format(llama_prompt, \n",
        "                        placeholders={'user_message': user_message, 'system_context': system_context},\n",
        "                        grammars=[{'model': [Model]}]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Serialization Use-Cases! \n",
        "\n",
        "1. 'JSON' is the classic go-to. Can handle simple stuff, nested-models, complex-grammars, etc. However, a simple missing terminal ('\"', '{\") can break the sampling chain. \n",
        "2. 'XML' is the safest go-to. Can handle all cases, except multiple grammar generation (see at the end of `Examples!` section). The use of starting and ending tags is handled easily by token sampling, and errors within naming/tag is handled by my parser. \n",
        "3. 'TOML' is best when we want to get multiple grammars generated in one-go. With a smaller inflation rate of the prompt (before and after grammarflow embeddings), it can handle a longer list of fields. However, it *cannot* work for nested models. TOML nested models usually are in the form given below. From my experimentations, LLMs have a hard time conforming to this and end up generating `obj2` field within `obj1` as a `Dict` object. Is it possible? Sure. Would I trust it? No.\n",
        "  ```\n",
        "  [obj1]\n",
        "  field = \"\" \n",
        "  field2 = \"\" \n",
        "\n",
        "  [obj1.obj2]\n",
        "  field3 = \"\" \n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Examples! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```\n",
            "<FunctionModel>\n",
            "<function_name> \"fib\" </function_name>\n",
            "<docstring> \"This function returns the fibonacci sequence.\" </docstring>\n",
            "<depedencies> List[\"numpy\"] </depedencies>\n",
            "<uuid> 123456789 </uuid>\n",
            "<is_python> True </is_python>\n",
            "<code> \"def fib(n):\\n    import numpy as np\\n    fib_seq = [0, 1]\\n    for i in range(2, n):\\n        fib_seq.append(fib_seq[i-1] + fib_seq[i-2])\\n    return fib_seq\" </code>\n",
            "</FunctionModel>\n",
            "```\n",
            "-------------\n",
            "{'FunctionModel': {'function_name': 'fib', 'docstring': 'This function returns the fibonacci sequence.', 'depedencies': typing.List[ForwardRef('numpy')], 'uuid': 123456789, 'is_python': True, 'code': 'def fib(n):\\\\n    import numpy as np\\\\n    fib_seq = [0, 1]\\\\n    for i in range(2, n):\\\\n        fib_seq.append(fib_seq[i-1] + fib_seq[i-2])\\\\n    return fib_seq'}}\n",
            "-------------\n",
            "{'before': 27, 'after': 143, 'factor': '4.3x'}\n",
            "-------------\n"
          ]
        }
      ],
      "source": [
        "# # Here's a simple example of asking an LLM to make code.\n",
        "# # This can be used within coding assistants which requires extra metadata.\n",
        "\n",
        "\n",
        "class FunctionModel(BaseModel):\n",
        "    function_name: str\n",
        "    docstring: str\n",
        "    depedencies: List[str]\n",
        "    uuid: Union[float, int]\n",
        "    is_python: bool\n",
        "    code: str\n",
        "\n",
        "\n",
        "input_str = \"I want to create a function that returns the fibonacci sequence. The function should be called 'fib'. The function can use numpy.\"\n",
        "\n",
        "with Constrain('xml') as manager:\n",
        "    prompt = manager.format(input_str, grammars=[{\"description\": \"No Code Generation\", \"model\": FunctionModel}])\n",
        "\n",
        "    llm_response = llm.request(prompt, temperature=0.01)\n",
        "    log(llm_response)\n",
        "\n",
        "    response = manager.parse(llm_response)\n",
        "    log(response)\n",
        "\n",
        "    log(manager.inflation_rate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# # The response will be of `Response` type, which can be used to extract the data. If adding the parsed response to this object fails, it will return the dict itself.\n",
        "print(response.FunctionModel.is_python)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```\n",
            "<FunctionModel>\n",
            "<function_name> fib </function_name>\n",
            "<docstring> This function returns the fibonacci sequence. </docstring>\n",
            "<depedencies> ['numpy'] </depedencies>\n",
            "<uuid> 987654321.0 </uuid>\n",
            "<is_python> True </is_python>\n",
            "<code> def fib(n):\n",
            "\tif n <= 1:\n",
            "\t\treturn n\n",
            "\telse:\n",
            "\t\treturn fib(n-1) + fib(n-2) </code>\n",
            "</FunctionModel>\n",
            "```\n",
            "-------------\n",
            "{\n",
            "    \"FunctionModel\": {\n",
            "        \"function_name\": \"fib\",\n",
            "        \"docstring\": \"This function returns the fibonacci sequence.\",\n",
            "        \"depedencies\": [\n",
            "            \"numpy\"\n",
            "        ],\n",
            "        \"uuid\": 987654321.0,\n",
            "        \"is_python\": true,\n",
            "        \"code\": \"def fib(n):\\tif n <= 1:\\t\\treturn n\\telse:\\t\\treturn fib(n-1) + fib(n-2)\"\n",
            "    }\n",
            "}\n",
            "-------------\n",
            "{'before': 27, 'after': 237, 'factor': '7.8x'}\n",
            "-------------\n"
          ]
        }
      ],
      "source": [
        "# Add some examples too!\n",
        "\n",
        "Sum_Function_Model = FunctionModel(\n",
        "    function_name=\"sum\",\n",
        "    docstring=\"This function returns the sum of the input list.\",\n",
        "    depedencies=[\"numpy\"],\n",
        "    uuid=123456789,\n",
        "    is_python=True,\n",
        "    code=\"def sum(a, b):\\n\\treturn a + b\"\n",
        ")\n",
        "\n",
        "\n",
        "with Constrain('xml') as manager:\n",
        "    prompt = manager.format(input_str, \n",
        "        grammars=[\n",
        "            {\n",
        "                'description': 'No Code Generation',\n",
        "                'model': FunctionModel\n",
        "            }\n",
        "        ],\n",
        "        examples=[\n",
        "            {\n",
        "                'query': \"Create a summation function in Python\",\n",
        "                'model': Sum_Function_Model\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "    llm_response = llm.request(prompt, temperature=0.01)\n",
        "    log(llm_response)\n",
        "\n",
        "    response = manager.parse(llm_response)\n",
        "    log(response)\n",
        "\n",
        "    log(manager.inflation_rate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```\n",
            "{\n",
            "\"ThoughtState\": {\n",
            "\"thought\": \"I need to gather information about Vladmir Putin.\",\n",
            "\"goal\": \"To understand who Vladmir Putin is.\",\n",
            "\"tool\": \"Web_Search\",\n",
            "\"action\": \"Read\",\n",
            "\"action_input\": \"Vladmir Putin biography\",\n",
            "\"thought_id\": \"1a2b3c4d\"\n",
            "}\n",
            "}\n",
            "```\n",
            "-------------\n",
            "{\n",
            "    \"ThoughtState\": {\n",
            "        \"thought\": \"I need to gather information about Vladmir Putin.\",\n",
            "        \"goal\": \"To understand who Vladmir Putin is.\",\n",
            "        \"tool\": \"Web_Search\",\n",
            "        \"action\": \"Read\",\n",
            "        \"action_input\": \"Vladmir Putin biography\",\n",
            "        \"thought_id\": \"1a2b3c4d\"\n",
            "    }\n",
            "}\n",
            "-------------\n",
            "{'before': 48, 'after': 230, 'factor': '3.8x'}\n",
            "-------------\n"
          ]
        }
      ],
      "source": [
        "# Sample ReAct Model with Llama Prompt\n",
        "# You can add descriptions within the grammar model to provide it's context and options. This is how we use the LLM in https://github.com/e-lab/Forestry_Student/.\n",
        "\n",
        "class ThoughtState(BaseModel):\n",
        "    thought: str\n",
        "    goal: str\n",
        "    tool: str = Field(...,\n",
        "                      description=\"Choose one of ['Web_QA', 'Web_Search', 'Web_Scraping', 'Web_Automation', 'Web_Research']\")\n",
        "    action: str = Field(...,\n",
        "                        description=\"Choose one of ['Create', 'Update', 'Delete', 'Read']\")\n",
        "    action_input: str = Field(..., description=\"The input data for the action\")\n",
        "    thought_id: Optional[str] = Field(\n",
        "        None, description=\"The unique identifier for the thought\")\n",
        "\n",
        "\n",
        "system_context = \"\"\"Your goal is to think and plan out how to solve questions using agent tools provided to you. Think about all aspects of your thought process.\"\"\"\n",
        "user_message = \"\"\"Who is Vladmir Putin?\"\"\"\n",
        "\n",
        "with Constrain('json') as manager:\n",
        "    prompt = manager.format(llama_prompt, placeholders={\n",
        "                          'user_message': user_message,\n",
        "                          'system_context': system_context\n",
        "                          },\n",
        "                          grammars=[{\n",
        "                              'description': 'This format describes your current thinking state',\n",
        "                              'model': [ThoughtState]}]\n",
        "    )\n",
        "\n",
        "    llm_response = llm.request(prompt, temperature=0.01)\n",
        "    log(llm_response)\n",
        "\n",
        "    response = manager.parse(llm_response)\n",
        "    log(response)\n",
        "\n",
        "    log(manager.inflation_rate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Web_Search\n"
          ]
        }
      ],
      "source": [
        "# # You can then access the response from the `response` object\n",
        "print(response.ThoughtState.tool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```\n",
            "{\n",
            "\"Project\": {\n",
            "\"name\": \"Multimodal Document Understanding Project\",\n",
            "\"description\": \"A project focused on developing a system that can understand and analyze documents using multiple modes of input such as text, images, and audio.\",\n",
            "\"project_url\": \"https://example.com/multimodal-document-understanding\",\n",
            "\"team_members\": [\n",
            "{\n",
            "\"name\": \"John Doe\",\n",
            "\"role\": \"Project Manager\"\n",
            "},\n",
            "{\n",
            "\"name\": \"Jane Smith\",\n",
            "\"role\": \"Lead Developer\"\n",
            "},\n",
            "{\n",
            "\"name\": \"Alice Johnson\",\n",
            "\"role\": \"Data Scientist\"\n",
            "}\n",
            "],\n",
            "\"task\": {\n",
            "\"title\": \"Research existing multimodal document understanding systems\",\n",
            "\"description\": \"Conduct a literature review and analyze current state-of-the-art systems in the field.\",\n",
            "\"assigned_to\": \"John Doe\",\n",
            "\"due_date\": [\"2022-10-15\"]\n",
            "}\n",
            "}\n",
            "}\n",
            "```\n",
            "-------------\n",
            "{\n",
            "    \"Project\": {\n",
            "        \"name\": \"Multimodal Document Understanding Project\",\n",
            "        \"description\": \"A project focused on developing a system that can understand and analyze documents using multiple modes of input such as text, images, and audio.\",\n",
            "        \"project_url\": \"https://example.com/multimodal-document-understanding\",\n",
            "        \"team_members\": [\n",
            "            {\n",
            "                \"name\": \"John Doe\",\n",
            "                \"role\": \"Project Manager\"\n",
            "            },\n",
            "            {\n",
            "                \"name\": \"Jane Smith\",\n",
            "                \"role\": \"Lead Developer\"\n",
            "            },\n",
            "            {\n",
            "                \"name\": \"Alice Johnson\",\n",
            "                \"role\": \"Data Scientist\"\n",
            "            }\n",
            "        ],\n",
            "        \"task\": {\n",
            "            \"title\": \"Research existing multimodal document understanding systems\",\n",
            "            \"description\": \"Conduct a literature review and analyze current state-of-the-art systems in the field.\",\n",
            "            \"assigned_to\": \"John Doe\",\n",
            "            \"due_date\": [\n",
            "                \"2022-10-15\"\n",
            "            ]\n",
            "        }\n",
            "    }\n",
            "}\n",
            "-------------\n",
            "{'before': 69, 'after': 263, 'factor': '2.8x'}\n",
            "-------------\n",
            "```\n",
            "<Project>\n",
            "<name> \"Multimodal Document Understanding Project\" </name>\n",
            "<description> \"This project aims to develop a system that can understand and analyze documents using multiple modes of input such as text, images, and audio.\" </description>\n",
            "<project_url> \"https://www.multimodalproject.com\" </project_url>\n",
            "<team_members> \n",
            "    <TeamMember>\n",
            "        <name> \"John Doe\" </name>\n",
            "        <role> \"Project Manager\" </role>\n",
            "    </TeamMember>\n",
            "    <TeamMember>\n",
            "        <name> \"Jane Smith\" </name>\n",
            "        <role> \"Lead Developer\" </role>\n",
            "    </TeamMember>\n",
            "    <TeamMember>\n",
            "        <name> \"Michael Johnson\" </name>\n",
            "        <role> \"Data Scientist\" </role>\n",
            "    </TeamMember>\n",
            "</team_members>\n",
            "<task> \n",
            "    <title> \"Data Collection and Preprocessing\" </title>\n",
            "    <description> \"Collect and preprocess text, image, and audio data for training the multimodal document understanding system.\" </description>\n",
            "    <assigned_to> \"Michael Johnson\" </assigned_to>\n",
            "    <due_date> List[\"2022-10-15\"] </due_date>\n",
            "</task>\n",
            "</Project>\n",
            "```\n",
            "-------------\n",
            "{'Project': {'name': 'Multimodal Document Understanding Project', 'description': 'This project aims to develop a system that can understand and analyze documents using multiple modes of input such as text, images, and audio.', 'project_url': 'https://www.multimodalproject.com', 'team_members': {'TeamMember': [{'name': 'John Doe', 'role': 'Project Manager'}, {'name': 'Jane Smith', 'role': 'Lead Developer'}, {'name': 'Michael Johnson', 'role': 'Data Scientist'}]}, 'task': {'title': 'Data Collection and Preprocessing', 'description': 'Collect and preprocess text, image, and audio data for training the multimodal document understanding system.', 'assigned_to': 'Michael Johnson', 'due_date': typing.List[ForwardRef('2022-10-15')]}}}\n",
            "-------------\n",
            "{'before': 69, 'after': 252, 'factor': '2.7x'}\n",
            "-------------\n"
          ]
        }
      ],
      "source": [
        "# You can add complex layers of grammars. You add even using Optional and Union types.\n",
        "# For complex and nested grammars, JSON and XML are the best formats to use. \n",
        "\n",
        "class TeamMember(BaseModel):\n",
        "    name: str\n",
        "    role: str\n",
        "\n",
        "class Task(BaseModel):\n",
        "    title: str\n",
        "    description: str\n",
        "    assigned_to: str = Field(..., pattern=\"^(Akshath|Raghav|Ravikiran)$\")\n",
        "    due_date: List[str]\n",
        "\n",
        "class Project(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    project_url: Optional[str] = None\n",
        "    team_members: List[TeamMember]\n",
        "    task: Task\n",
        "\n",
        "for serialization in ['json', 'xml']:\n",
        "    with Constrain(serialization) as manager:\n",
        "        system_context = \"\"\"You are a project manager and you are responsible for managing a project. You have to manage the project, it's grammars and other aspects. Ensure that you fill out all required fields.\"\"\"\n",
        "        user_message = \"\"\"Make me a project plan for a new project on multimodal document understanding projct.\"\"\"\n",
        "\n",
        "        prompt = manager.format(llama_prompt, placeholders={'user_message': user_message,\n",
        "                            'system_context': system_context},\n",
        "                            grammars=[{\n",
        "                                'description': 'This format elaborates on the project and its grammars.',\n",
        "                                'model': [Project]},\n",
        "        ]\n",
        "        )\n",
        "\n",
        "        llm_response = llm.request(prompt, temperature=0.01)\n",
        "        log(llm_response)\n",
        "\n",
        "        response = manager.parse(llm_response)\n",
        "        log(response)\n",
        "\n",
        "        log(manager.inflation_rate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "``` \n",
            "[EventIdea]\n",
            "event_name = \"Summer BBQ Party\"\n",
            "event_description = \"A fun outdoor gathering with delicious food and games\"\n",
            "event_duration = \"4 hours\"\n",
            "```\n",
            "\n",
            "``` \n",
            "[BudgetPlan]\n",
            "budget = 500\n",
            "items = [\"Food\", \"Drinks\", \"Games\"]\n",
            "prices = [200, 100, 200]\n",
            "total_cost = 500\n",
            "[EventSchedule]\n",
            "event_name = \"Summer BBQ Party\"\n",
            "event_time = 12:00 PM\n",
            "event_duration = \"4 hours\"\n",
            "```\n",
            "-------------\n",
            "[{'EventIdea': [{'event_name': 'Summer BBQ Party', 'event_description': 'A fun outdoor gathering with delicious food and games', 'event_duration': '4 hours'}]}, {'BudgetPlan': [{'budget': 500, 'items': ['Food', 'Drinks', 'Games'], 'prices': [200, 100, 200], 'total_cost': 500}], 'EventSchedule': [{'event_name': 'Summer BBQ Party', 'event_time': 12, ':00 PMevent_duration': '4 hours'}]}]\n",
            "-------------\n",
            "{'before': 22, 'after': 173, 'factor': '6.9x'}\n",
            "-------------\n"
          ]
        }
      ],
      "source": [
        "# You can add multiple grammars to the same prompt. NOT RECOMMENDED.\n",
        "# If you wish to do so, generally, TOML and JSON are the best formats to use.\n",
        "\n",
        "class EventIdea(BaseModel):\n",
        "    event_name: str\n",
        "    event_description: str\n",
        "    event_duration: str\n",
        "\n",
        "class BudgetPlan(BaseModel):\n",
        "    budget: float\n",
        "    items: List[str]\n",
        "    prices: List[int]\n",
        "    total_cost: int\n",
        "\n",
        "class EventSchedule(BaseModel):\n",
        "    event_name: str\n",
        "    event_time: float\n",
        "    event_duration: str\n",
        "\n",
        "prompt = \"I am hosting a birthday party for my girlfriend tomorrow. I want to buy a cake, balloons, some roses and ice cream. I have a budget of 500$. Can you create a sample event schedule and budget plan for me?.\"\n",
        "\n",
        "with Constrain('toml', 'multi_response') as manager:\n",
        "    prompt = manager.format(llama_prompt, \n",
        "        grammars=[\n",
        "            {\"task_description\": \"Brainstorming Event Ideas\", \"model\": EventIdea},\n",
        "            {\n",
        "                \"task_description\": \"Budget Planning And Activity Planning\",\n",
        "                \"model\": [BudgetPlan, EventSchedule],\n",
        "            },\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    llm_response = llm.request(prompt, temperature=0.01)\n",
        "    log(llm_response)\n",
        "\n",
        "    response = manager.parse(llm_response)\n",
        "    log(response)\n",
        "\n",
        "    log(manager.inflation_rate())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Grammars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> \"GBNF (GGML BNF) is a format for defining formal grammars to constrain model outputs in llama.cpp. For example, you can use it to force the model to generate valid JSON, or speak only in emojis.\"\n",
        "\n",
        "Read more about it here: https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "from grammarflow.grammars.gnbf import GNBF\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional, List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TeamMember(BaseModel):\n",
        "    name: str\n",
        "    role: str\n",
        "\n",
        "class TaskUpdate(BaseModel):\n",
        "    update_time: float\n",
        "    comment: Optional[str] = None\n",
        "    status: bool\n",
        "\n",
        "class Task(BaseModel):\n",
        "    title: str\n",
        "    description: str\n",
        "    assigned_to: str = Field(..., pattern='(\"Akshath\"|\"Raghav\"|\"Ravikiran\")')\n",
        "    due_date: List[str]\n",
        "    updates: List[TaskUpdate]\n",
        "\n",
        "class Project(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    project_url: Optional[str] = Field(None, pattern='\"https://\"[0-9a-fA-F]*')\n",
        "    team_members: List[TeamMember]  \n",
        "    task: Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "grammar = GNBF(Project).generate_grammar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root ::= ws Project\n",
            "Project ::= nl \"{\" \"\\\"Project\\\":\" ws \"{\" ws \"\\\"name\\\":\" ws string \",\" nl \"\\\"description\\\":\" ws string \",\" nl \"\\\"project-url\\\":\" ws project-url \",\" nl \"\\\"team-members\\\":\" ws TeamMember \",\" nl \"\\\"task\\\":\" ws Task \"}\" ws \"}\"\n",
            "project-url ::= \"https://\"[0-9a-fA-F]*\n",
            "assigned-to ::= (\"Akshath\"|\"Raghav\"|\"Ravikiran\")\n",
            "ws ::= [ \\t\\n]\n",
            "nl ::= [\\n]\n",
            "string ::=  \"\\\"\" (\n",
            "            [^\"\\\\] |\n",
            "            \"\\\\\" ([\"\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])\n",
            "            )* \"\\\"\"\n",
            "TeamMember ::= nl \"{\" ws \"\\\"name\\\":\" ws string \",\" nl \"\\\"role\\\":\" ws string \"}\"\n",
            "number ::= (\"-\"? ([0-9] | [1-9] [0-9]*)) (\".\" [0-9]+)? ([eE] [-+]? [0-9]+)?\n",
            "boolean ::= (\"True\" | \"False\")\n",
            "TaskUpdate ::= nl \"{\" ws \"\\\"update-time\\\":\" ws number \",\" nl \"\\\"comment\\\":\" ws string \",\" nl \"\\\"status\\\":\" ws boolean \"}\"\n",
            "array ::= \"[\" ws (\n",
            "                due-date-value\n",
            "                (\",\" ws due-date-value)*\n",
            "            )? \"]\" ws\n",
            "due-date-value ::= string\n",
            "Task ::= nl \"{\" ws \"\\\"title\\\":\" ws string \",\" nl \"\\\"description\\\":\" ws string \",\" nl \"\\\"assigned-to\\\":\" ws assigned-to \",\" nl \"\\\"due-date\\\":\" ws array \",\" nl \"\\\"updates\\\":\" ws TaskUpdate \"}\"\n"
          ]
        }
      ],
      "source": [
        "print(grammar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "from_string grammar:\n",
            "root ::= ws Project \n",
            "ws ::= [ <U+0009><U+000A>] \n",
            "Project ::= nl [{] [\"] [P] [r] [o] [j] [e] [c] [t] [\"] [:] ws [{] ws [\"] [n] [a] [m] [e] [\"] [:] ws string [,] nl [\"] [d] [e] [s] [c] [r] [i] [p] [t] [i] [o] [n] [\"] [:] ws string [,] nl [\"] [p] [r] [o] [j] [e] [c] [t] [-] [u] [r] [l] [\"] [:] ws project-url [,] nl [\"] [t] [e] [a] [m] [-] [m] [e] [m] [b] [e] [r] [s] [\"] [:] ws TeamMember [,] nl [\"] [t] [a] [s] [k] [\"] [:] ws Task [}] ws [}] \n",
            "nl ::= [<U+000A>] \n",
            "string ::= [\"] string_13 [\"] \n",
            "project-url ::= [h] [t] [t] [p] [s] [:] [/] [/] project-url_8 \n",
            "TeamMember ::= nl [{] ws [\"] [n] [a] [m] [e] [\"] [:] ws string [,] nl [\"] [r] [o] [l] [e] [\"] [:] ws string [}] \n",
            "Task ::= nl [{] ws [\"] [t] [i] [t] [l] [e] [\"] [:] ws string [,] nl [\"] [d] [e] [s] [c] [r] [i] [p] [t] [i] [o] [n] [\"] [:] ws string [,] nl [\"] [a] [s] [s] [i] [g] [n] [e] [d] [-] [t] [o] [\"] [:] ws assigned-to [,] nl [\"] [d] [u] [e] [-] [d] [a] [t] [e] [\"] [:] ws array [,] nl [\"] [u] [p] [d] [a] [t] [e] [s] [\"] [:] ws TaskUpdate [}] \n",
            "project-url_8 ::= [0-9a-fA-F] project-url_8 | \n",
            "assigned-to ::= assigned-to_10 \n",
            "assigned-to_10 ::= [A] [k] [s] [h] [a] [t] [h] | [R] [a] [g] [h] [a] [v] | [R] [a] [v] [i] [k] [i] [r] [a] [n] \n",
            "string_11 ::= [^\"\\] | [\\] string_12 \n",
            "string_12 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
            "string_13 ::= string_11 string_13 | \n",
            "number ::= number_15 number_21 number_25 \n",
            "number_15 ::= number_16 number_17 \n",
            "number_16 ::= [-] | \n",
            "number_17 ::= [0-9] | [1-9] number_18 \n",
            "number_18 ::= [0-9] number_18 | \n",
            "number_19 ::= [.] number_20 \n",
            "number_20 ::= [0-9] number_20 | [0-9] \n",
            "number_21 ::= number_19 | \n",
            "number_22 ::= [eE] number_23 number_24 \n",
            "number_23 ::= [-+] | \n",
            "number_24 ::= [0-9] number_24 | [0-9] \n",
            "number_25 ::= number_22 | \n",
            "boolean ::= boolean_27 \n",
            "boolean_27 ::= [T] [r] [u] [e] | [F] [a] [l] [s] [e] \n",
            "TaskUpdate ::= nl [{] ws [\"] [u] [p] [d] [a] [t] [e] [-] [t] [i] [m] [e] [\"] [:] ws number [,] nl [\"] [c] [o] [m] [m] [e] [n] [t] [\"] [:] ws string [,] nl [\"] [s] [t] [a] [t] [u] [s] [\"] [:] ws boolean [}] \n",
            "array ::= [[] ws array_34 []] ws \n",
            "array_30 ::= due-date-value array_33 \n",
            "due-date-value ::= string \n",
            "array_32 ::= [,] ws due-date-value \n",
            "array_33 ::= array_32 array_33 | \n",
            "array_34 ::= array_30 | \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<llama_cpp.llama_grammar.LlamaGrammar at 0x2ae9dd5c77a0>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Using llama.cpp, we can verify if our grammar string is accepted.\n",
        "# If successful, no error is thrown. Unfortunately, llama-cpp-python prints out the syntax tree to stdout. \n",
        "GNBF.verify_grammar(grammar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "from_string grammar:\n",
            "root ::= ws Project \n",
            "ws ::= [ <U+0009><U+000A>] \n",
            "Project ::= [<] [P] [r] [o] [j] [e] [c] [t] [>] ws [<] [n] [a] [m] [e] [>] ws string ws [<] [/] [n] [a] [m] [e] [>] ws [<] [d] [e] [s] [c] [r] [i] [p] [t] [i] [o] [n] [>] ws string ws [<] [/] [d] [e] [s] [c] [r] [i] [p] [t] [i] [o] [n] [>] ws [<] [p] [r] [o] [j] [e] [c] [t] [-] [u] [r] [l] [>] ws project-url ws [<] [/] [p] [r] [o] [j] [e] [c] [t] [-] [u] [r] [l] [>] ws [<] [t] [e] [a] [m] [-] [m] [e] [m] [b] [e] [r] [s] [>] ws TeamMember ws [<] [/] [t] [e] [a] [m] [-] [m] [e] [m] [b] [e] [r] [s] [>] ws [<] [t] [a] [s] [k] [>] ws Task ws [<] [/] [t] [a] [s] [k] [>] ws [<] [/] [P] [r] [o] [j] [e] [c] [t] [>] \n",
            "string ::= [\"] string_13 [\"] \n",
            "project-url ::= [h] [t] [t] [p] [s] [:] [/] [/] project-url_7 \n",
            "TeamMember ::= [<] [n] [a] [m] [e] [>] ws string ws [<] [/] [n] [a] [m] [e] [>] ws [<] [r] [o] [l] [e] [>] ws string ws [<] [/] [r] [o] [l] [e] [>] \n",
            "Task ::= [<] [t] [i] [t] [l] [e] [>] ws string ws [<] [/] [t] [i] [t] [l] [e] [>] ws [<] [d] [e] [s] [c] [r] [i] [p] [t] [i] [o] [n] [>] ws string ws [<] [/] [d] [e] [s] [c] [r] [i] [p] [t] [i] [o] [n] [>] ws [<] [a] [s] [s] [i] [g] [n] [e] [d] [-] [t] [o] [>] ws assigned-to ws [<] [/] [a] [s] [s] [i] [g] [n] [e] [d] [-] [t] [o] [>] ws [<] [d] [u] [e] [-] [d] [a] [t] [e] [>] ws array ws [<] [/] [d] [u] [e] [-] [d] [a] [t] [e] [>] ws [<] [u] [p] [d] [a] [t] [e] [s] [>] ws TaskUpdate ws [<] [/] [u] [p] [d] [a] [t] [e] [s] [>] \n",
            "project-url_7 ::= [0-9a-fA-F] project-url_7 | \n",
            "assigned-to ::= assigned-to_9 \n",
            "assigned-to_9 ::= [A] [k] [s] [h] [a] [t] [h] | [R] [a] [g] [h] [a] [v] | [R] [a] [v] [i] [k] [i] [r] [a] [n] \n",
            "nl ::= [<U+000A>] \n",
            "string_11 ::= [^\"\\] | [\\] string_12 \n",
            "string_12 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
            "string_13 ::= string_11 string_13 | \n",
            "number ::= number_15 number_21 number_25 \n",
            "number_15 ::= number_16 number_17 \n",
            "number_16 ::= [-] | \n",
            "number_17 ::= [0-9] | [1-9] number_18 \n",
            "number_18 ::= [0-9] number_18 | \n",
            "number_19 ::= [.] number_20 \n",
            "number_20 ::= [0-9] number_20 | [0-9] \n",
            "number_21 ::= number_19 | \n",
            "number_22 ::= [eE] number_23 number_24 \n",
            "number_23 ::= [-+] | \n",
            "number_24 ::= [0-9] number_24 | [0-9] \n",
            "number_25 ::= number_22 | \n",
            "boolean ::= boolean_27 \n",
            "boolean_27 ::= [T] [r] [u] [e] | [F] [a] [l] [s] [e] \n",
            "TaskUpdate ::= [<] [u] [p] [d] [a] [t] [e] [-] [t] [i] [m] [e] [>] ws number ws [<] [/] [u] [p] [d] [a] [t] [e] [-] [t] [i] [m] [e] [>] ws [<] [c] [o] [m] [m] [e] [n] [t] [>] ws string ws [<] [/] [c] [o] [m] [m] [e] [n] [t] [>] ws [<] [s] [t] [a] [t] [u] [s] [>] ws boolean ws [<] [/] [s] [t] [a] [t] [u] [s] [>] \n",
            "array ::= [[] ws array_34 []] ws \n",
            "array_30 ::= due-date-value array_33 \n",
            "due-date-value ::= string \n",
            "array_32 ::= [,] ws due-date-value \n",
            "array_33 ::= array_32 array_33 | \n",
            "array_34 ::= array_30 | \n",
            "\n",
            "from_string grammar:\n",
            "root ::= ws Project \n",
            "ws ::= [ <U+0009><U+000A>] \n",
            "Project ::= [[] [P] [r] [o] [j] [e] [c] [t] []] nl [n] [a] [m] [e] ws [=] ws string nl [d] [e] [s] [c] [r] [i] [p] [t] [i] [o] [n] ws [=] ws string nl [p] [r] [o] [j] [e] [c] [t] [-] [u] [r] [l] ws [=] ws project-url nl TeamMember nl Task \n",
            "nl ::= [<U+000A>] \n",
            "string ::= [\"] string_13 [\"] \n",
            "project-url ::= [h] [t] [t] [p] [s] [:] [/] [/] project-url_8 \n",
            "TeamMember ::= [n] [a] [m] [e] ws [=] ws string nl [r] [o] [l] [e] ws [=] ws string \n",
            "Task ::= [t] [i] [t] [l] [e] ws [=] ws string nl [d] [e] [s] [c] [r] [i] [p] [t] [i] [o] [n] ws [=] ws string nl [a] [s] [s] [i] [g] [n] [e] [d] [-] [t] [o] ws [=] ws assigned-to nl [d] [u] [e] [-] [d] [a] [t] [e] ws [=] ws array nl TaskUpdate \n",
            "project-url_8 ::= [0-9a-fA-F] project-url_8 | \n",
            "assigned-to ::= assigned-to_10 \n",
            "assigned-to_10 ::= [A] [k] [s] [h] [a] [t] [h] | [R] [a] [g] [h] [a] [v] | [R] [a] [v] [i] [k] [i] [r] [a] [n] \n",
            "string_11 ::= [^\"\\] | [\\] string_12 \n",
            "string_12 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
            "string_13 ::= string_11 string_13 | \n",
            "number ::= number_15 number_21 number_25 \n",
            "number_15 ::= number_16 number_17 \n",
            "number_16 ::= [-] | \n",
            "number_17 ::= [0-9] | [1-9] number_18 \n",
            "number_18 ::= [0-9] number_18 | \n",
            "number_19 ::= [.] number_20 \n",
            "number_20 ::= [0-9] number_20 | [0-9] \n",
            "number_21 ::= number_19 | \n",
            "number_22 ::= [eE] number_23 number_24 \n",
            "number_23 ::= [-+] | \n",
            "number_24 ::= [0-9] number_24 | [0-9] \n",
            "number_25 ::= number_22 | \n",
            "boolean ::= boolean_27 \n",
            "boolean_27 ::= [T] [r] [u] [e] | [F] [a] [l] [s] [e] \n",
            "TaskUpdate ::= [u] [p] [d] [a] [t] [e] [-] [t] [i] [m] [e] ws [=] ws number nl [c] [o] [m] [m] [e] [n] [t] ws [=] ws string nl [s] [t] [a] [t] [u] [s] ws [=] ws boolean \n",
            "array ::= [[] ws array_34 []] ws \n",
            "array_30 ::= due-date-value array_33 \n",
            "due-date-value ::= string \n",
            "array_32 ::= [,] ws due-date-value \n",
            "array_33 ::= array_32 array_33 | \n",
            "array_34 ::= array_30 | \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<llama_cpp.llama_grammar.LlamaGrammar at 0x2ae9de7b7d70>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grammar = GNBF(Project).generate_grammar('xml')\n",
        "GNBF.verify_grammar(grammar)\n",
        "\n",
        "grammar = GNBF(Project).generate_grammar('toml')\n",
        "GNBF.verify_grammar(grammar) "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
