{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Any, List, Tuple, Type, Optional, Union\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LLM:\n",
        "    def __init__(self):\n",
        "        self.client = OpenAI(\n",
        "            api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "        )\n",
        "\n",
        "    def request(self, prompt, temperature=0.2, context=None):\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "\n",
        "llm = LLM()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def log(msg):\n",
        "    print(msg)\n",
        "    print(\"-------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How to make Prompts? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from grammarflow.prompt.builder import PromptBuilder  # Prompt builder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prompts can be supplied to the `Constrain` block (below) as a string, `Prompt` or `PromptBuilder` object. \n",
        "\n",
        "`PromptBuilder` is a straightforward interface to build prompts, which are built using 'sections' which are user-defined. You can specificy which sections of the prompt can be conditionally switched on/off, which can be edited using placeholders, which are fixed sections, where the grammars can be defined, etc.\n",
        "\n",
        "Each prompt can have one `define_grammar`, `placeholders` and `add_few_shot_examples` boolean fields. These switch on certain functionalities within the sections. You can pass in a function to the `enable_on` field for the conditional enabling.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample Llama Prompt\n",
        "\n",
        "llama_prompt = PromptBuilder()\n",
        "llama_prompt.add_section(\n",
        "    text=\"<s>[INST] <<SYS>>\\n{system_context}\\n<</SYS>>\",\n",
        "    placeholders=[\"system_context\"],\n",
        "    define_grammar=True,\n",
        ")\n",
        "llama_prompt.add_section(\n",
        "    text=\"{user_message}[/INST]\",\n",
        "    placeholders=[\"user_message\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How to use `enable_on`: \n",
        "\n",
        "Say we are working on a conversation chatbot, a chain that needs history for context. We might only want certain sections to be enabled when we have a history. We can use `enable_on` to enable or disable sections based on the presence of a history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dummy Example \n",
        "\n",
        "def check_previous_interaction(id_): return id_ > 1\n",
        "\n",
        "sample_prompt = PromptBuilder() \n",
        "sample_prompt.add_section(\n",
        "    text=\"You are a intelligent search machine. Your goal is to think about what topics to search about to provide the user with relevant information. Here is his question: {question}\",\n",
        "    placeholders=['question']\n",
        ") \n",
        "sample_prompt.add_section(\n",
        "  define_grammar=True\n",
        ")\n",
        "sample_prompt.add_section(\n",
        "  text=\"Choose keywords from the context given below: \\n{history}\",\n",
        "  placeholders=[\"history\"],\n",
        "  enable_on=check_previous_interaction\n",
        ") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How to make Grammars? \n",
        "\n",
        "Using Pydantic, we can define how data should be in pure python. GrammarFlow takes care of the rest. In this guide, you will find multiple examples of pydantic models for different use-cases. (Some of them are quite random, but I'm using them to prove effectiveness!)\n",
        "\n",
        "Here are some important rules! \n",
        "\n",
        "1. You can use Optional from `typing`, but the LLM won't understand when and when not to output an optional field. From experience, there are better ways to deal with Optional fields, such as enabling it within the grammar when needed. \n",
        "2. When you want to use regex, you can do it using `Field(..., pattern=\"\", description=\"\")`. The `pattern` field will be used during decoding, but is ignored during prompt embedding. This is because LLMs cannot 100% perform regex handling unless explained in human-terms. Moreover, when you see LLMs conforming to the regex expectations, its mostly during the token sampling that it is achieved. So, to overcome this, you can embed a semantic explanation of the regex, like `Field(..., pattern=\"^(Akshath|Raghav|Ravikiran)$\", description=\"Akshath OR Raghav OR Ravikiran\")`.\n",
        "3. Avoid using `Dict` in the pydantic model directly. Instead, make another model with the Dict fields and add that as a parameter to the original model. This becomes a nested structure, and can be handled well. If you choose `Dict[str, str]` method, then all I can do is put this into the prompt, which is ineffective and random. \n",
        "\n",
        "### Important note on acceptable regex for GNBF! \n",
        "\n",
        "Ensure your regex is solely `string` or `number` constraining. \n",
        "> Example: `'(\"Akshath\"|\"Raghav\"|\"Ravikiran\")'` for multiple OR scenarios; `'\"https://\"[0-9a-fA-F]*'` for links, etc. \n",
        "\n",
        "If you want a `string` to be present, enclose it in double-qoutes. To allow for any other sequences, bound with `()` or `[]` and specify directly (`[0-9a-fA-F]*`). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Constraining with GrammarFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from grammarflow.constrain import Constrain  # Main class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A `Constrain` block acts as a context manager. It keeps track of the serialization type and the #grammars you want to handle. \n",
        "\n",
        "It offers three main functions. \n",
        "1. `.format()` to format a prompt of your choice. Pass in `placeholders` as present in the format {'placeholder': text}. `grammars` needs to be a list of {'description': , 'model': }. `examples` needs to be a list of {'query': , 'model'}. \n",
        "2. `.get_grammar()` to get the corresponding GNBF grammar of the model you pass in. \n",
        "3. `.parse()` to parse the response into a `Response` dataclass.  \n",
        "4. `.inflation_rate()` can be used to get the token size at which the latest prompt has been increased. For smaller prompts, the number is >4x. For larger prompts, the number is <2x. You will find that **with increase of prompt size, the cost you will pay per LLM call will remain the same.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_message = \"\"\n",
        "system_context = \"\"\n",
        "\n",
        "\n",
        "class Model(BaseModel):\n",
        "    model_name: str\n",
        "\n",
        "\n",
        "with Constrain('json') as manager:\n",
        "    prompt = manager.format(llama_prompt, \n",
        "                        placeholders={'user_message': user_message, 'system_context': system_context},\n",
        "                        grammars=[{'model': [Model]}]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Serialization Use-Cases! \n",
        "\n",
        "1. 'JSON' is the classic go-to. Can handle simple stuff, nested-models, complex-grammars, etc. However, a simple missing terminal ('\"', '{\") can break the sampling chain. \n",
        "2. 'XML' is the safest go-to. Can handle all cases, except multiple grammar generation (see at the end of `Examples!` section). The use of starting and ending tags is handled easily by token sampling, and errors within naming/tag is handled by my parser. \n",
        "3. 'TOML' is best when we want to get multiple grammars generated in one-go. With a smaller inflation rate of the prompt (before and after grammarflow embeddings), it can handle a longer list of fields. However, it *cannot* work for nested models. TOML nested models usually are in the form given below. From my experimentations, LLMs have a hard time conforming to this and end up generating `obj2` field within `obj1` as a `Dict` object. Is it possible? Sure. Would I trust it? No.\n",
        "  ```\n",
        "  [obj1]\n",
        "  field = \"\" \n",
        "  field2 = \"\" \n",
        "\n",
        "  [obj1.obj2]\n",
        "  field3 = \"\" \n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Examples! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```\n",
            "<FunctionModel>\n",
            "<function_name> \"fib\" </function_name>\n",
            "<docstring> \"This function returns the fibonacci sequence.\" </docstring>\n",
            "<depedencies> [\"numpy\"] </depedencies>\n",
            "<uuid> 123456789 </uuid>\n",
            "<is_python> true </is_python>\n",
            "<code> \"import numpy as np\\n\\ndef fib(n):\\n    a, b = 0, 1\\n    result = []\\n    for _ in range(n):\\n        result.append(a)\\n        a, b = b, a + b\\n    return np.array(result)\" </code>\n",
            "</FunctionModel>\n",
            "```\n",
            "-------------\n",
            "{\n",
            "    \"FunctionModel\": {\n",
            "        \"function_name\": \"fib\",\n",
            "        \"docstring\": \"This function returns the fibonacci sequence.\",\n",
            "        \"depedencies\": [\n",
            "            \"numpy\"\n",
            "        ],\n",
            "        \"uuid\": 123456789,\n",
            "        \"is_python\": true,\n",
            "        \"code\": \"import numpy as np\\\\n\\\\ndef fib(n):\\\\n    a, b = 0, 1\\\\n    result = []\\\\n    for _ in range(n):\\\\n        result.append(a)\\\\n        a, b = b, a + b\\\\n    return np.array(result)\"\n",
            "    }\n",
            "}\n",
            "-------------\n",
            "{'before': 27, 'after': 143, 'factor': '4.3x'}\n",
            "-------------\n"
          ]
        }
      ],
      "source": [
        "# # Here's a simple example of asking an LLM to make code.\n",
        "# # This can be used within coding assistants which requires extra metadata.\n",
        "\n",
        "\n",
        "class FunctionModel(BaseModel):\n",
        "    function_name: str\n",
        "    docstring: str\n",
        "    depedencies: List[str]\n",
        "    uuid: Union[float, int]\n",
        "    is_python: bool\n",
        "    code: str\n",
        "\n",
        "\n",
        "input_str = \"I want to create a function that returns the fibonacci sequence. The function should be called 'fib'. The function can use numpy.\"\n",
        "\n",
        "with Constrain('xml') as manager:\n",
        "    prompt = manager.format(input_str, grammars=[{\"description\": \"No Code Generation\", \"model\": FunctionModel}])\n",
        "\n",
        "    llm_response = llm.request(prompt, temperature=0.01)\n",
        "    log(llm_response)\n",
        "\n",
        "    response = manager.parse(llm_response)\n",
        "    log(response)\n",
        "\n",
        "    log(manager.inflation_rate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# # The response will be of `Response` type, which can be used to extract the data. If adding the parsed response to this object fails, it will return the dict itself.\n",
        "print(response.FunctionModel.is_python)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```\n",
            "<FunctionModel>\n",
            "<function_name> fib </function_name>\n",
            "<docstring> This function returns the fibonacci sequence. </docstring>\n",
            "<depedencies> ['numpy'] </depedencies>\n",
            "<uuid> 987654321.0 </uuid>\n",
            "<is_python> True </is_python>\n",
            "<code> def fib(n):\n",
            "\tif n <= 1:\n",
            "\t\treturn n\n",
            "\telse:\n",
            "\t\treturn fib(n-1) + fib(n-2) </code>\n",
            "</FunctionModel>\n",
            "```\n",
            "-------------\n",
            "{\n",
            "    \"FunctionModel\": {\n",
            "        \"function_name\": \"fib\",\n",
            "        \"docstring\": \"This function returns the fibonacci sequence.\",\n",
            "        \"depedencies\": [\n",
            "            \"numpy\"\n",
            "        ],\n",
            "        \"uuid\": 987654321.0,\n",
            "        \"is_python\": true,\n",
            "        \"code\": \"def fib(n):\\tif n <= 1:\\t\\treturn n\\telse:\\t\\treturn fib(n-1) + fib(n-2)\"\n",
            "    }\n",
            "}\n",
            "-------------\n",
            "{'before': 27, 'after': 237, 'factor': '7.8x'}\n",
            "-------------\n"
          ]
        }
      ],
      "source": [
        "# Add some examples too!\n",
        "\n",
        "Sum_Function_Model = FunctionModel(\n",
        "    function_name=\"sum\",\n",
        "    docstring=\"This function returns the sum of the input list.\",\n",
        "    depedencies=[\"numpy\"],\n",
        "    uuid=123456789,\n",
        "    is_python=True,\n",
        "    code=\"def sum(a, b):\\n\\treturn a + b\"\n",
        ")\n",
        "\n",
        "\n",
        "with Constrain('xml') as manager:\n",
        "    prompt = manager.format(input_str, \n",
        "        grammars=[\n",
        "            {\n",
        "                'description': 'No Code Generation',\n",
        "                'model': FunctionModel\n",
        "            }\n",
        "        ],\n",
        "        examples=[\n",
        "            {\n",
        "                'query': \"Create a summation function in Python\",\n",
        "                'model': Sum_Function_Model\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "    llm_response = llm.request(prompt, temperature=0.01)\n",
        "    log(llm_response)\n",
        "\n",
        "    response = manager.parse(llm_response)\n",
        "    log(response)\n",
        "\n",
        "    log(manager.inflation_rate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```\n",
            "{\n",
            "\"ThoughtState\": {\n",
            "\"thought\": \"I need to gather information about Vladmir Putin.\",\n",
            "\"goal\": \"To learn more about Vladmir Putin.\",\n",
            "\"tool\": \"Web_Search\",\n",
            "\"action\": \"Read\",\n",
            "\"action_input\": \"Vladmir Putin\",\n",
            "\"thought_id\": \"1a2b3c4d\"\n",
            "}\n",
            "}\n",
            "```\n",
            "-------------\n",
            "{\n",
            "    \"ThoughtState\": {\n",
            "        \"thought\": \"I need to gather information about Vladmir Putin.\",\n",
            "        \"goal\": \"To learn more about Vladmir Putin.\",\n",
            "        \"tool\": \"Web_Search\",\n",
            "        \"action\": \"Read\",\n",
            "        \"action_input\": \"Vladmir Putin\",\n",
            "        \"thought_id\": \"1a2b3c4d\"\n",
            "    }\n",
            "}\n",
            "-------------\n",
            "{'before': 48, 'after': 230, 'factor': '3.8x'}\n",
            "-------------\n"
          ]
        }
      ],
      "source": [
        "# Sample ReAct Model with Llama Prompt\n",
        "# You can add descriptions within the grammar model to provide it's context and options. This is how we use the LLM in https://github.com/e-lab/Forestry_Student/.\n",
        "\n",
        "class ThoughtState(BaseModel):\n",
        "    thought: str\n",
        "    goal: str\n",
        "    tool: str = Field(...,\n",
        "                      description=\"Choose one of ['Web_QA', 'Web_Search', 'Web_Scraping', 'Web_Automation', 'Web_Research']\")\n",
        "    action: str = Field(...,\n",
        "                        description=\"Choose one of ['Create', 'Update', 'Delete', 'Read']\")\n",
        "    action_input: str = Field(..., description=\"The input data for the action\")\n",
        "    thought_id: Optional[str] = Field(\n",
        "        None, description=\"The unique identifier for the thought\")\n",
        "\n",
        "\n",
        "system_context = \"\"\"Your goal is to think and plan out how to solve questions using agent tools provided to you. Think about all aspects of your thought process.\"\"\"\n",
        "user_message = \"\"\"Who is Vladmir Putin?\"\"\"\n",
        "\n",
        "with Constrain('json') as manager:\n",
        "    prompt = manager.format(llama_prompt, placeholders={\n",
        "                          'user_message': user_message,\n",
        "                          'system_context': system_context\n",
        "                          },\n",
        "                          grammars=[{\n",
        "                              'description': 'This format describes your current thinking state',\n",
        "                              'model': [ThoughtState]}]\n",
        "    )\n",
        "\n",
        "    llm_response = llm.request(prompt, temperature=0.01)\n",
        "    log(llm_response)\n",
        "\n",
        "    response = manager.parse(llm_response)\n",
        "    log(response)\n",
        "\n",
        "    log(manager.inflation_rate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Web_Search\n"
          ]
        }
      ],
      "source": [
        "# # You can then access the response from the `response` object\n",
        "print(response.ThoughtState.tool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```\n",
            "{\n",
            "\"Project\": {\n",
            "\"name\": \"Multimodal Document Understanding Project\",\n",
            "\"description\": \"A project focused on developing a system for understanding documents using multiple modes of input such as text, images, and audio.\",\n",
            "\"project_url\": \"https://example.com/multimodal-document-understanding\",\n",
            "\"team_members\": [\n",
            "{\n",
            "\"name\": \"John Doe\",\n",
            "\"role\": \"Project Manager\"\n",
            "},\n",
            "{\n",
            "\"name\": \"Jane Smith\",\n",
            "\"role\": \"Software Engineer\"\n",
            "},\n",
            "{\n",
            "\"name\": \"Alice Johnson\",\n",
            "\"role\": \"Data Scientist\"\n",
            "}\n",
            "],\n",
            "\"task\": {\n",
            "\"title\": \"Research existing multimodal document understanding systems\",\n",
            "\"description\": \"Conduct a literature review and analyze current state-of-the-art systems in the field.\",\n",
            "\"assigned_to\": \"John Doe\",\n",
            "\"due_date\": [\"2022-10-15\"]\n",
            "}\n",
            "}\n",
            "}\n",
            "```\n",
            "-------------\n",
            "{\n",
            "    \"Project\": {\n",
            "        \"name\": \"Multimodal Document Understanding Project\",\n",
            "        \"description\": \"A project focused on developing a system for understanding documents using multiple modes of input such as text, images, and audio.\",\n",
            "        \"project_url\": \"https://example.com/multimodal-document-understanding\",\n",
            "        \"team_members\": [\n",
            "            {\n",
            "                \"name\": \"John Doe\",\n",
            "                \"role\": \"Project Manager\"\n",
            "            },\n",
            "            {\n",
            "                \"name\": \"Jane Smith\",\n",
            "                \"role\": \"Software Engineer\"\n",
            "            },\n",
            "            {\n",
            "                \"name\": \"Alice Johnson\",\n",
            "                \"role\": \"Data Scientist\"\n",
            "            }\n",
            "        ],\n",
            "        \"task\": {\n",
            "            \"title\": \"Research existing multimodal document understanding systems\",\n",
            "            \"description\": \"Conduct a literature review and analyze current state-of-the-art systems in the field.\",\n",
            "            \"assigned_to\": \"John Doe\",\n",
            "            \"due_date\": [\n",
            "                \"2022-10-15\"\n",
            "            ]\n",
            "        }\n",
            "    }\n",
            "}\n",
            "-------------\n",
            "```\n",
            "<Project>\n",
            "<name> \"Multimodal Document Understanding Project\" </name>\n",
            "<description> \"This project aims to develop a system that can understand and analyze documents using multiple modes of input such as text, images, and audio.\" </description>\n",
            "<project_url> \"https://www.multimodalproject.com\" </project_url>\n",
            "<team_members> \n",
            "    <TeamMember>\n",
            "        <name> \"John Doe\" </name>\n",
            "        <role> \"Project Manager\" </role>\n",
            "    </TeamMember>\n",
            "    <TeamMember>\n",
            "        <name> \"Jane Smith\" </name>\n",
            "        <role> \"Lead Developer\" </role>\n",
            "    </TeamMember>\n",
            "    <TeamMember>\n",
            "        <name> \"Alice Johnson\" </name>\n",
            "        <role> \"Data Scientist\" </role>\n",
            "    </TeamMember>\n",
            "</team_members>\n",
            "<task> \n",
            "    <title> \"Data Collection and Preprocessing\" </title>\n",
            "    <description> \"Collect and preprocess text, image, and audio data for training the multimodal document understanding system.\" </description>\n",
            "    <assigned_to> \"Alice Johnson\" </assigned_to>\n",
            "    <due_date> List[\"2022-10-15\"] </due_date>\n",
            "</task>\n",
            "</Project>\n",
            "```\n",
            "-------------\n",
            "{'Project': {'name': 'Multimodal Document Understanding Project', 'description': 'This project aims to develop a system that can understand and analyze documents using multiple modes of input such as text, images, and audio.', 'project_url': 'https://www.multimodalproject.com', 'team_members': {'TeamMember': [{'name': 'John Doe', 'role': 'Project Manager'}, {'name': 'Jane Smith', 'role': 'Lead Developer'}, {'name': 'Alice Johnson', 'role': 'Data Scientist'}]}, 'task': {'title': 'Data Collection and Preprocessing', 'description': 'Collect and preprocess text, image, and audio data for training the multimodal document understanding system.', 'assigned_to': 'Alice Johnson', 'due_date': typing.List[ForwardRef('2022-10-15')]}}}\n",
            "-------------\n",
            "```\n",
            "[Project]\n",
            "name = \"Multimodal Document Understanding Project\"\n",
            "description = \"A project focused on developing a system that can understand and analyze documents using multiple modes of input such as text, images, and audio.\"\n",
            "project_url = \"https://www.multimodal-doc-understanding.com\"\n",
            "team_members = [\n",
            "    [TeamMember]\n",
            "    name = \"John Doe\"\n",
            "    role = \"Project Manager\",\n",
            "    [TeamMember]\n",
            "    name = \"Jane Smith\"\n",
            "    role = \"Lead Developer\",\n",
            "    [TeamMember]\n",
            "    name = \"Alice Johnson\"\n",
            "    role = \"Data Scientist\"\n",
            "]\n",
            "task = [Task]\n",
            "title = \"Develop Natural Language Processing Module\"\n",
            "description = \"Create a module that can process and analyze text data from documents.\"\n",
            "assigned_to = \"Jane Smith\"\n",
            "due_date = \"2022-10-15\"\n",
            "```\n",
            "-------------\n"
          ]
        },
        {
          "ename": "ParsingError",
          "evalue": "ERROR: Unable to parse response into TOML format!",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSyntaxError\u001b[0m                               Traceback (most recent call last)",
            "File \u001b[0;32m/depot/euge/data/araviki/SyntaxShaper/grammarflow/grammars/toml.py:232\u001b[0m, in \u001b[0;36mTOML.parse_toml\u001b[0;34m(toml_string)\u001b[0m\n\u001b[1;32m    231\u001b[0m i \u001b[38;5;241m=\u001b[39m skip_whitespace(toml_string, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 232\u001b[0m section, i \u001b[38;5;241m=\u001b[39m parse_section(toml_string, i)\n\u001b[1;32m    233\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(section\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m/depot/euge/data/araviki/SyntaxShaper/grammarflow/grammars/toml.py:136\u001b[0m, in \u001b[0;36mTOML.parse_toml.<locals>.parse_section\u001b[0;34m(toml_string, i)\u001b[0m\n\u001b[1;32m    135\u001b[0m i \u001b[38;5;241m=\u001b[39m skip_whitespace(toml_string, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 136\u001b[0m value, i \u001b[38;5;241m=\u001b[39m parse_value(toml_string, i)\n\u001b[1;32m    137\u001b[0m section[key\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)][subkey\u001b[38;5;241m.\u001b[39mreplace(\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()] \u001b[38;5;241m=\u001b[39m value\n",
            "File \u001b[0;32m/depot/euge/data/araviki/SyntaxShaper/grammarflow/grammars/toml.py:147\u001b[0m, in \u001b[0;36mTOML.parse_toml.<locals>.parse_value\u001b[0;34m(toml_string, i)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m toml_string[i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_array(toml_string, i)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m toml_string[i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
            "File \u001b[0;32m/depot/euge/data/araviki/SyntaxShaper/grammarflow/grammars/toml.py:179\u001b[0m, in \u001b[0;36mTOML.parse_toml.<locals>.parse_array\u001b[0;34m(toml_string, i)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m toml_string[i] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 179\u001b[0m     value, i \u001b[38;5;241m=\u001b[39m parse_value(toml_string, i)\n\u001b[1;32m    180\u001b[0m     array \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [value]\n",
            "File \u001b[0;32m/depot/euge/data/araviki/SyntaxShaper/grammarflow/grammars/toml.py:147\u001b[0m, in \u001b[0;36mTOML.parse_toml.<locals>.parse_value\u001b[0;34m(toml_string, i)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m toml_string[i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_array(toml_string, i)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m toml_string[i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
            "File \u001b[0;32m/depot/euge/data/araviki/SyntaxShaper/grammarflow/grammars/toml.py:179\u001b[0m, in \u001b[0;36mTOML.parse_toml.<locals>.parse_array\u001b[0;34m(toml_string, i)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m toml_string[i] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 179\u001b[0m     value, i \u001b[38;5;241m=\u001b[39m parse_value(toml_string, i)\n\u001b[1;32m    180\u001b[0m     array \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [value]\n",
            "File \u001b[0;32m/depot/euge/data/araviki/SyntaxShaper/grammarflow/grammars/toml.py:151\u001b[0m, in \u001b[0;36mTOML.parse_toml.<locals>.parse_value\u001b[0;34m(toml_string, i)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_other(toml_string, i)\n",
            "File \u001b[0;32m/depot/euge/data/araviki/SyntaxShaper/grammarflow/grammars/toml.py:171\u001b[0m, in \u001b[0;36mTOML.parse_toml.<locals>.parse_other\u001b[0;34m(toml_string, i)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28meval\u001b[39m(val), i\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
            "\u001b[0;31mSyntaxError\u001b[0m: invalid syntax (<string>, line 0)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mParsingError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m llm_response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mrequest(prompt, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     35\u001b[0m log(llm_response)\n\u001b[0;32m---> 37\u001b[0m response \u001b[38;5;241m=\u001b[39m manager\u001b[38;5;241m.\u001b[39mparse(llm_response)\n\u001b[1;32m     38\u001b[0m log(response)\n",
            "File \u001b[0;32m/depot/euge/data/araviki/SyntaxShaper/grammarflow/constrain.py:121\u001b[0m, in \u001b[0;36mConstrain.parse\u001b[0;34m(self, return_value)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_value: \n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \n\u001b[0;32m--> 121\u001b[0m parsed_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_helper(return_value)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(parsed_response)\n",
            "File \u001b[0;32m/depot/euge/data/araviki/SyntaxShaper/grammarflow/constrain.py:147\u001b[0m, in \u001b[0;36mConstrain.parse_helper\u001b[0;34m(self, return_value)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JSON\u001b[38;5;241m.\u001b[39mparse(return_value)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoml\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TOML\u001b[38;5;241m.\u001b[39mparse(return_value)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxml\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m XML\u001b[38;5;241m.\u001b[39mparse(return_value)\n",
            "File \u001b[0;32m/depot/euge/data/araviki/SyntaxShaper/grammarflow/grammars/toml.py:248\u001b[0m, in \u001b[0;36mTOML.parse\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TOML\u001b[38;5;241m.\u001b[39mparse_toml(text)\n",
            "File \u001b[0;32m/depot/euge/data/araviki/SyntaxShaper/grammarflow/grammars/toml.py:243\u001b[0m, in \u001b[0;36mTOML.parse_toml\u001b[0;34m(toml_string)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m storage\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParsingError(\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mERROR: Unable to parse response into TOML format!\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
            "\u001b[0;31mParsingError\u001b[0m: ERROR: Unable to parse response into TOML format!"
          ]
        }
      ],
      "source": [
        "# You can add complex layers of grammars. You add even using Optional and Union types.\n",
        "# For complex and nested grammars, JSON and XML are the best formats to use. \n",
        "\n",
        "class TeamMember(BaseModel):\n",
        "    name: str\n",
        "    role: str\n",
        "\n",
        "class Task(BaseModel):\n",
        "    title: str\n",
        "    description: str\n",
        "    assigned_to: str = Field(..., pattern=\"^(Akshath|Raghav|Ravikiran)$\")\n",
        "    due_date: List[str]\n",
        "\n",
        "class Project(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    project_url: Optional[str] = None\n",
        "    team_members: List[TeamMember]\n",
        "    task: Task\n",
        "\n",
        "for serialization in ['json', 'xml']:\n",
        "    with Constrain(serialization) as manager:\n",
        "        system_context = \"\"\"You are a project manager and you are responsible for managing a project. You have to manage the project, it's grammars and other aspects. Ensure that you fill out all required fields.\"\"\"\n",
        "        user_message = \"\"\"Make me a project plan for a new project on multimodal document understanding projct.\"\"\"\n",
        "\n",
        "        prompt = manager.format(llama_prompt, placeholders={'user_message': user_message,\n",
        "                            'system_context': system_context},\n",
        "                            grammars=[{\n",
        "                                'description': 'This format elaborates on the project and its grammars.',\n",
        "                                'model': [Project]},\n",
        "        ]\n",
        "        )\n",
        "\n",
        "        llm_response = llm.request(prompt, temperature=0.01)\n",
        "        log(llm_response)\n",
        "\n",
        "        response = manager.parse(llm_response)\n",
        "        log(response)\n",
        "\n",
        "        log(manager.inflation_rate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# You can add multiple grammars to the same prompt. NOT RECOMMENDED.\n",
        "# If you wish to do so, generally, TOML and JSON are the best formats to use.\n",
        "\n",
        "class EventIdea(BaseModel):\n",
        "    event_name: str\n",
        "    event_description: str\n",
        "    event_duration: str\n",
        "\n",
        "class BudgetPlan(BaseModel):\n",
        "    budget: float\n",
        "    items: List[str]\n",
        "    prices: List[int]\n",
        "    total_cost: int\n",
        "\n",
        "class EventSchedule(BaseModel):\n",
        "    event_name: str\n",
        "    event_time: float\n",
        "    event_duration: str\n",
        "\n",
        "prompt = \"I am hosting a birthday party for my girlfriend tomorrow. I want to buy a cake, balloons, some roses and ice cream. I have a budget of 500$. Can you create a sample event schedule and budget plan for me?.\"\n",
        "\n",
        "with Constrain('toml', 'multi_response') as manager:\n",
        "    prompt = manager.format(llama_prompt, \n",
        "        grammars=[\n",
        "            {\"task_description\": \"Brainstorming Event Ideas\", \"model\": EventIdea},\n",
        "            {\n",
        "                \"task_description\": \"Budget Planning And Activity Planning\",\n",
        "                \"model\": [BudgetPlan, EventSchedule],\n",
        "            },\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    llm_response = llm.request(prompt, temperature=0.01)\n",
        "    log(llm_response)\n",
        "\n",
        "    response = manager.parse(llm_response)\n",
        "    log(response)\n",
        "\n",
        "    log(manager.inflation_rate())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Grammars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> \"GBNF (GGML BNF) is a format for defining formal grammars to constrain model outputs in llama.cpp. For example, you can use it to force the model to generate valid JSON, or speak only in emojis.\"\n",
        "\n",
        "Read more about it here: https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from grammarflow.grammars.gnbf import GNBF\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional, List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TeamMember(BaseModel):\n",
        "    name: str\n",
        "    role: str\n",
        "\n",
        "class TaskUpdate(BaseModel):\n",
        "    update_time: float\n",
        "    comment: Optional[str] = None\n",
        "    status: bool\n",
        "\n",
        "class Task(BaseModel):\n",
        "    title: str\n",
        "    description: str\n",
        "    assigned_to: str = Field(..., pattern='(\"Akshath\"|\"Raghav\"|\"Ravikiran\")')\n",
        "    due_date: List[str]\n",
        "    updates: List[TaskUpdate]\n",
        "\n",
        "class Project(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    project_url: Optional[str] = Field(None, pattern='\"https://\"[0-9a-fA-F]*')\n",
        "    team_members: List[TeamMember]  \n",
        "    task: Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "grammar = GNBF(Project).generate_grammar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root ::= ws Project\n",
            "Project ::= nl \"{\" \"\\\"Project\\\":\" ws \"{\" ws \"\\\"name\\\":\" ws string \",\" nl \"\\\"description\\\":\" ws string \",\" nl \"\\\"project-url\\\":\" ws project-url \",\" nl \"\\\"team-members\\\":\" ws TeamMember \",\" nl \"\\\"task\\\":\" ws Task \"}\" ws \"}\"\n",
            "project-url ::= \"https://\"[0-9a-fA-F]\n",
            "assigned-to ::= (\"Akshath\"|\"Raghav\"|\"Ravikiran\")\n",
            "ws ::= [ \\t\\n]\n",
            "nl ::= [\\n]\n",
            "string ::=  \"\\\"\" (\n",
            "            [^\"\\\\] |\n",
            "            \"\\\\\" ([\"\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])\n",
            "            )* \"\\\"\"\n",
            "TeamMember ::= nl \"{\" ws \"\\\"name\\\":\" ws string \",\" nl \"\\\"role\\\":\" ws string \"}\"\n",
            "number ::= (\"-\"? ([0-9] | [1-9] [0-9]*)) (\".\" [0-9]+)? ([eE] [-+]? [0-9]+)?\n",
            "boolean ::= (\"True\" | \"False\")\n",
            "TaskUpdate ::= nl \"{\" ws \"\\\"update-time\\\":\" ws number \",\" nl \"\\\"comment\\\":\" ws string \",\" nl \"\\\"status\\\":\" ws boolean \"}\"\n",
            "array ::= \"[\" ws (\n",
            "                due-date-value\n",
            "                (\",\" ws due-date-value)*\n",
            "            )? \"]\" ws\n",
            "due-date-value ::= string\n",
            "Task ::= nl \"{\" ws \"\\\"title\\\":\" ws string \",\" nl \"\\\"description\\\":\" ws string \",\" nl \"\\\"assigned-to\\\":\" ws assigned-to \",\" nl \"\\\"due-date\\\":\" ws array \",\" nl \"\\\"updates\\\":\" ws TaskUpdate \"}\"\n"
          ]
        }
      ],
      "source": [
        "print(grammar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "from_string grammar:\n",
            "root ::= ws Project \n",
            "ws ::= [ <U+0009><U+000A>] \n",
            "Project ::= nl [{] [\"] [P] [r] [o] [j] [e] [c] [t] [\"] [:] ws [{] ws [\"] [n] [a] [m] [e] [\"] [:] ws string [,] nl [\"] [d] [e] [s] [c] [r] [i] [p] [t] [i] [o] [n] [\"] [:] ws string [,] nl [\"] [p] [r] [o] [j] [e] [c] [t] [-] [u] [r] [l] [\"] [:] ws project-url [,] nl [\"] [t] [e] [a] [m] [-] [m] [e] [m] [b] [e] [r] [s] [\"] [:] ws TeamMember [,] nl [\"] [t] [a] [s] [k] [\"] [:] ws Task [}] ws [}] \n",
            "nl ::= [<U+000A>] \n",
            "string ::= [\"] string_12 [\"] \n",
            "project-url ::= [h] [t] [t] [p] [s] [:] [/] [/] [0-9a-fA-F] \n",
            "TeamMember ::= nl [{] ws [\"] [n] [a] [m] [e] [\"] [:] ws string [,] nl [\"] [r] [o] [l] [e] [\"] [:] ws string [}] \n",
            "Task ::= nl [{] ws [\"] [t] [i] [t] [l] [e] [\"] [:] ws string [,] nl [\"] [d] [e] [s] [c] [r] [i] [p] [t] [i] [o] [n] [\"] [:] ws string [,] nl [\"] [a] [s] [s] [i] [g] [n] [e] [d] [-] [t] [o] [\"] [:] ws assigned-to [,] nl [\"] [d] [u] [e] [-] [d] [a] [t] [e] [\"] [:] ws array [,] nl [\"] [u] [p] [d] [a] [t] [e] [s] [\"] [:] ws TaskUpdate [}] \n",
            "assigned-to ::= assigned-to_9 \n",
            "assigned-to_9 ::= [A] [k] [s] [h] [a] [t] [h] | [R] [a] [g] [h] [a] [v] | [R] [a] [v] [i] [k] [i] [r] [a] [n] \n",
            "string_10 ::= [^\"\\] | [\\] string_11 \n",
            "string_11 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
            "string_12 ::= string_10 string_12 | \n",
            "number ::= number_14 number_20 number_24 \n",
            "number_14 ::= number_15 number_16 \n",
            "number_15 ::= [-] | \n",
            "number_16 ::= [0-9] | [1-9] number_17 \n",
            "number_17 ::= [0-9] number_17 | \n",
            "number_18 ::= [.] number_19 \n",
            "number_19 ::= [0-9] number_19 | [0-9] \n",
            "number_20 ::= number_18 | \n",
            "number_21 ::= [eE] number_22 number_23 \n",
            "number_22 ::= [-+] | \n",
            "number_23 ::= [0-9] number_23 | [0-9] \n",
            "number_24 ::= number_21 | \n",
            "boolean ::= boolean_26 \n",
            "boolean_26 ::= [T] [r] [u] [e] | [F] [a] [l] [s] [e] \n",
            "TaskUpdate ::= nl [{] ws [\"] [u] [p] [d] [a] [t] [e] [-] [t] [i] [m] [e] [\"] [:] ws number [,] nl [\"] [c] [o] [m] [m] [e] [n] [t] [\"] [:] ws string [,] nl [\"] [s] [t] [a] [t] [u] [s] [\"] [:] ws boolean [}] \n",
            "array ::= [[] ws array_33 []] ws \n",
            "array_29 ::= due-date-value array_32 \n",
            "due-date-value ::= string \n",
            "array_31 ::= [,] ws due-date-value \n",
            "array_32 ::= array_31 array_32 | \n",
            "array_33 ::= array_29 | \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<llama_cpp.llama_grammar.LlamaGrammar at 0x2ae9dce27770>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Using llama.cpp, we can verify if our grammar string is accepted.\n",
        "# If successful, no error is thrown. Unfortunately, llama-cpp-python prints out the syntax tree to stdout. \n",
        "GNBF.verify_grammar(grammar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "from_string grammar:\n",
            "root ::= ws Project \n",
            "ws ::= [ <U+0009><U+000A>] \n",
            "Project ::= [<] [P] [r] [o] [j] [e] [c] [t] [>] ws [<] [n] [a] [m] [e] [>] ws string ws [<] [/] [n] [a] [m] [e] [>] ws [<] [d] [e] [s] [c] [r] [i] [p] [t] [i] [o] [n] [>] ws string ws [<] [/] [d] [e] [s] [c] [r] [i] [p] [t] [i] [o] [n] [>] ws [<] [p] [r] [o] [j] [e] [c] [t] [-] [u] [r] [l] [>] ws [h] [t] [t] [p] [s] [:] [/] [/] [.] [*] ws [<] [/] [p] [r] [o] [j] [e] [c] [t] [-] [u] [r] [l] [>] ws [<] [t] [e] [a] [m] [-] [m] [e] [m] [b] [e] [r] [s] [>] ws [^] [(] [A] [k] [s] [h] [a] [t] [h] [|] [R] [a] [g] [h] [a] [v] [|] [R] [a] [v] [i] [k] [i] [r] [a] [n] [)] [$] ws [<] [/] [t] [e] [a] [m] [-] [m] [e] [m] [b] [e] [r] [s] [>] ws [<] [g] [r] [a] [m] [m] [a] [r] [s] [>] ws Task ws [<] [/] [g] [r] [a] [m] [m] [a] [r] [s] [>] ws [<] [/] [P] [r] [o] [j] [e] [c] [t] [>] \n",
            "string ::= [\"] string_20 [\"] \n",
            "Task ::= [<] [t] [i] [t] [l] [e] [>] ws string ws [<] [/] [t] [i] [t] [l] [e] [>] ws [<] [d] [e] [s] [c] [r] [i] [p] [t] [i] [o] [n] [>] ws string ws [<] [/] [d] [e] [s] [c] [r] [i] [p] [t] [i] [o] [n] [>] ws [<] [a] [s] [s] [i] [g] [n] [e] [d] [-] [t] [o] [>] ws [^] [(] [A] [k] [s] [h] [a] [t] [h] [|] [R] [a] [g] [h] [a] [v] [|] [R] [a] [v] [i] [k] [i] [r] [a] [n] [)] [$] ws [<] [/] [a] [s] [s] [i] [g] [n] [e] [d] [-] [t] [o] [>] ws [<] [d] [u] [e] [-] [d] [a] [t] [e] [>] ws array ws [<] [/] [d] [u] [e] [-] [d] [a] [t] [e] [>] ws [<] [u] [p] [d] [a] [t] [e] [s] [>] ws TaskUpdate ws [<] [/] [u] [p] [d] [a] [t] [e] [s] [>] \n",
            "nl ::= [<U+000A>] \n",
            "number ::= number_7 number_13 number_17 \n",
            "number_7 ::= number_8 number_9 \n",
            "number_8 ::= [-] | \n",
            "number_9 ::= [0-9] | [1-9] number_10 \n",
            "number_10 ::= [0-9] number_10 | \n",
            "number_11 ::= [.] number_12 \n",
            "number_12 ::= [0-9] number_12 | [0-9] \n",
            "number_13 ::= number_11 | \n",
            "number_14 ::= [eE] number_15 number_16 \n",
            "number_15 ::= [-+] | \n",
            "number_16 ::= [0-9] number_16 | [0-9] \n",
            "number_17 ::= number_14 | \n",
            "string_18 ::= [^\"\\] | [\\] string_19 \n",
            "string_19 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
            "string_20 ::= string_18 string_20 | \n",
            "boolean ::= boolean_22 \n",
            "boolean_22 ::= [T] [r] [u] [e] | [F] [a] [l] [s] [e] \n",
            "TaskUpdate ::= [<] [u] [p] [d] [a] [t] [e] [-] [t] [i] [m] [e] [>] ws number ws [<] [/] [u] [p] [d] [a] [t] [e] [-] [t] [i] [m] [e] [>] ws [<] [c] [o] [m] [m] [e] [n] [t] [>] ws string ws [<] [/] [c] [o] [m] [m] [e] [n] [t] [>] ws [<] [s] [t] [a] [t] [u] [s] [>] ws boolean ws [<] [/] [s] [t] [a] [t] [u] [s] [>] \n",
            "array ::= [[] ws array_29 []] ws \n",
            "array_25 ::= due-date-value array_28 \n",
            "due-date-value ::= string \n",
            "array_27 ::= [,] ws due-date-value \n",
            "array_28 ::= array_27 array_28 | \n",
            "array_29 ::= array_25 | \n",
            "\n",
            "from_string grammar:\n",
            "root ::= ws Project \n",
            "ws ::= [ <U+0009><U+000A>] \n",
            "Project ::= [[] [P] [r] [o] [j] [e] [c] [t] []] nl [n] [a] [m] [e] ws [=] ws string nl [d] [e] [s] [c] [r] [i] [p] [t] [i] [o] [n] ws [=] ws string nl [h] [t] [t] [p] [s] [:] [/] [/] [.] [*] nl [^] [(] [A] [k] [s] [h] [a] [t] [h] [|] [R] [a] [g] [h] [a] [v] [|] [R] [a] [v] [i] [k] [i] [r] [a] [n] [)] [$] nl Task \n",
            "nl ::= [<U+000A>] \n",
            "string ::= [\"] string_20 [\"] \n",
            "Task ::= [t] [i] [t] [l] [e] ws [=] ws string nl [d] [e] [s] [c] [r] [i] [p] [t] [i] [o] [n] ws [=] ws string nl [^] [(] [A] [k] [s] [h] [a] [t] [h] [|] [R] [a] [g] [h] [a] [v] [|] [R] [a] [v] [i] [k] [i] [r] [a] [n] [)] [$] nl [d] [u] [e] [-] [d] [a] [t] [e] ws [=] ws array nl TaskUpdate \n",
            "number ::= number_7 number_13 number_17 \n",
            "number_7 ::= number_8 number_9 \n",
            "number_8 ::= [-] | \n",
            "number_9 ::= [0-9] | [1-9] number_10 \n",
            "number_10 ::= [0-9] number_10 | \n",
            "number_11 ::= [.] number_12 \n",
            "number_12 ::= [0-9] number_12 | [0-9] \n",
            "number_13 ::= number_11 | \n",
            "number_14 ::= [eE] number_15 number_16 \n",
            "number_15 ::= [-+] | \n",
            "number_16 ::= [0-9] number_16 | [0-9] \n",
            "number_17 ::= number_14 | \n",
            "string_18 ::= [^\"\\] | [\\] string_19 \n",
            "string_19 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
            "string_20 ::= string_18 string_20 | \n",
            "boolean ::= boolean_22 \n",
            "boolean_22 ::= [T] [r] [u] [e] | [F] [a] [l] [s] [e] \n",
            "TaskUpdate ::= [u] [p] [d] [a] [t] [e] [-] [t] [i] [m] [e] ws [=] ws number nl [c] [o] [m] [m] [e] [n] [t] ws [=] ws string nl [s] [t] [a] [t] [u] [s] ws [=] ws boolean \n",
            "array ::= [[] ws array_29 []] ws \n",
            "array_25 ::= due-date-value array_28 \n",
            "due-date-value ::= string \n",
            "array_27 ::= [,] ws due-date-value \n",
            "array_28 ::= array_27 array_28 | \n",
            "array_29 ::= array_25 | \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<llama_cpp.llama_grammar.LlamaGrammar at 0x2ace10d100b0>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grammar = GNBF(Project).generate_grammar('xml')\n",
        "GNBF.verify_grammar(grammar)\n",
        "\n",
        "grammar = GNBF(Project).generate_grammar('toml')\n",
        "GNBF.verify_grammar(grammar) "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
