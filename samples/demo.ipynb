{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Any, List, Tuple, Type, Optional, Union\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helpers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LLM:\n",
        "    def __init__(self):\n",
        "        self.client = OpenAI(\n",
        "            api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "        )\n",
        "\n",
        "    def invoke(self, config: dict):\n",
        "        with PromptContextManager(config) as filled_prompt:\n",
        "            return self.request(filled_prompt, temperature=0.01)\n",
        "\n",
        "    def request(self, prompt, temperature=0.2, context=None):\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "\n",
        "llm = LLM()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def log(msg):\n",
        "    print(msg)\n",
        "    print(\"-------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Making Prompts!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from grammarflow.prompt.builder import PromptBuilder  # Prompt builder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can using PromptBuilder to make your own prompt templates. Otherwise, you can just pass in your prompt as a string and `constrain` does the needful. \n",
        "\n",
        "If using PromptBuilder, you will add sections within your prompt. Each section takes these attributes: \n",
        "- text: str, Fixed text; Can have placeholders, but needs to be specified with `placeholder` attribute.\n",
        "- placeholder: str, Exact `placeholder` identifiers in `text`.\n",
        "- define_grammar: True/False, Can be used only once in the template. Defines the section where grammar will be explained.\n",
        "- add_few_shot_examples: True/False, Adds a filled-up Pydantic model to the prompt aligning with the grammar format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample Llama Prompt\n",
        "\n",
        "llama_prompt = PromptBuilder()\n",
        "llama_prompt.add_section(\n",
        "    text=\"<s>[INST] <<SYS>>\\n{system_context}\\n<</SYS>>\",\n",
        "    placeholders=\"system_context\",\n",
        "    define_grammar=True,\n",
        ")\n",
        "llama_prompt.add_section(\n",
        "    text=\"{user_message}[/INST]\",\n",
        "    placeholders=\"user_message\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using Constrain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from grammarflow.constrain import Constrain  # Main class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Begin by creating a `Constrain` object with a previously defined `prompt_config`. You can instead, just pass in a string which contains your prompt. \n",
        "\n",
        "2. Use `set_config` method on the `Constrain` instance to specify the output format and how the responses should be structured. In this case:\n",
        "   - `format`: Specifies the output format, here set to 'json'. Can be 'XML' or 'TOML' \n",
        "   - `return_sequence`: Determines how responses are returned. Can be 'single_response' or 'multiple_response'. \n",
        "\n",
        "3. The `format_prompt` method is used to assemble the final prompt using placeholders and task-specific configurations:\n",
        "   - `placeholders`: Needs to be a dict of {'placeholder': 'string'}. Required, if your prompt template contains placeholders. \n",
        "   - `grammars`: A list of grammars. Each task includes:\n",
        "       - `description`: A brief explanation of what the task entails or aims to achieve. Optional.\n",
        "       - `model`: Needs to be an empty pydantic model. \n",
        "\n",
        "4. Upon uisng `format_prompt`, `manager` holds the final prompt. You can get it by calling `manager.prompt`\n",
        "\n",
        "5. `manager.parse(llm_response)` takes in the response string and outputs the parsed object. It will return `Response` object, which is a simple wrapper for easy accessing of nested fields. \n",
        "\n",
        "5. You can view the inflation rate of tokens using `manager.inflation_rate()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_message = \"\"\n",
        "system_context = \"\"\n",
        "\n",
        "\n",
        "class Model(BaseModel):\n",
        "    model_name: str\n",
        "\n",
        "\n",
        "with Constrain(llama_prompt) as manager:\n",
        "    manager.set_config(\n",
        "        format='json',\n",
        "        return_sequence='single_response'\n",
        "    )\n",
        "\n",
        "    manager.format_prompt(placeholders={'user_message': user_message,\n",
        "                          'system_context': system_context},\n",
        "                          grammars=[{\n",
        "                              'model': [Model]},\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    prompt = manager.prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Examples! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Annotations(BaseModel): \n",
        "    materials: List[str] = Field(..., description=\"ADD SOMETHING\")\n",
        "    conditions: List[str]= Field(..., description=\"ADD SOMETHING\")\n",
        "    parameters: List[str]= Field(..., description=\"ADD SOMETHING\")\n",
        "    processes: List[str]= Field(..., description=\"ADD SOMETHING\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = PromptBuilder() \n",
        "prompt.add_section(\n",
        "  text=\"\"\"\n",
        "Your role is that of a DATA ANNOTATOR for research paper abstracts. You are expected to identify the materials used, different processes involved (such as types of chromatography, purification, methods, preparation, study, etc), and conditions (temperature units, quantity units, mathematical units, percentages, coefficients, any numbers) and any parameters- like factor names, rate, yield, etc mentioned in the abstract. \n",
        "\n",
        "I want you to look at the abstract given below and return all the key phrases you find. \n",
        "  \"\"\"\n",
        ") \n",
        "prompt.add_section(\n",
        "  define_grammar = True\n",
        ")\n",
        "prompt.add_section(\n",
        "  text=\"\"\"\n",
        "Here is an example: \n",
        "\n",
        "Abstract: Of the three particle sizes studied (10µm, 20µm, 50µm) only 10µm silica resin was able to produce purified API at the yield (>96%) and productivity (> 1kg/kg-resin/day) necessitated by the project. The second case study uses DoE studies to identify critical process parameters of column load, mobile phase solvent ratio and basic modifier level for a low-resolution, preparative, chiral separation.\n",
        "Annotations: \n",
        "```xml\n",
        "<materials>['silica resin'] </materials>\n",
        "<conditions>['10µm, 20µm, 50µm',  'only 10µm silica resin, ' yield (>96%)', 'productivity (> 1kg/kg-resin/day)',]</conditions> \n",
        "<parameters>[ ' column load', 'mobile phase solvent ratio']</parameters>\n",
        "<processes>[ 'chiral separation']</processes>\n",
        "```\n",
        "\n",
        "Every str object within the list you return for each of the tags must contain at least 2 words and not exceed 8 words. Remember that your role is to automate the data annotation process for  a chemistry based project. \n",
        "\n",
        "Begin!\n",
        "\"\"\"\n",
        ")\n",
        "prompt.add_section(\n",
        "  text=\"Abstract: {abstract}\\nAnnotations:\", \n",
        "  placeholders=[\"abstract\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "abstract = \"\"\" The simultaneous determination of multi-mycotoxins in food commodities are highly desirable due to their potential toxic effects and mass consumption of foods. Herein, liquid chromatography-quadrupole exactive orbitrap mass spectrometry was proposed to analyze multi-mycotoxins in commercial vegetable oils. Specifically, the method featured a successive liquid–liquid extraction process, in which the complementary solvents consisted of acetonitrile and water were optimized. Resultantly, matrix effects were reduced greatly. External calibration approach revealed good quantification property for each analyte. Under optimal conditions, the recovery ranging from 80.8% to 109.7%, relative standard deviation less than 11.7%, and good limit of quantification (0.35 to 45.4ng/g) were achieved. The high accuracy of proposed method was also validated. The detection of 20 commercial vegetable oils revealed that aflatoxins B1 and B2, zearalenone were observed in 10 real samples. The as-developed method is simple and low-cost, which merits the wide applications for scanning mycotoxins in oil matrices.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "with Constrain(prompt) as manager:\n",
        "    manager.set_config(\n",
        "        format='xml',\n",
        "        return_sequence='single_response'\n",
        "    )\n",
        "\n",
        "    manager.format_prompt(placeholders={\n",
        "        \"abstract\": abstract}, \n",
        "                          grammars=[{\n",
        "                              'model': [Annotations]},\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    prompt = manager.prompt\n",
        "    llm_response = llm.request(prompt)\n",
        "    response = manager.parse(llm_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"Annotations\": {\n",
            "        \"materials\": \"['commercial vegetable oils', 'acetonitrile', 'water']\",\n",
            "        \"conditions\": \"['liquid chromatography-quadrupole exactive orbitrap mass spectrometry', 'successive liquid\\u2013liquid extraction process', 'acetonitrile and water were optimized', 'under optimal conditions', 'good limit of quantification (0.35 to 45.4ng/g)']\",\n",
            "        \"parameters\": \"['recovery ranging', 'relative standard deviation', 'limit of quantification']\",\n",
            "        \"processes\": \"['liquid chromatography-quadrupole exactive orbitrap mass spectrometry', 'liquid\\u2013liquid extraction process', 'external calibration approach']\"\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"['liquid chromatography-quadrupole exactive orbitrap mass spectrometry', 'successive liquid–liquid extraction process', 'acetonitrile and water were optimized', 'under optimal conditions', 'good limit of quantification (0.35 to 45.4ng/g)']\""
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.Annotations.conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Stop here",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStop here\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mValueError\u001b[0m: Stop here"
          ]
        }
      ],
      "source": [
        "raise ValueError(\"Stop here\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_previous_interaction(id_): return id_ > 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Step(BaseModel):\n",
        "    thought: str = Field(..., description=\"This should concisely explain what you want to know for your goal.\")\n",
        "    action: str = Field(..., description=\"Your options: \\\n",
        "'load_md_file' (Provide the name of the file you want to load. Eg: 'README.md') | 'get_link_from_filename' (Provide the name (can be incomplete) of the file, and get it's link).\")\n",
        "    action_input: str = Field(..., description=\"The input for the action you want to take. Eg: 'README.md' | 'readme'\")\n",
        "    \n",
        "prompt = PromptBuilder() \n",
        "prompt.add_section(\n",
        "  text=\"Your role is that of a {role}. In this ongoing conversation, your goal is to {goal}.\\nYour final result should contain {deliverables}.\", \n",
        "  placeholders=[\"role\", \"goal\", \"deliverables\"]\n",
        ")\n",
        "prompt.add_section(\n",
        "  define_grammar=True\n",
        ") \n",
        "prompt.add_section(\n",
        "  text=\"\\nIn our previous interaction, you wanted to {thought} using {action}. You observed: {observation}.\",\n",
        "  placeholders=[\"thought\", \"action\", \"observation\"], \n",
        "  enable_on=check_previous_interaction\n",
        ")\n",
        "prompt.add_section(\n",
        "  text=\"Create the next Step in the conversation. Think through your reasoning and the action you want to take. Ensure that you are progressing towards your goal.\",\n",
        "  enable_on=check_previous_interaction\n",
        ")\n",
        "prompt.add_section(\n",
        "  text=\"Below is the history of the conversation so far.\\n{hsitory}\\n\",\n",
        "  placeholders=[\"history\"],\n",
        "  enable_on=check_previous_interaction\n",
        ") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "role = \"software developer trying to reproduce a codebase\"\n",
        "goal = \"create a roadmap to set up the environment a github repository\"\n",
        "deliverables = \"the links to the files needed in each step of your roadmap, and the code for each step\"\n",
        "thought = None \n",
        "action = None\n",
        "observation = None\n",
        "history = None\n",
        "\n",
        "id_ = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your role is that of a software developer trying to reproduce a codebase. In this ongoing conversation, your goal is to create a roadmap to set up the environment a github repository.\n",
            "Your final result should contain the links to the files needed in each step of your roadmap, and the code for each step.\n",
            "\n",
            "Here is the XML output format you are expected to return your response in.\n",
            "\n",
            "Your thinking state\n",
            "```\n",
            "<Step>\n",
            "<thought> #string# </thought> # This should concisely explain what you want to know for your goal.\n",
            "<action> #string# </action> # Your options: 'load_md_file' (Provide the name of the file you want to load. Eg: 'README.md') | 'get_link_from_filename' (Provide the name (can be incomplete) of the file, and get it's link).\n",
            "<action_input> #string# </action_input> # The input for the action you want to take. Eg: 'README.md' | 'readme'\n",
            "</Step>\n",
            "\n",
            "```\n",
            "\n",
            "RETURN ONLY ONE OF <Step>. DO NOT FORGET TO COVER YOUR OUTPUTS WITH '```'.\n"
          ]
        }
      ],
      "source": [
        "with Constrain(prompt) as manager: \n",
        "  manager.set_config(\n",
        "    format='xml'\n",
        "  ) \n",
        "  manager.format_prompt(\n",
        "    placeholders={ \n",
        "      \"role\": role, \n",
        "      \"goal\": goal, \n",
        "      \"deliverables\": deliverables,\n",
        "      \"thought\": thought, \n",
        "      \"action\": action,\n",
        "      \"observation\": observation,\n",
        "      \"history\": history\n",
        "    }, \n",
        "    grammars=[{\n",
        "      'description': 'Your thinking state', \n",
        "      'model': Step\n",
        "    }], \n",
        "    enable_on={\n",
        "      'id_':id_ \n",
        "    }\n",
        "  ) \n",
        "  \n",
        "  print(manager.prompt)\n",
        "  # resp = llm(manager.prompt, temperature=0.01)\n",
        "\n",
        "  # thought = response.Step.thought \n",
        "  # action = response.Step.action\n",
        "  # action_input = response.Step.action_input\n",
        "\n",
        "  # if action == \"load_md_file\":  \n",
        "  #   git.find_files(action_input)\n",
        "  # elif action == \"get_link_from_filename\":\n",
        "  #   git.find_files(action_input)\n",
        "  #   git.get_file_url(action_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "Exception",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m()\n",
            "\u001b[0;31mException\u001b[0m: "
          ]
        }
      ],
      "source": [
        "raise Exception()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```\n",
            "<FunctionModel>\n",
            "<function_name>fib</function_name>\n",
            "<docstring>Function to generate the Fibonacci sequence</docstring>\n",
            "<depedencies>numpy</depedencies>\n",
            "<uuid>123456789</uuid>\n",
            "<is_python>true</is_python>\n",
            "<code>\n",
            "def fib(n):\n",
            "    import numpy as np\n",
            "    a, b = 0, 1\n",
            "    result = []\n",
            "    for _ in range(n):\n",
            "        result.append(a)\n",
            "        a, b = b, a + b\n",
            "    return np.array(result)\n",
            "</code>\n",
            "</FunctionModel>\n",
            "```\n",
            "-------------\n",
            "{\n",
            "    \"FunctionModel\": {\n",
            "        \"function_name\": \"fib\",\n",
            "        \"docstring\": \"Function to generate the Fibonacci sequence\",\n",
            "        \"depedencies\": \"numpy\",\n",
            "        \"uuid\": 123456789,\n",
            "        \"is_python\": true,\n",
            "        \"code\": \"def fib(n):    import numpy as np    a, b = 0, 1    result = []    for _ in range(n):        result.append(a)        a, b = b, a + b    return np.array(result)\"\n",
            "    }\n",
            "}\n",
            "-------------\n",
            "{'before': 27, 'after': 146, 'factor': '4.4x'}\n",
            "-------------\n"
          ]
        }
      ],
      "source": [
        "# Here's a simple example of asking an LLM to make code.\n",
        "# This can be used within coding assistants which requires extra metadata.\n",
        "\n",
        "\n",
        "class FunctionModel(BaseModel):\n",
        "    function_name: str\n",
        "    docstring: str\n",
        "    depedencies: List[str]\n",
        "    uuid: Union[float, int]\n",
        "    is_python: bool\n",
        "    code: str\n",
        "\n",
        "\n",
        "input_str = \"I want to create a function that returns the fibonacci sequence. The function should be called 'fib'. The function can use numpy.\"\n",
        "\n",
        "with Constrain(input_str) as manager:\n",
        "    manager.set_config(format=\"xml\", return_sequence=\"single_response\")\n",
        "\n",
        "    manager.format_prompt(\n",
        "        grammars=[{\"description\": \"No Code Generation\", \"model\": FunctionModel}]\n",
        "    )\n",
        "\n",
        "    prompt = manager.prompt\n",
        "\n",
        "    llm_response = llm.request(prompt, temperature=0.01)\n",
        "    log(llm_response)\n",
        "\n",
        "    response = manager.parse(llm_response)\n",
        "    log(response)\n",
        "\n",
        "    log(manager.inflation_rate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# The response will be of `Response` type, which can be used to extract the data. If adding the parsed response to this object fails, it will return the dict itself.\n",
        "print(response.FunctionModel.is_python)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```\n",
            "<FunctionModel>\n",
            "<function_name> fib </function_name>\n",
            "<docstring> This function returns the Fibonacci sequence up to the input number. </docstring>\n",
            "<depedencies> ['numpy'] </depedencies>\n",
            "<uuid> 987654321 </uuid>\n",
            "<is_python> True </is_python>\n",
            "<code> def fib(n):\n",
            "    a, b = 0, 1\n",
            "    result = []\n",
            "    while a < n:\n",
            "        result.append(a)\n",
            "        a, b = b, a + b\n",
            "    return result </code>\n",
            "</FunctionModel>\n",
            "```\n",
            "-------------\n",
            "{\n",
            "    \"FunctionModel\": {\n",
            "        \"function_name\": \"fib\",\n",
            "        \"docstring\": \"This function returns the Fibonacci sequence up to the input number.\",\n",
            "        \"depedencies\": \"['numpy']\",\n",
            "        \"uuid\": 987654321,\n",
            "        \"is_python\": true,\n",
            "        \"code\": \"def fib(n):    a, b = 0, 1    result = []    while a < n:        result.append(a)        a, b = b, a + b    return result\"\n",
            "    }\n",
            "}\n",
            "-------------\n",
            "{'before': 27, 'after': 252, 'factor': '8.3x'}\n",
            "-------------\n"
          ]
        }
      ],
      "source": [
        "# Add some examples too!\n",
        "\n",
        "Sum_Function_Model = FunctionModel(\n",
        "    function_name=\"sum\",\n",
        "    docstring=\"This function returns the sum of the input list.\",\n",
        "    depedencies=[\"numpy\"],\n",
        "    uuid=123456789,\n",
        "    is_python=True,\n",
        "    code=\"def sum(a, b):\\n\\treturn a + b\"\n",
        ")\n",
        "\n",
        "\n",
        "with Constrain(input_str) as manager:\n",
        "    manager.set_config(\n",
        "        format='xml',\n",
        "        return_sequence='single_response'\n",
        "    )\n",
        "\n",
        "    manager.format_prompt(\n",
        "        grammars=[\n",
        "            {\n",
        "                'description': 'No Code Generation',\n",
        "                'model': FunctionModel\n",
        "            }\n",
        "        ],\n",
        "        examples=[\n",
        "            {\n",
        "                'query': \"Create a summation function in Python\",\n",
        "                'model': Sum_Function_Model\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    prompt = manager.prompt\n",
        "\n",
        "    llm_response = llm.request(prompt, temperature=0.01)\n",
        "    log(llm_response)\n",
        "\n",
        "    response = manager.parse(llm_response)\n",
        "    log(response)\n",
        "\n",
        "    log(manager.inflation_rate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```json\n",
            "{\n",
            "\"ThoughtState\": {\n",
            "\"thought\": \"Vladimir Putin is the President of Russia.\",\n",
            "\"goal\": \"To provide information about Vladimir Putin.\",\n",
            "\"tool\": \"Web_Search\",\n",
            "\"action\": \"Read\",\n",
            "\"action_input\": \"Vladimir Putin\",\n",
            "\"thought_id\": \"12345\"\n",
            "    }\n",
            "}\n",
            "```\n",
            "-------------\n",
            "{\n",
            "    \"ThoughtState\": {\n",
            "        \"thought\": \"Vladimir Putin is the President of Russia.\",\n",
            "        \"goal\": \"To provide information about Vladimir Putin.\",\n",
            "        \"tool\": \"Web_Search\",\n",
            "        \"action\": \"Read\",\n",
            "        \"action_input\": \"Vladimir Putin\",\n",
            "        \"thought_id\": \"12345\"\n",
            "    }\n",
            "}\n",
            "-------------\n",
            "{'before': 115, 'after': 277, 'factor': '1.4x'}\n",
            "-------------\n"
          ]
        }
      ],
      "source": [
        "# Sample ReAct Model with Llama Prompt\n",
        "# You can add descriptions within the grammar model to provide it's context and options. This is how we use the LLM in https://github.com/e-lab/Forestry_Student/.\n",
        "\n",
        "class ThoughtState(BaseModel):\n",
        "    thought: str\n",
        "    goal: str\n",
        "    tool: str = Field(...,\n",
        "                      description=\"Choose one of ['Web_QA', 'Web_Search', 'Web_Scraping', 'Web_Automation', 'Web_Research']\")\n",
        "    action: str = Field(...,\n",
        "                        description=\"Choose one of ['Create', 'Update', 'Delete', 'Read']\")\n",
        "    action_input: str = Field(..., description=\"The input data for the action\")\n",
        "    thought_id: Optional[str] = Field(\n",
        "        None, description=\"The unique identifier for the thought\")\n",
        "\n",
        "\n",
        "system_context = \"\"\"Your goal is to think and plan out how to solve questions using agent tools provided to you. Think about all aspects of your thought process.\"\"\"\n",
        "user_message = \"\"\"Who is Vladmir Putin?\"\"\"\n",
        "\n",
        "with Constrain(llama_prompt) as manager:\n",
        "    manager.set_config(\n",
        "        format='json',\n",
        "        return_sequence='single_response'\n",
        "    )\n",
        "\n",
        "    manager.format_prompt(placeholders={\n",
        "                          'user_message': user_message,\n",
        "                          'system_context': system_context\n",
        "                          },\n",
        "                          grammars=[{\n",
        "                              'description': 'This format describes your current thinking state',\n",
        "                              'model': [ThoughtState]},\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    prompt = manager.prompt\n",
        "\n",
        "    llm_response = llm.request(prompt, temperature=0.01)\n",
        "    log(llm_response)\n",
        "\n",
        "    response = manager.parse(llm_response)\n",
        "    log(response)\n",
        "\n",
        "    log(manager.inflation_rate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Web_Search\n"
          ]
        }
      ],
      "source": [
        "# You can then access the response from the `response` object\n",
        "print(response.ThoughtState.tool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "``` \n",
            "<Project>\n",
            "<name> Multimodal Document Understanding Project </name>\n",
            "<description> This project aims to develop a system that can understand and analyze multimodal documents, including text, images, and videos. </description>\n",
            "<project_url> www.multimodalproject.com </project_url>\n",
            "<team_members>\n",
            "    <TeamMember>\n",
            "        <name> John Doe </name>\n",
            "        <role> Project Manager </role>\n",
            "    </TeamMember>\n",
            "    <TeamMember>\n",
            "        <name> Jane Smith </name>\n",
            "        <role> Data Scientist </role>\n",
            "    </TeamMember>\n",
            "</team_members>\n",
            "<grammars>\n",
            "    <Task>\n",
            "        <title> Develop text analysis module </title>\n",
            "        <description> Implement natural language processing techniques to extract key information from text documents. </description>\n",
            "        <assigned_to>\n",
            "            <TeamMember>\n",
            "                <name> Jane Smith </name>\n",
            "                <role> Data Scientist </role>\n",
            "            </TeamMember>\n",
            "        </assigned_to>\n",
            "        <due_date> 2022-10-15 </due_date>\n",
            "        <updates>\n",
            "            <TaskUpdate>\n",
            "                <update_time> 1633660800 </update_time>\n",
            "                <comment> Completed initial data preprocessing </comment>\n",
            "                <status> true </status>\n",
            "            </TaskUpdate>\n",
            "        </updates>\n",
            "    </Task>\n",
            "    <Task>\n",
            "        <title> Develop image analysis module </title>\n",
            "        <description> Implement computer vision algorithms to analyze images and extract relevant features. </description>\n",
            "        <assigned_to>\n",
            "            <TeamMember>\n",
            "                <name> John Doe </name>\n",
            "                <role> Project Manager </role>\n",
            "            </TeamMember>\n",
            "        </assigned_to>\n",
            "        <due_date> 2022-11-01 </due_date>\n",
            "        <updates>\n",
            "            <TaskUpdate>\n",
            "                <update_time> 1635724800 </update_time>\n",
            "                <comment> Completed image feature extraction </comment>\n",
            "                <status> true </status>\n",
            "            </TaskUpdate>\n",
            "        </updates>\n",
            "    </Task>\n",
            "</grammars>\n",
            "</Project>\n",
            "```\n",
            "-------------\n",
            "{\n",
            "    \"Project\": {\n",
            "        \"name\": \"Multimodal Document Understanding Project\",\n",
            "        \"description\": \"This project aims to develop a system that can understand and analyze multimodal documents, including text, images, and videos.\",\n",
            "        \"project_url\": \"www.multimodalproject.com\",\n",
            "        \"team_members\": {\n",
            "            \"TeamMember\": [\n",
            "                {\n",
            "                    \"name\": \"John Doe\",\n",
            "                    \"role\": \"Project Manager\"\n",
            "                },\n",
            "                {\n",
            "                    \"name\": \"Jane Smith\",\n",
            "                    \"role\": \"Data Scientist\"\n",
            "                }\n",
            "            ]\n",
            "        },\n",
            "        \"grammars\": {\n",
            "            \"Task\": [\n",
            "                {\n",
            "                    \"title\": \"Develop text analysis module\",\n",
            "                    \"description\": \"Implement natural language processing techniques to extract key information from text documents.\",\n",
            "                    \"assigned_to\": {\n",
            "                        \"TeamMember\": {\n",
            "                            \"name\": \"Jane Smith\",\n",
            "                            \"role\": \"Data Scientist\"\n",
            "                        }\n",
            "                    },\n",
            "                    \"due_date\": \"2022-10-15\",\n",
            "                    \"updates\": {\n",
            "                        \"TaskUpdate\": {\n",
            "                            \"update_time\": 1633660800,\n",
            "                            \"comment\": \"Completed initial data preprocessing\",\n",
            "                            \"status\": true\n",
            "                        }\n",
            "                    }\n",
            "                },\n",
            "                {\n",
            "                    \"title\": \"Develop image analysis module\",\n",
            "                    \"description\": \"Implement computer vision algorithms to analyze images and extract relevant features.\",\n",
            "                    \"assigned_to\": {\n",
            "                        \"TeamMember\": {\n",
            "                            \"name\": \"John Doe\",\n",
            "                            \"role\": \"Project Manager\"\n",
            "                        }\n",
            "                    },\n",
            "                    \"due_date\": \"2022-11-01\",\n",
            "                    \"updates\": {\n",
            "                        \"TaskUpdate\": {\n",
            "                            \"update_time\": 1635724800,\n",
            "                            \"comment\": \"Completed image feature extraction\",\n",
            "                            \"status\": true\n",
            "                        }\n",
            "                    }\n",
            "                }\n",
            "            ]\n",
            "        }\n",
            "    }\n",
            "}\n",
            "-------------\n",
            "{'before': 306, 'after': 540, 'factor': '0.8x'}\n",
            "-------------\n"
          ]
        }
      ],
      "source": [
        "# You can add complex layers of grammars. You add even using Optional and Union types.\n",
        "\n",
        "class TeamMember(BaseModel):\n",
        "    name: str\n",
        "    role: str\n",
        "\n",
        "\n",
        "class TaskUpdate(BaseModel):\n",
        "    update_time: float\n",
        "    comment: Optional[str] = None\n",
        "    status: bool\n",
        "\n",
        "\n",
        "class Task(BaseModel):\n",
        "    title: str\n",
        "    description: str\n",
        "    assigned_to: List[TeamMember]\n",
        "    due_date: List[str]\n",
        "    updates: List[TaskUpdate]\n",
        "\n",
        "\n",
        "class Project(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    project_url: Optional[str] = None\n",
        "    team_members: List[TeamMember]\n",
        "    grammars: Task\n",
        "\n",
        "\n",
        "with Constrain(llama_prompt) as manager:\n",
        "    manager.set_config(\n",
        "        format='xml',\n",
        "        return_sequence='single_response'\n",
        "    )\n",
        "\n",
        "    system_context = \"\"\"You are a project manager and you are responsible for managing a project. You have to manage the project, it's grammars and other aspects. Ensure that you fill out all required fields.\"\"\"\n",
        "    user_message = \"\"\"Make me a project plan for a new project on multimodal document understanding projct.\"\"\"\n",
        "\n",
        "    manager.format_prompt(placeholders={'user_message': user_message,\n",
        "                          'system_context': system_context},\n",
        "                          grammars=[{\n",
        "                              'description': 'This format elaborates on the project and its grammars.',\n",
        "                              'model': [Project]},\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    prompt = manager.prompt\n",
        "\n",
        "    llm_response = llm.request(prompt, temperature=0.01)\n",
        "    log(llm_response)\n",
        "\n",
        "    response = manager.parse(llm_response)\n",
        "    log(response)\n",
        "\n",
        "    log(manager.inflation_rate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"Task\": [\n",
            "        {\n",
            "            \"title\": \"Develop text analysis module\",\n",
            "            \"description\": \"Implement natural language processing techniques to extract key information from text documents.\",\n",
            "            \"assigned_to\": {\n",
            "                \"TeamMember\": {\n",
            "                    \"name\": \"Jane Smith\",\n",
            "                    \"role\": \"Data Scientist\"\n",
            "                }\n",
            "            },\n",
            "            \"due_date\": \"2022-10-15\",\n",
            "            \"updates\": {\n",
            "                \"TaskUpdate\": {\n",
            "                    \"update_time\": 1633660800,\n",
            "                    \"comment\": \"Completed initial data preprocessing\",\n",
            "                    \"status\": true\n",
            "                }\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"title\": \"Develop image analysis module\",\n",
            "            \"description\": \"Implement computer vision algorithms to analyze images and extract relevant features.\",\n",
            "            \"assigned_to\": {\n",
            "                \"TeamMember\": {\n",
            "                    \"name\": \"John Doe\",\n",
            "                    \"role\": \"Project Manager\"\n",
            "                }\n",
            "            },\n",
            "            \"due_date\": \"2022-11-01\",\n",
            "            \"updates\": {\n",
            "                \"TaskUpdate\": {\n",
            "                    \"update_time\": 1635724800,\n",
            "                    \"comment\": \"Completed image feature extraction\",\n",
            "                    \"status\": true\n",
            "                }\n",
            "            }\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# The Response object allows for easy access\n",
        "print(response.Project.grammars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```\n",
            "[EventIdea]\n",
            "event_name = \"Birthday Party for Girlfriend\"\n",
            "event_description = \"A special celebration for my girlfriend's birthday\"\n",
            "event_duration = \"3 hours\"\n",
            "\n",
            "[BudgetPlan]\n",
            "budget = 500\n",
            "items = [\"Cake\", \"Balloons\", \"Roses\", \"Ice Cream\"]\n",
            "prices = [50, 20, 30, 40]\n",
            "total_cost = 140\n",
            "\n",
            "[EventSchedule]\n",
            "event_name = \"Birthday Party for Girlfriend\"\n",
            "event_time = 12\n",
            "event_duration = \"3 hours\"\n",
            "```\n",
            "-------------\n",
            "{\n",
            "    \"EventIdea\": [\n",
            "        {\n",
            "            \"event_name\": \"Birthday Party for Girlfriend\",\n",
            "            \"event_description\": \"A special celebration for my girlfriend's birthday\",\n",
            "            \"event_duration\": \"3 hours\"\n",
            "        }\n",
            "    ],\n",
            "    \"BudgetPlan\": [\n",
            "        {\n",
            "            \"budget\": 500,\n",
            "            \"items\": [\n",
            "                \"Cake\",\n",
            "                \"Balloons\",\n",
            "                \"Roses\",\n",
            "                \"Ice Cream\"\n",
            "            ],\n",
            "            \"prices\": [\n",
            "                50,\n",
            "                20,\n",
            "                30,\n",
            "                40\n",
            "            ],\n",
            "            \"total_cost\": 140\n",
            "        }\n",
            "    ],\n",
            "    \"EventSchedule\": [\n",
            "        {\n",
            "            \"event_name\": \"Birthday Party for Girlfriend\",\n",
            "            \"event_time\": 12,\n",
            "            \"event_duration\": \"3 hours\"\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "-------------\n",
            "{'before': 48, 'after': 198, 'factor': '3.1x'}\n",
            "-------------\n"
          ]
        }
      ],
      "source": [
        "# You can add multiple grammars to the same prompt. NOT RECOMMENDED.\n",
        "\n",
        "class EventIdea(BaseModel):\n",
        "    event_name: str\n",
        "    event_description: str\n",
        "    event_duration: str\n",
        "\n",
        "\n",
        "class BudgetPlan(BaseModel):\n",
        "    budget: float\n",
        "    items: List[str]\n",
        "    prices: List[int]\n",
        "    total_cost: int\n",
        "\n",
        "\n",
        "class EventSchedule(BaseModel):\n",
        "    event_name: str\n",
        "    event_time: float\n",
        "    event_duration: str\n",
        "\n",
        "\n",
        "prompt = \"I am hosting a birthday party for my girlfriend tomorrow. I want to buy a cake, balloons, some roses and ice cream. I have a budget of 500$. Can you create a sample event schedule and budget plan for me?.\"\n",
        "\n",
        "with Constrain(prompt) as manager:\n",
        "    manager.set_config(\n",
        "        format=\"toml\",\n",
        "        return_sequence=\"multi_response\",\n",
        "    )\n",
        "\n",
        "    manager.format_prompt(\n",
        "        grammars=[\n",
        "            {\"task_description\": \"Brainstorming Event Ideas\", \"model\": EventIdea},\n",
        "            {\n",
        "                \"task_description\": \"Budget Planning And Activity Planning\",\n",
        "                \"model\": [BudgetPlan, EventSchedule],\n",
        "            },\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    prompt = manager.prompt\n",
        "\n",
        "    llm_response = llm.request(prompt, temperature=0.01)\n",
        "    log(llm_response)\n",
        "\n",
        "    response = manager.parse(llm_response)\n",
        "    log(response)\n",
        "\n",
        "    log(manager.inflation_rate())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Grammars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> \"GBNF (GGML BNF) is a format for defining formal grammars to constrain model outputs in llama.cpp. For example, you can use it to force the model to generate valid JSON, or speak only in emojis.\"\n",
        "\n",
        "Read more about it here: https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from grammarflow.grammars.gnbf import GNBF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grammar = GNBF(Project).generate_grammar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root ::= project ws\n",
            "project ::= \"{\" ws \"\\\"name\\\":\" ws string \",\" ws \"\\\"description\\\":\" ws string \",\" ws \"\\\"project-url\\\":\" ws string \",\" ws \"\\\"team-members\\\":\" ws teammember \",\" ws \"\\\"grammars\\\":\" ws grammars \"}\" ws\n",
            "ws ::= [ \\t\\n]*\n",
            "string ::=  \"\\\"\" (\n",
            "            [^\"\\\\] |\n",
            "            \"\\\\\" ([\"\\\\/bfnrt] | \"u\" [0-9a-fa-f] [0-9a-fa-f] [0-9a-fa-f] [0-9a-fa-f])\n",
            "            )* \"\\\"\"\n",
            "teammember ::= \"{\" ws \"\\\"name\\\":\" ws string \",\" ws \"\\\"role\\\":\" ws string \"}\" ws\n",
            "number ::= (\"-\"? ([0-9] | [1-9] [0-9]*)) (\".\" [0-9]+)? ([ee] [-+]? [0-9]+)?\n",
            "taskupdate ::= \"{\" ws \"\\\"update-time\\\":\" ws number \",\" ws \"\\\"comment\\\":\" ws string \",\" ws \"\\\"status\\\":\" ws status \"}\" ws\n",
            "array ::= \"[\" ws (\n",
            "                due-date-value\n",
            "                (\",\" ws due-date-value)*\n",
            "            )? \"]\" ws\n",
            "due-date-value ::= string\n",
            "task ::= \"{\" ws \"\\\"title\\\":\" ws string \",\" ws \"\\\"description\\\":\" ws string \",\" ws \"\\\"assigned-to\\\":\" ws teammember \",\" ws \"\\\"due-date\\\":\" ws array \",\" ws \"\\\"updates\\\":\" ws taskupdate \"}\" ws\n",
            "-------------\n"
          ]
        }
      ],
      "source": [
        "log(grammar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "from_string grammar:\n",
            "root ::= project ws \n",
            "project ::= [{] ws [\"] [n] [a] [m] [e] [\"] [:] ws string [,] ws [\"] [d] [e] [s] [c] [r] [i] [p] [t] [i] [o] [n] [\"] [:] ws string [,] ws [\"] [p] [r] [o] [j] [e] [c] [t] [-] [u] [r] [l] [\"] [:] ws string [,] ws [\"] [t] [e] [a] [m] [-] [m] [e] [m] [b] [e] [r] [s] [\"] [:] ws teammember [,] ws [\"] [g] [r] [a] [m] [m] [a] [r] [s] [\"] [:] ws grammars [}] ws \n",
            "ws ::= ws_6 \n",
            "string ::= [\"] string_9 [\"] \n",
            "teammember ::= [{] ws [\"] [n] [a] [m] [e] [\"] [:] ws string [,] ws [\"] [r] [o] [l] [e] [\"] [:] ws string [}] ws \n",
            "print_grammar: error printing grammar: malformed rule, does not end with LLAMA_GRETYPE_END: 5\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<llama_cpp.llama_grammar.LlamaGrammar at 0x7fc102ff2c70>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Using llama.cpp, we can verify if our grammar string is accepted.\n",
        "GNBF.verify_grammar(grammar)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
