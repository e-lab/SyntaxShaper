{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: **The point of this notebook is to show a negligble 'n_badcalls' value (where the model outputs something unparseable) at HotPotQA.**\n",
    "> **With proper web-search tools, and proper formatting, there'd be a much higher success rate than what you'd get with this notebook. For comparison purposes, I've done my best to maintain the code from the original authors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from grammarflow import *\n",
    "from pydantic import BaseModel, Field\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    def __init__(self):\n",
    "        self.client = openai.OpenAI(\n",
    "            api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        )\n",
    "\n",
    "    def invoke(self, config: dict):\n",
    "        with PromptContextManager(config) as filled_prompt:\n",
    "            return self.request(filled_prompt, temperature=0.01)\n",
    "\n",
    "    def __call__(self, prompt, temperature=0.2, context=None):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "llm = LLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikienv, wrappers\n",
    "env = wikienv.WikiEnv()\n",
    "env = wrappers.HotPotQAWrapper(env, split=\"dev\")\n",
    "env = wrappers.LoggingWrapper(env)\n",
    "\n",
    "def step(env, action):\n",
    "    attempts = 0\n",
    "    while attempts < 10:\n",
    "        try:\n",
    "            return env.step(action)\n",
    "        except requests.exceptions.Timeout:\n",
    "            attempts += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GrammarFlow\n",
    "\n",
    "> HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems.\n",
    "\n",
    "[ReAct](https://github.com/ysymyth/ReAct) adds a bunch of useful helper functions to their evaluation code for prompting against this dataset. This sample directory cleans up their code and adds GrammarFlow to it. \n",
    "\n",
    "All we need to do is: \n",
    "1. Create a Step model to constrain our thinking state\n",
    "2. Create a Prompt for our ReAct flow. Our `make_prompt()` function takes care of it. \n",
    "3. Within the original `webthink()` function, we make changes along the lines of: \n",
    "  - Tracking and logging all steps between iterations\n",
    "  - Use `Constrain()` from GrammarFlow to make the extraction of 'Thought', 'Action' and 'Action_Input' pythonic and error-prone. \n",
    "\n",
    "Note: Remember that prompts are stateful, and thus need to be discarded after every `Constrain()` block. That's why we use `make_prompt()`\n",
    "\n",
    "Our goal was to show that we can easily infuse existing prompting code with GrammarFlow! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GrammarFlow Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step(BaseModel):\n",
    "    thought: str = Field(..., description=\"Concisely describe your thought process in your current thinking step. Use your previous step's output to guide your current step.\")\n",
    "    action: str = Field(..., description=\"You only have 3 options: search | lookup | finish\")\n",
    "    action_input: str = Field(..., description=\"Your input to the above action.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_previous_interaction(id_): return id_ > 1\n",
    "\n",
    "def make_prompt(): \n",
    "  prompt = PromptBuilder() \n",
    "  prompt.add_section(\n",
    "    text=\"\"\"\n",
    "  {question}\n",
    "\n",
    "  Your goal is to solve the above QA task in steps. First, think about what information you need to answer the question. You have access to tools as defined below:\n",
    "  1. `search`: to get the information you need from WIKIPEDIA. This is not a search engine. This needs a single keyword or noun as input. \n",
    "  2. `lookup`: to find more information from the paragraph returned by search. This matches the action_input you give to setences in search. \n",
    "  3. `finish`: to return your final complete answer as input field. Use this if you believe you have enough information to completely answer the task.\"\"\", \n",
    "    placeholders=[\"question\"]\n",
    "  ) \n",
    "  prompt.add_section(\n",
    "    define_grammar=True\n",
    "  ) \n",
    "  prompt.add_section(\n",
    "    text=\"Use the below Similar: [] keywords for search. Context: \\n{history}\",\n",
    "    placeholders=[\"history\"],\n",
    "    enable_on=check_previous_interaction\n",
    "  ) \n",
    "  prompt.add_section(\n",
    "    text=\"Create the next [Step] {id_} using the information available above: \", \n",
    "    placeholders=[\"id_\"]\n",
    "  )\n",
    "  return prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = wrappers.EXAMPLE\n",
    "question = \"\" # Will be defined in later blocks. Re-using ReAct's example for now.\n",
    "\n",
    "# Helper functions \n",
    "def load_history(history: dict): \n",
    "  return \"\\n\".join([f\"\"\"thought {id_}: {value['thought']}\\naction {id_}: {value['action']}\\naction_input {id_}: {value['action_input']}\\nObservation {id_}: {value['observation']}\"\"\"\n",
    "   for id_, value in history.items()])\n",
    "\n",
    "def log_step(response, observation, id_, to_print=True): \n",
    "  if to_print:\n",
    "    print('--------------------------')\n",
    "    print('Step {}'.format(id_))\n",
    "    print('Thought:', response.Step.thought)\n",
    "    print('Action:', response.Step.action)\n",
    "    print('Action Input:', response.Step.action_input)\n",
    "    print('Observation:', observation)\n",
    "    print('--------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def webthink(idx=None, env=env, to_print=False): \n",
    "  # Initializing Stateful vars\n",
    "  thought = None \n",
    "  action = None\n",
    "  observation = None\n",
    "  id_ = 1\n",
    "  history, history_ = {}, None\n",
    "  n_calls, n_badcalls = 0, 0\n",
    "\n",
    "  # Initializing question\n",
    "  question = env.reset(idx=idx)\n",
    "  print(question)\n",
    "\n",
    "  for i in range(7):\n",
    "    if history:\n",
    "      history_ = load_history(history) # Initializes history every run \n",
    "\n",
    "    # The only major change we've made! \n",
    "    with Constrain(make_prompt()) as manager: \n",
    "      manager.set_config(\n",
    "        format='toml'\n",
    "      ) \n",
    "      manager.format_prompt(\n",
    "        placeholders={ \n",
    "          \"question\": question,\n",
    "          'example': example, \n",
    "          \"history\": history_\n",
    "        }, \n",
    "        grammars=[{\n",
    "          'description': 'Your thinking state:', \n",
    "          'model': Step\n",
    "        }], \n",
    "        enable_on={\n",
    "          'id_':id_ \n",
    "        }\n",
    "      ) \n",
    "      \n",
    "      prompt = manager.prompt\n",
    "      response = llm(prompt, temperature=0.01)\n",
    "      n_calls += 1\n",
    "\n",
    "      try: \n",
    "        response = manager.parse(response)\n",
    "        observation, r, done, info = step(env, f\"{response.Step.action}[{response.Step.action_input}]\")\n",
    "        observation = observation.replace(\"\\\\n\", \" \")\n",
    "      except Exception as e:\n",
    "        n_badcalls += 1\n",
    "        observation, done = \"Error in parsing. Follow the [Step] format and try again.\", False \n",
    "\n",
    "    log_step(response, observation, id_, to_print=to_print)\n",
    "    \n",
    "    history[id_] = { \n",
    "      \"thought\": response.Step.thought, \n",
    "      \"action\": response.Step.action, \n",
    "      \"action_input\": response.Step.action_input, \n",
    "      \"observation\": observation\n",
    "    }\n",
    "\n",
    "    print(id_)\n",
    "    id_ += 1\n",
    "\n",
    "    if done: \n",
    "      break \n",
    "\n",
    "  if not done:\n",
    "      final = \"Failed\"\n",
    "      observation, r, done, info = step(env, \"finish[]\")\n",
    "\n",
    "  info.update({'n_calls': n_calls, 'n_badcalls': n_badcalls, 'steps': id_})\n",
    "\n",
    "  return r, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = list(range(7409))\n",
    "random.Random(221).shuffle(idxs)\n",
    "\n",
    "rs = []\n",
    "infos = [] \n",
    "bad_calls = 0\n",
    "old_time = time.time()\n",
    "for i in idxs[:4]:\n",
    "    r, info = webthink(i, to_print=False)\n",
    "    rs.append(info['em'])\n",
    "    bad_calls += info['n_badcalls']\n",
    "    infos.append(info)\n",
    "    print('RESULTS: ', sum(rs), len(rs), sum(rs) / len(rs), (time.time() - old_time) / len(rs))\n",
    "    print('--------------------------')\n",
    "    print()\n",
    "\n",
    "print('Bad calls: ', bad_calls) \n",
    "print(infos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üëç"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
