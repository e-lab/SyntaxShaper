{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "os.environ['PYTHONPATH'] = '/home/aksha/Workbench/Research/Labs/e-lab/parser/constrain'\n",
    "from pydantic import BaseModel, Field\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Union, Optional, Type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constrain.grammars.gnbf import GNBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'ThoughtState', 'type': 'object', 'properties': {'thought': {'title': 'Thought', 'type': 'string'}, 'goal': {'title': 'Goal', 'type': 'string'}, 'tool': {'title': 'Tool', 'description': \"Choose one of ['Web_QA', 'Web_Search', 'Web_Scraping', 'Web_Automation', 'Web_Research']\", 'type': 'string'}, 'action': {'title': 'Action', 'description': \"Choose one of ['Create', 'Update', 'Delete', 'Read']\", 'type': 'string'}, 'action_input': {'title': 'Action Input', 'description': 'The input data for the action', 'type': 'string'}, 'thought_id': {'title': 'Thought Id', 'description': 'The unique identifier for the thought', 'type': 'string'}}, 'required': ['thought', 'goal', 'tool', 'action', 'action_input']}\n",
      "-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "root ::= thoughtstate ws \n",
      "thoughtstate ::= [{] ws [\"] [t] [h] [o] [u] [g] [h] [t] [\"] [:] ws string [,] ws [\"] [g] [o] [a] [l] [\"] [:] ws string [,] ws [\"] [t] [o] [o] [l] [\"] [:] ws string [,] ws [\"] [a] [c] [t] [i] [o] [n] [\"] [:] ws string [,] ws [\"] [a] [c] [t] [i] [o] [n] [-] [i] [n] [p] [u] [t] [\"] [:] ws string [,] ws [\"] [t] [h] [o] [u] [g] [h] [t] [-] [i] [d] [\"] [:] ws string [}] ws \n",
      "ws ::= ws_4 \n",
      "string ::= [\"] string_7 [\"] ws \n",
      "ws_4 ::= [ <U+0009><U+000A>] ws_4 | \n",
      "string_5 ::= [^\"\\] | [\\] string_6 \n",
      "string_6 ::= [\"\\/bfnrt] | [u] [0-9a-fa-f] [0-9a-fa-f] [0-9a-fa-f] [0-9a-fa-f] \n",
      "string_7 ::= string_5 string_7 | \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<llama_cpp.llama_grammar.LlamaGrammar at 0x7f43b05f7c70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ThoughtState(BaseModel):\n",
    "    thought: str \n",
    "    goal: str \n",
    "    tool: str = Field(..., description=\"Choose one of ['Web_QA', 'Web_Search', 'Web_Scraping', 'Web_Automation', 'Web_Research']\")\n",
    "    action: str = Field(..., description=\"Choose one of ['Create', 'Update', 'Delete', 'Read']\")\n",
    "    action_input: str = Field(..., description=\"The input data for the action\")\n",
    "    thought_id: Optional[str] = Field(None, description=\"The unique identifier for the thought\")\n",
    "    \n",
    "\n",
    "print(ThoughtState.schema())\n",
    "print('-------------------')\n",
    "converter = GNBF(ThoughtState.schema())\n",
    "grammar = converter.generate_grammar()\n",
    "converter.verify_grammar(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "root ::= thoughtstate ws \n",
      "thoughtstate ::= [{] ws [\"] [t] [h] [o] [u] [g] [h] [t] [\"] [:] ws string [,] ws [\"] [g] [o] [a] [l] [\"] [:] ws string [,] ws [\"] [t] [o] [o] [l] [\"] [:] ws string [,] ws [\"] [a] [c] [t] [i] [o] [n] [\"] [:] ws string [,] ws [\"] [a] [c] [t] [i] [o] [n] [-] [i] [n] [p] [u] [t] [\"] [:] ws string [,] ws [\"] [t] [h] [o] [u] [g] [h] [t] [-] [i] [d] [\"] [:] ws string [}] ws \n",
      "ws ::= ws_4 \n",
      "string ::= [\"] string_7 [\"] ws \n",
      "ws_4 ::= [ <U+0009><U+000A>] ws_4 | \n",
      "string_5 ::= [^\"\\] | [\\] string_6 \n",
      "string_6 ::= [\"\\/bfnrt] | [u] [0-9a-fa-f] [0-9a-fa-f] [0-9a-fa-f] [0-9a-fa-f] \n",
      "string_7 ::= string_5 string_7 | \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<llama_cpp.llama_grammar.LlamaGrammar at 0x7f467e6f8ee0>"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root ::= thoughtstate ws\n",
      "thoughtstate ::= \"{\" ws \"\\\"thought\\\":\" ws string \",\" ws \"\\\"goal\\\":\" ws string \",\" ws \"\\\"tool\\\":\" ws string \",\" ws \"\\\"action\\\":\" ws string \",\" ws \"\\\"action-input\\\":\" ws string \",\" ws \"\\\"thought-id\\\":\" ws string \"}\" ws\n",
      "ws ::= [ \\t\\n]*\n",
      "string ::= \n",
      "            \"\\\"\" (\n",
      "            [^\"\\\\] |\n",
      "            \"\\\\\" ([\"\\\\/bfnrt] | \"u\" [0-9a-fa-f] [0-9a-fa-f] [0-9a-fa-f] [0-9a-fa-f])\n",
      "            )* \"\\\"\" ws\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f4ec0435db64d8f871fa7d2a1485213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b37dda90c274b65ad3c2f1511cd5c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9ba81e2617471e88c9a4bf6939485a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012ded9fef03442983611804165f8856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" \n",
    "```\n",
    "{\n",
    "\"ThoughtState\": {\n",
    "\"thought\": \"Vladimir Putin is the current President of Russia.\",\n",
    "\"goal\": \"To provide information about Vladimir Putin.\",\n",
    "\"tool\": \"Web_Search\",\n",
    "\"action\": \"Read\",\n",
    "\"action_input\": \"Vladimir Putin biography\",\n",
    "\"thought_id\": \"12345\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I'm not sure what you mean by that.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\", padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "# source: https://huggingface.co/microsoft/DialoGPT-medium\n",
    "\n",
    "# encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "new_user_input_ids = tokenizer.encode(\"Can you generate a simple JSON object?\" + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "# append the new user input tokens to the chat history\n",
    "bot_input_ids = new_user_input_ids\n",
    "\n",
    "# generated a response while limiting the total chat history to 1000 tokens, \n",
    "chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# pretty print last output tokens from bot\n",
    "print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_tokens_ids = [tokenizer.encode(token)[0] for token in ['{', '}', '\"', ':', ',']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_boost_factor = 1.1  # Slightly boost content token probabilities\n",
    "structure_boost_factor = 5  # Significantly boost terminal token probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.10.0 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from accelerate) (2.1.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from accelerate) (0.19.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: pyyaml in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from accelerate) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: psutil in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: sympy in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: filelock in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: jinja2 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: networkx in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: typing-extensions in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: fsspec in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: requests in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.27.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaee1e7b37144b42a176e8348acbb711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/92.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d843f6d493524eecbabc9d8a01fb5954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e56cc10d35d4a0a8efc2dbd977c3d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00019.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmistralai/Mixtral-8x7B-v0.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mixtral-8x7B-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy favourite condiment is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/transformers/modeling_utils.py:3264\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3261\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[1;32m   3263\u001b[0m     \u001b[38;5;66;03m# rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 3264\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3273\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3275\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3276\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3277\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3280\u001b[0m     is_safetensors_available()\n\u001b[1;32m   3281\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m   3282\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3283\u001b[0m ):\n\u001b[1;32m   3284\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/transformers/utils/hub.py:1038\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1038\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    413\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/huggingface_hub/file_download.py:1461\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1458\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1459\u001b[0m             _check_disk_space(expected_size, local_dir)\n\u001b[0;32m-> 1461\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1471\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblob_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/huggingface_hub/file_download.py:541\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, _nb_retries)\u001b[0m\n\u001b[1;32m    539\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 541\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    543\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/urllib3/response.py:1033\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1033\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1036\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/urllib3/response.py:925\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    923\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 925\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    927\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/urllib3/response.py:852\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    849\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 852\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    854\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    861\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/urllib3/response.py:835\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/http/client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 463\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/http/client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    502\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/ssl.py:1275\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1272\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1274\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/ssl.py:1133\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
    "\n",
    "prompt = \"My favourite condiment is\"\n",
    "\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': 'value',\n",
       " 'array': [1, 2, 3],\n",
       " 'bro': {'ayo': 'huh'},\n",
       " 'nested': {'another_key': 'another_value'}}"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_string = r'{\"key\": \"value\", \"array\": [1, 2, 3], \"bro\": {\"ayo\": \"huh\"}, \"nested\": {\"another_key\": \"another_value\"}} ohhhh'\n",
    "JSON.parse(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'products': [{'name': 'Hammer', 'sku': 738594937}, {}, {'name': 'Nail', 'sku': 284758393, 'color': 'gray'}], 'products.sub': [{'hey': 'yo'}]}\n"
     ]
    }
   ],
   "source": [
    "toml_string = '''\n",
    "[products]\n",
    "name = \"Hammer\"\n",
    "sku = 738594937\n",
    "\n",
    "[products]\n",
    "\n",
    "[products]\n",
    "name = \"Nail\"\n",
    "sku = 284758393\n",
    "color = \"gray\"\n",
    "\n",
    "[products.sub]\n",
    "hey = \"yo\"\n",
    "'''\n",
    "print(TOML.parse(toml_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"studentsList\": {\n",
      "        \"student\": {\n",
      "            \"attributes\": {\n",
      "                \"ind\": \"2\"\n",
      "            },\n",
      "            \"firstName\": {\n",
      "                \"value\": \"Wirt\"\n",
      "            },\n",
      "            \"lastName\": {\n",
      "                \"value\": \"Wood\"\n",
      "            },\n",
      "            \"certificate\": {\n",
      "                \"value\": \"True\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\" \n",
    "<studentsList>\n",
    "    <student id=\"1\">\n",
    "        <firstName>Greg</firstName>\n",
    "        <lastName>Dean</lastName>\n",
    "        <certificate>True</certificate>\n",
    "        <scores>\n",
    "            <module1>70</module1>\n",
    "            <module12>80</module12>\n",
    "            <module3>90</module3>\n",
    "        </scores>\n",
    "    </student>\n",
    "    <student ind=\"2\">\n",
    "        <firstName>Wirt</firstName>\n",
    "        <lastName>Wood</lastName>\n",
    "        <certificate>True</certificate>\n",
    "    </student>\n",
    "</studentsList>\n",
    "\"\"\"\n",
    "\n",
    "text = \"\"\" \n",
    "<studentsList>\n",
    "    <student ind=\"2\">\n",
    "        <firstName>Wirt</firstName>\n",
    "        <lastName>Wood</lastName>\n",
    "        <certificate>True</certificate>\n",
    "    </student>\n",
    "</studentsList>\n",
    "\"\"\"\n",
    "\n",
    "print(json.dumps(parse_xml(text), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex:\n",
      "```\n",
      "<Complex>\n",
      "<real> #float# </real>\n",
      "<imaginary> #float# </imaginary>\n",
      "</Complex>\n",
      "```\n",
      "\n",
      "\n",
      "Complex:\n",
      "```\n",
      "[Complex]\n",
      "real = # Type: float\n",
      "imaginary = # Type: float\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "{\n",
      "\"Complex\": {\n",
      "real = # Type: float\n",
      "imaginary = # Type: float\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Complex(BaseModel):\n",
    "    real: float\n",
    "    imaginary: float\n",
    "\n",
    "print(XML.make_format([{'model': Complex}], 'single')[0])\n",
    "print(TOML.make_format([{'model': Complex}], 'single')[0])\n",
    "print(JSON.make_format([{'model': Complex}], 'single')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TOML:\n",
    "    @staticmethod\n",
    "    def make_format(tasks: List[dict], return_sequence: str) -> str:\n",
    "        grammar, instruct = \"\", []\n",
    "        for task in tasks:\n",
    "            model = task.get('model')\n",
    "            command = task.get('task_name', '')\n",
    "            if isinstance(model, list):\n",
    "                name = \"_\".join([m.__name__ for m in model])\n",
    "            else:\n",
    "                name = model.__name__\n",
    "                model = [model]\n",
    "            instruct.append(name)\n",
    "\n",
    "            variables = ModelParser.extract_variables_with_descriptions(model)\n",
    "            forma = TOML.generate_prompt_from_variables(variables, nested=True)\n",
    "            grammar += f\"{name}:\\n```\\n{forma}\\n```\\n\\n\"\n",
    "\n",
    "        return grammar, instruct\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_prompt_from_variables(variables_info: dict, nested: bool = False) -> str:\n",
    "        prompt_lines = []\n",
    "        for model_name, fields in variables_info.items():\n",
    "            if nested:\n",
    "                prompt_lines.append(f\"[{model_name}]\")\n",
    "            for var_name, details in fields.items():\n",
    "                line = f'{var_name} = '\n",
    "                if details.get('description'):\n",
    "                    line += f'\"{details[\"description\"]}\"'\n",
    "                line += f'# Type: {details[\"type\"]}'\n",
    "                if str(details.get(\"default\")) not in ['PydanticUndefined', 'None']:\n",
    "                    line += f', Default: \"{details[\"default\"]}\"'\n",
    "                prompt_lines.append(line)\n",
    "        return \"\\n\".join(prompt_lines)\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_single_model_prompt(fields: dict, model_name: str, nested: bool = False) -> dict:\n",
    "        model_data = {}\n",
    "        for var_name, details in fields.items():\n",
    "            model_data[var_name] = {\n",
    "                'description': details['description'],\n",
    "                'type': details['type'],\n",
    "                'default': details['default'] if str(details.get(\"default\")) != 'PydanticUndefined' else None\n",
    "            }\n",
    "        return model_data\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_toml(toml_string):\n",
    "        def parse_section(toml_string, i):\n",
    "            start = i\n",
    "            while toml_string[i] != ']':\n",
    "                i += 1\n",
    "            key = toml_string[start:i]\n",
    "            i = skip_whitespace(toml_string, i + 1)\n",
    "            section = {key: {}}\n",
    "            while i < len(toml_string) and toml_string[i] not in '[':\n",
    "                subkey, i = parse_key(toml_string, i)\n",
    "                i = skip_whitespace(toml_string, i)\n",
    "                if toml_string[i] == '=':\n",
    "                    i = skip_whitespace(toml_string, i + 1)\n",
    "                    value, i = parse_value(toml_string, i)\n",
    "                    section[key][subkey.replace('\\n', '')] = value\n",
    "                i = skip_whitespace(toml_string, i)\n",
    "            return section, i\n",
    "\n",
    "        def parse_key(toml_string, i):\n",
    "            start = i\n",
    "            while toml_string[i] not in '=':\n",
    "                i += 1\n",
    "            return toml_string[start:i], i\n",
    "\n",
    "        def parse_value(toml_string, i):\n",
    "            if toml_string[i] == '\"':\n",
    "                print('starting string')\n",
    "                return parse_string(toml_string, i + 1)\n",
    "            elif toml_string[i] == '[':\n",
    "                print('starting array')\n",
    "                return parse_array(toml_string, i)\n",
    "            else:\n",
    "                return parse_number(toml_string, i)\n",
    "\n",
    "        def parse_string(toml_string, i):\n",
    "            start = i\n",
    "            while toml_string[i] != '\"':\n",
    "                print('char', toml_string[i])\n",
    "                i += 1\n",
    "            print('string', toml_string[start:i])\n",
    "            return toml_string[start:i], i \n",
    "\n",
    "        def parse_number(toml_string, i):\n",
    "            start = i\n",
    "            while toml_string[i] in '0123456789.-':\n",
    "                i += 1\n",
    "\n",
    "            val = toml_string[start:i]\n",
    "            try: \n",
    "                return int(val), i\n",
    "            except ValueError:\n",
    "                return '', i\n",
    "\n",
    "        def parse_array(toml_string, i):\n",
    "            array = []\n",
    "            i = skip_whitespace(toml_string, i + 1)\n",
    "            while toml_string[i] != ']':\n",
    "                print('array', toml_string[:i])\n",
    "                value, i = parse_value(toml_string, i)\n",
    "                array.append(value)\n",
    "                i = skip_whitespace(toml_string, i + 1)\n",
    "            array = [x for x in array if x]\n",
    "            return array, i + 1\n",
    "\n",
    "        def skip_whitespace(toml_string, i):\n",
    "            while i < len(toml_string) and toml_string[i] in ' \\t\\n\\r':\n",
    "                print('skipped IN CONSIDERED', toml_string[i])\n",
    "                i += 1\n",
    "            print('skipped', toml_string[:i])    \n",
    "            return i\n",
    "        \n",
    "        i = 0\n",
    "        storage = {}\n",
    "        while i < len(toml_string):\n",
    "            i = skip_whitespace(toml_string, i)\n",
    "            if i < len(toml_string) and toml_string[i] == '[':\n",
    "                section, i = parse_section(toml_string, i + 1)\n",
    "                key = list(section.keys())[0]\n",
    "                if key in storage:\n",
    "                    storage[key].append(section[key])\n",
    "                else:\n",
    "                    storage[key] = [section[key]]\n",
    "            else:\n",
    "                break\n",
    "        return storage\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse(text):\n",
    "        return TOML.parse_toml(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped IN CONSIDERED  \n",
      "skipped IN CONSIDERED \n",
      "\n",
      "skipped  \n",
      "\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\" \n",
    "Man\n",
    "[BudgetPlan]\n",
    "total_cost = 500\n",
    "all_items = [\"cake\", \"balloons\", \"roses\", \"ice cream\"]\n",
    "[EventSchedule]\n",
    "start_time = \"12:00 PM\"\n",
    "end_time = \"4:00 PM\"\n",
    "activities = [\"cake cutting\", \"balloon decoration\", \"rose gifting\", \"ice cream party\"]\n",
    "\"\"\"\n",
    "\n",
    "print(TOML.parse(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tokenize_and_count_hf(text, model_name='bert-base-uncased'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# Example usage:\n",
    "# tokens, count = tokenize_and_count_hf(, 'Hello, world!')\n",
    "# print(f'Tokens: {tokens}, Count: {count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def tokenize_and_count_tiktoken(text, model_name=\"gpt-3.5-turbo\"):\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    tokens = encoding.encode(text)\n",
    "    return tokens, len(tokens), encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = \"\"\" \n",
    "```\n",
    "{\n",
    "\"ThoughtState\": {\n",
    "\"thought\": \"Vladimir Putin is the current President of Russia.\",\n",
    "\"goal\": \"To provide information about Vladimir Putin.\",\n",
    "\"tool\": \"Web_Search\",\n",
    "\"action\": \"Read\",\n",
    "\"action_input\": \"Vladimir Putin biography\",\n",
    "\"thought_id\": \"12345\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "expected_tokens = \"\"\" \n",
    "{\n",
    "\"ThoughtState\": {\n",
    "\"thought\": \"\",\n",
    "\"goal\": \"\",\n",
    "\"tool\": \"\",\n",
    "\"action\": \"\",\n",
    "\"action_input\": \"\",\n",
    "\"thought_id\": \"\"\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder: \n",
    "    def __init__(self, expected): \n",
    "        self.encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "        self.expected_tokens  = self.tokenize(expected)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return self.encoding.encode(text)\n",
    "        \n",
    "    def get_next_states(current_state, tokens, expected_tokens):\n",
    "    # Initialize probabilities for each token in the input string\n",
    "    token_probabilities = [0] * len(tokens)  # Placeholder for actual probability calculation\n",
    "\n",
    "    # Determine the type of the next expected token (terminal or non-terminal)\n",
    "    next_expected_token_type = get_token_type(expected_tokens, current_state)\n",
    "\n",
    "    # Calculate probabilities for each token in the input string\n",
    "    for i, token in enumerate(tokens):\n",
    "        token_type = 'terminal' if token in expected_tokens else 'non-terminal'\n",
    "        if token_type == next_expected_token_type:\n",
    "            # Assign higher probability if the token matches the expected type\n",
    "            token_probabilities[i] += calculate_probability(token, i, tokens, expected_tokens)\n",
    "\n",
    "    # Choose the most probable next token\n",
    "    next_token_index = token_probabilities.index(max(token_probabilities))\n",
    "    next_token = tokens[next_token_index]\n",
    "\n",
    "    return next_token\n",
    "    \n",
    "    def get_next_states(self, current_state, tokens):\n",
    "        token_probabilities = [self.calculate_probability(token, i, tokens) for i, token in enumerate(tokens)]\n",
    "        # Choose the token with the highest probability score as the most probable next state\n",
    "        next_token_index = token_probabilities.index(max(token_probabilities))\n",
    "        next_token = tokens[next_token_index]\n",
    "        return next_token, next_token_index\n",
    "\n",
    "    def decode(self, input_string):\n",
    "        tokens = self.tokenize(input_string)\n",
    "        path = []\n",
    "        current_state = 'root'  # Starting state\n",
    "        token_index = 0\n",
    "\n",
    "        while token_index < len(tokens):\n",
    "            current_token = tokens[token_index]\n",
    "            next_token, next_token_index = self.get_next_states(current_state, tokens[token_index:])\n",
    "            path.append(next_token)\n",
    "            token_index += next_token_index + 1  # Move to the index of the next token\n",
    "\n",
    "            # Update current_state based on your state transition logic\n",
    "            # Placeholder: current_state = next_token\n",
    "            current_state = next_token\n",
    "\n",
    "            if current_state == 'complete':\n",
    "                break  # Terminate if the decoding process is complete\n",
    "\n",
    "        return ' '.join(path)\n",
    "\n",
    "    def calculate_probability(token, index, tokens, expected_tokens):\n",
    "        \"\"\"\n",
    "        Calculates a probability score for a token based on its match with expected type,\n",
    "        its closeness to the expected position, and its fit within the expected sequence.\n",
    "        \n",
    "        :param token: The current token being evaluated.\n",
    "        :param index: The index of the current token in the input tokens list.\n",
    "        :param tokens: The list of all tokens in the input string.\n",
    "        :param expected_tokens: The list of tokens in the expected format, including placeholders for non-terminal tokens.\n",
    "        :return: A probability score for the token.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Constants for weighting different factors\n",
    "        TYPE_WEIGHT = 0.5\n",
    "        POSITION_WEIGHT = 0.3\n",
    "        SEQUENCE_WEIGHT = 0.2\n",
    "\n",
    "        # Determine if the token is terminal or non-terminal\n",
    "        is_terminal = token in expected_tokens\n",
    "        type_score = TYPE_WEIGHT if is_terminal else 0\n",
    "\n",
    "        # Calculate positional closeness score\n",
    "        # Assuming expected position is based on the order in expected_tokens\n",
    "        if token in expected_tokens:\n",
    "            expected_index = expected_tokens.index(token)\n",
    "            position_diff = abs(index - expected_index)\n",
    "        else:\n",
    "            # If it's a non-terminal, we find the closest terminal token's expected position\n",
    "            closest_terminal_index = min([i for i, t in enumerate(tokens) if t in expected_tokens], key=lambda x: abs(x - index))\n",
    "            expected_index = expected_tokens.index(tokens[closest_terminal_index])\n",
    "            position_diff = abs(index - expected_index)\n",
    "        \n",
    "        # Normalize position_diff to a score between 0 and 1, assuming max diff is the length of tokens\n",
    "        max_diff = len(tokens)\n",
    "        positional_score = POSITION_WEIGHT * (1 - (position_diff / max_diff))\n",
    "\n",
    "        # Calculate sequence fit score\n",
    "        # This simplistic approach checks if the next expected token matches the next actual token\n",
    "        # More complex logic could involve checking subsequences\n",
    "        if index + 1 < len(tokens) and tokens[index + 1] in expected_tokens:\n",
    "            next_expected_token = expected_tokens[expected_tokens.index(token) + 1] if token in expected_tokens else None\n",
    "            sequence_fit_score = SEQUENCE_WEIGHT if tokens[index + 1] == next_expected_token else 0\n",
    "        else:\n",
    "            sequence_fit_score = 0\n",
    "\n",
    "        # Combine scores for final probability\n",
    "        probability_score = type_score + positional_score + sequence_fit_score\n",
    "        return probability_score\n",
    "\n",
    "    def get_token_type(expected_tokens, current_state):\n",
    "        if current_state in expected_tokens:\n",
    "            return 'terminal'\n",
    "        else:\n",
    "            return 'non-terminal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m decoder \u001b[38;5;241m=\u001b[39m Decoder(expected_tokens)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 25\u001b[0m, in \u001b[0;36mDecoder.decode\u001b[0;34m(self, input_string)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m token_index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens):\n\u001b[1;32m     24\u001b[0m     current_token \u001b[38;5;241m=\u001b[39m tokens[token_index]\n\u001b[0;32m---> 25\u001b[0m     next_token, next_token_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken_index\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     path\u001b[38;5;241m.\u001b[39mappend(next_token)\n\u001b[1;32m     27\u001b[0m     token_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m next_token_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Move to the index of the next token\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[37], line 11\u001b[0m, in \u001b[0;36mDecoder.get_next_states\u001b[0;34m(self, current_state, tokens)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_next_states\u001b[39m(\u001b[38;5;28mself\u001b[39m, current_state, tokens):\n\u001b[0;32m---> 11\u001b[0m     token_probabilities \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_probability(token, i, tokens) \u001b[38;5;28;01mfor\u001b[39;00m i, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens)]\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Choose the token with the highest probability score as the most probable next state\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     next_token_index \u001b[38;5;241m=\u001b[39m token_probabilities\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mmax\u001b[39m(token_probabilities))\n",
      "Cell \u001b[0;32mIn[37], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_next_states\u001b[39m(\u001b[38;5;28mself\u001b[39m, current_state, tokens):\n\u001b[0;32m---> 11\u001b[0m     token_probabilities \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_probability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens)]\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Choose the token with the highest probability score as the most probable next state\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     next_token_index \u001b[38;5;241m=\u001b[39m token_probabilities\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mmax\u001b[39m(token_probabilities))\n",
      "Cell \u001b[0;32mIn[37], line 66\u001b[0m, in \u001b[0;36mDecoder.calculate_probability\u001b[0;34m(token, index, tokens, expected_tokens)\u001b[0m\n\u001b[1;32m     63\u001b[0m     position_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(index \u001b[38;5;241m-\u001b[39m expected_index)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# If it's a non-terminal, we find the closest terminal token's expected position\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     closest_terminal_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m([i \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m expected_tokens], key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mabs\u001b[39m(x \u001b[38;5;241m-\u001b[39m index))\n\u001b[1;32m     67\u001b[0m     expected_index \u001b[38;5;241m=\u001b[39m expected_tokens\u001b[38;5;241m.\u001b[39mindex(tokens[closest_terminal_index])\n\u001b[1;32m     68\u001b[0m     position_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(index \u001b[38;5;241m-\u001b[39m expected_index)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(expected_tokens)\n",
    "decoder.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = tokenize_and_count_tiktoken(expected_tokens)\n",
    "real = tokenize_and_count_tiktoken(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_states(0, real, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real[2].decode([72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[720,\n",
       " 14196,\n",
       " 4077,\n",
       " 517,\n",
       " 1,\n",
       " 85269,\n",
       " 1423,\n",
       " 794,\n",
       " 341,\n",
       " 1,\n",
       " 61665,\n",
       " 794,\n",
       " 330,\n",
       " 53,\n",
       " 18599,\n",
       " 31204,\n",
       " 21810,\n",
       " 374,\n",
       " 279,\n",
       " 1510,\n",
       " 4900,\n",
       " 315,\n",
       " 8524,\n",
       " 10560,\n",
       " 1,\n",
       " 35039,\n",
       " 794,\n",
       " 330,\n",
       " 1271,\n",
       " 3493,\n",
       " 2038,\n",
       " 922,\n",
       " 36011,\n",
       " 21810,\n",
       " 10560,\n",
       " 1,\n",
       " 14506,\n",
       " 794,\n",
       " 330,\n",
       " 6109,\n",
       " 67013,\n",
       " 761,\n",
       " 1,\n",
       " 1335,\n",
       " 794,\n",
       " 330,\n",
       " 4518,\n",
       " 761,\n",
       " 1,\n",
       " 1335,\n",
       " 6022,\n",
       " 794,\n",
       " 330,\n",
       " 53,\n",
       " 18599,\n",
       " 31204,\n",
       " 21810,\n",
       " 48345,\n",
       " 761,\n",
       " 1,\n",
       " 61665,\n",
       " 851,\n",
       " 794,\n",
       " 330,\n",
       " 4513,\n",
       " 1774,\n",
       " 702,\n",
       " 262,\n",
       " 457,\n",
       " 534,\n",
       " 14196,\n",
       " 4077]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
